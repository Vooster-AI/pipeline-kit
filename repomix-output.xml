This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: *.md, ./**/*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.github/
  ISSUE_TEMPLATE/
    1-bug-report.yml
    2-feature-request.yml
    3-docs-issue.yml
  workflows/
    ci.yml
    codespell.yml
    coverage.yml
    pr-comment.yml
    release.yml
    rust-ci.yml
    security.yml
  dependabot.yaml
.pipeline-kit/
  pipelines/
    code-review.yaml
    simple-task.yaml
  config.toml
.ruler/
  ruler.toml
pipeline-kit-cli/
  bin/
    pipeline-kit.js
  lib/
    platform.js
  scripts/
    install.js
    test_install_native_deps.sh
  test/
    binary-path.test.js
    install.test.js
    integration.test.js
    platform.test.js
  .gitignore
  .npmignore
  package.json
  tsconfig.json
  vitest.config.js
pipeline-kit-rs/
  .cargo/
    audit.toml
  .config/
    nextest.toml
  crates/
    cli/
      src/
        main.rs
      Cargo.toml
    core/
      src/
        agents/
          adapters/
            claude_adapter.rs
            codex_adapter.rs
            cursor_adapter.rs
            gemini_adapter.rs
            mock_agent.rs
            mod.rs
            qwen_adapter.rs
          agent_type.rs
          base.rs
          cli_executor.rs
          factory.rs
          manager.rs
          mod.rs
        config/
          error.rs
          loader.rs
          mod.rs
          models.rs
        engine/
          mod.rs
        init/
          error.rs
          generator.rs
          mod.rs
          templates.rs
        state/
          manager.rs
          mod.rs
          process.rs
        lib.rs
      tests/
        common/
          assertions.rs
          fixtures.rs
          mock_agents.rs
          mock_cli.py
          mod.rs
        agent_integration.rs
        cli_executor.rs
        e2e_pipeline.rs
        pipeline_engine.rs
        qwen_integration.rs
      Cargo.toml
    protocol/
      src/
        agent_models.rs
        config_models.rs
        ipc.rs
        lib.rs
        pipeline_models.rs
        process_models.rs
      tests/
        serialization.rs
      Cargo.toml
    protocol-ts/
      src/
        lib.rs
      Cargo.toml
    tui/
      src/
        widgets/
          command_composer.rs
          dashboard.rs
          detail_view.rs
          mod.rs
        app.rs
        event_handler.rs
        event.rs
        lib.rs
        main.rs
        tui.rs
      Cargo.toml
  Cargo.toml
scripts/
  asciicheck.py
  pre-release-check.sh
  verify-ts-types.sh
templates/
  pipelines/
    code-review.yaml
    simple-task.yaml
  config.toml
.codespellignore
.codespellrc
.gitignore
.mcp.json
LICENSE
package.json
pnpm-workspace.yaml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "enabledMcpjsonServers": [
    "codex-github-fetcher"
  ],
  "enableAllProjectMcpServers": true
}
</file>

<file path=".github/ISSUE_TEMPLATE/1-bug-report.yml">
name: üêõ Bug Report
description: Report an issue that should be fixed
labels:
  - bug
  - needs triage
body:
  - type: markdown
    attributes:
      value: |
        Thank you for submitting a bug report! It helps make Pipeline Kit better for everyone.

        Make sure you are running the [latest](https://www.npmjs.com/package/pipeline-kit) version of Pipeline Kit. The bug you are experiencing may already have been fixed.

        Please try to include as much information as possible.

  - type: input
    id: version
    attributes:
      label: What version of Pipeline Kit are you using?
      description: Copy the output of `pipeline-kit --version`
  - type: input
    id: platform
    attributes:
      label: What platform is your computer?
      description: |
        For MacOS and Linux: copy the output of `uname -mprs`
        For Windows: copy the output of `"$([Environment]::OSVersion | ForEach-Object VersionString) $(if ([Environment]::Is64BitOperatingSystem) { "x64" } else { "x86" })"` in the PowerShell console
  - type: textarea
    id: steps
    attributes:
      label: What steps can reproduce the bug?
      description: Explain the bug and provide a code snippet that can reproduce it.
    validations:
      required: true
  - type: textarea
    id: expected
    attributes:
      label: What is the expected behavior?
      description: If possible, please provide text instead of a screenshot.
  - type: textarea
    id: actual
    attributes:
      label: What do you see instead?
      description: If possible, please provide text instead of a screenshot.
  - type: textarea
    id: notes
    attributes:
      label: Additional information
      description: Is there anything else you think we should know?
</file>

<file path=".github/ISSUE_TEMPLATE/2-feature-request.yml">
name: üéÅ Feature Request
description: Propose a new feature for Pipeline Kit
labels:
  - enhancement
  - needs triage
body:
  - type: markdown
    attributes:
      value: |
        Is Pipeline Kit missing a feature that you'd like to see? Feel free to propose it here.

        Before you submit a feature:
        1. Search existing issues for similar features. If you find one, üëç it rather than opening a new one.
        2. The Pipeline Kit team will try to balance the varying needs of the community when prioritizing or rejecting new features. Not all features will be accepted.

  - type: textarea
    id: feature
    attributes:
      label: What feature would you like to see?
    validations:
      required: true
  - type: textarea
    id: author
    attributes:
      label: Are you interested in implementing this feature?
      description: Please wait for acknowledgement before implementing or opening a PR.
  - type: textarea
    id: notes
    attributes:
      label: Additional information
      description: Is there anything else you think we should know?
</file>

<file path=".github/ISSUE_TEMPLATE/3-docs-issue.yml">
name: üìó Documentation Issue
description: Tell us if there is missing or incorrect documentation
labels: [docs]
body:
  - type: markdown
    attributes:
      value: |
        Thank you for submitting a documentation request. It helps make Pipeline Kit better.
  - type: dropdown
    attributes:
      label: What is the type of issue?
      multiple: true
      options:
        - Documentation is missing
        - Documentation is incorrect
        - Documentation is confusing
        - Example code is not working
        - Something else
  - type: textarea
    attributes:
      label: What is the issue?
    validations:
      required: true
  - type: textarea
    attributes:
      label: Where did you find it?
      description: If possible, please provide the URL(s) or file path where you found this issue.
</file>

<file path=".github/workflows/codespell.yml">
# Codespell configuration is within .codespellrc
---
name: Codespell

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: read

jobs:
  codespell:
    name: Check for spelling errors
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v5
      - name: Annotate locations with typos
        uses: codespell-project/codespell-problem-matcher@v1
      - name: Codespell
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
</file>

<file path=".github/workflows/coverage.yml">
name: Code Coverage

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  coverage:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    defaults:
      run:
        working-directory: pipeline-kit-rs

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: llvm-tools-preview

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            pipeline-kit-rs/target/
          key: ${{ runner.os }}-coverage-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-coverage-

      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      - name: Generate coverage report
        run: |
          cargo llvm-cov --all-features --workspace \
            --lcov --output-path ../lcov.info
        env:
          RUST_BACKTRACE: 1

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: lcov.info
          token: ${{ secrets.CODECOV_TOKEN }}
          fail_ci_if_error: false
          verbose: true

      - name: Generate coverage summary
        run: |
          echo "### üìä Code Coverage Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cargo llvm-cov report --summary-only >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full report available on [Codecov](https://codecov.io/gh/${{ github.repository }})" >> $GITHUB_STEP_SUMMARY

      - name: Upload coverage artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: lcov.info
          retention-days: 30
</file>

<file path=".github/workflows/pr-comment.yml">
name: PR Comment

on:
  workflow_run:
    workflows: ["rust-ci", "Code Coverage", "Security Audit"]
    types:
      - completed

# Only run on pull requests
jobs:
  comment:
    name: Post PR Comment
    runs-on: ubuntu-latest
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.conclusion != 'cancelled'
    permissions:
      pull-requests: write
      actions: read

    steps:
      - name: Download PR number artifact
        uses: actions/github-script@v7
        id: pr-number
        with:
          script: |
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.payload.workflow_run.id,
            });

            // Try to find PR number from artifacts or use PR from the workflow run
            const prNumber = context.payload.workflow_run.pull_requests[0]?.number;
            if (!prNumber) {
              console.log('No PR number found, skipping comment');
              return null;
            }
            return prNumber;
          result-encoding: string

      - name: Find existing comment
        uses: peter-evans/find-comment@v3
        if: steps.pr-number.outputs.result != 'null'
        id: find-comment
        with:
          issue-number: ${{ steps.pr-number.outputs.result }}
          comment-author: 'github-actions[bot]'
          body-includes: '<!-- pipeline-kit-ci-summary -->'

      - name: Build comment body
        if: steps.pr-number.outputs.result != 'null'
        id: comment-body
        uses: actions/github-script@v7
        with:
          script: |
            const workflowName = context.payload.workflow_run.name;
            const conclusion = context.payload.workflow_run.conclusion;
            const runUrl = context.payload.workflow_run.html_url;

            let emoji = '‚úÖ';
            if (conclusion === 'failure') emoji = '‚ùå';
            else if (conclusion === 'success') emoji = '‚úÖ';
            else if (conclusion === 'skipped') emoji = '‚è≠Ô∏è';

            let body = `<!-- pipeline-kit-ci-summary -->\n`;
            body += `## ü§ñ CI Results\n\n`;
            body += `| Workflow | Status | Details |\n`;
            body += `|----------|--------|----------|\n`;
            body += `| **${workflowName}** | ${emoji} ${conclusion} | [View Run](${runUrl}) |\n`;
            body += `\n`;

            if (workflowName === 'Code Coverage') {
              body += `### üìä Coverage Report\n`;
              body += `Coverage report is available on [Codecov](https://codecov.io/gh/${context.repo.owner}/${context.repo.repo}/pull/${{ steps.pr-number.outputs.result }}).\n\n`;
            }

            if (workflowName === 'Security Audit') {
              if (conclusion === 'success') {
                body += `### üîí Security Audit\n`;
                body += `No security vulnerabilities detected. All dependencies are secure.\n\n`;
              } else {
                body += `### ‚ö†Ô∏è Security Audit\n`;
                body += `Security issues detected. Please review the [workflow run](${runUrl}) for details.\n\n`;
              }
            }

            body += `\n---\n`;
            body += `*Last updated: ${new Date().toISOString()}*`;

            return body;

      - name: Create or update comment
        uses: peter-evans/create-or-update-comment@v4
        if: steps.pr-number.outputs.result != 'null'
        with:
          comment-id: ${{ steps.find-comment.outputs.comment-id }}
          issue-number: ${{ steps.pr-number.outputs.result }}
          body: ${{ steps.comment-body.outputs.result }}
          edit-mode: replace
</file>

<file path=".github/workflows/release.yml">
name: release

on:
  push:
    tags:
      - "v*.*.*"

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  # ==========================================
  # Job 1: Tag Validation
  # ==========================================
  tag-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5

      - name: Validate tag matches Cargo.toml version
        shell: bash
        run: |
          set -euo pipefail
          echo "::group::Tag validation"

          # 1. Verify this is a tag push
          [[ "${GITHUB_REF_TYPE}" == "tag" ]] \
            || { echo "‚ùå Not a tag push"; exit 1; }

          # 2. Verify tag format (v*.*.*)
          [[ "${GITHUB_REF_NAME}" =~ ^v[0-9]+\.[0-9]+\.[0-9]+(-alpha|-beta)?$ ]] \
            || { echo "‚ùå Tag '${GITHUB_REF_NAME}' doesn't match v*.*.* format"; exit 1; }

          # 3. Extract versions
          tag_ver="${GITHUB_REF_NAME#v}"  # Remove 'v' prefix
          cargo_ver="$(grep -m1 '^version' pipeline-kit-rs/Cargo.toml \
                        | sed -E 's/version *= *"([^"]+)".*/\1/')"

          # 4. Compare versions
          [[ "${tag_ver}" == "${cargo_ver}" ]] \
            || { echo "‚ùå Tag ${tag_ver} ‚â† Cargo.toml ${cargo_ver}"; exit 1; }

          echo "‚úÖ Tag and Cargo.toml version match (${tag_ver})"
          echo "::endgroup::"

  # ==========================================
  # Job 2: Build Binaries (6 platforms)
  # ==========================================
  build:
    needs: tag-check
    name: ${{ matrix.platform-name }}
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        include:
          # macOS
          - runner: macos-13
            target: x86_64-apple-darwin
            platform-name: macos-x64
            binary: pipeline
          - runner: macos-14
            target: aarch64-apple-darwin
            platform-name: macos-arm64
            binary: pipeline

          # Linux (musl for portability)
          - runner: ubuntu-latest
            target: x86_64-unknown-linux-musl
            platform-name: linux-x64
            binary: pipeline

          # Linux ARM64 (cross-compilation from x64)
          - runner: ubuntu-latest
            target: aarch64-unknown-linux-musl
            platform-name: linux-arm64
            binary: pipeline
            cross: true

          # Windows
          - runner: windows-latest
            target: x86_64-pc-windows-msvc
            platform-name: windows-x64
            binary: pipeline.exe

          # Windows ARM64 (cross-compilation from x64)
          - runner: windows-latest
            target: aarch64-pc-windows-msvc
            platform-name: windows-arm64
            binary: pipeline.exe
            cross: true

    steps:
      - uses: actions/checkout@v5

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: ${{ matrix.target }}

      - name: Cache Cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            pipeline-kit-rs/target/
          key: ${{ matrix.runner }}-${{ matrix.target }}-release-${{ hashFiles('**/Cargo.lock') }}

      - name: Install musl tools (Linux only)
        if: contains(matrix.target, 'musl') && !matrix.cross
        run: sudo apt install -y musl-tools

      - name: Install cross-compilation tools (Linux ARM64)
        if: matrix.target == 'aarch64-unknown-linux-musl'
        run: |
          sudo apt update
          sudo apt install -y gcc-aarch64-linux-gnu musl-tools
          rustup target add aarch64-unknown-linux-musl

      - name: Build binary (native)
        if: ${{ !matrix.cross }}
        working-directory: pipeline-kit-rs
        run: cargo build --target ${{ matrix.target }} --release --bin pipeline

      - name: Build binary (cross-compile)
        if: matrix.cross
        working-directory: pipeline-kit-rs
        env:
          CARGO_TARGET_AARCH64_UNKNOWN_LINUX_MUSL_LINKER: aarch64-linux-gnu-gcc
        run: cargo build --target ${{ matrix.target }} --release --bin pipeline

      - name: Create archive structure
        shell: bash
        run: |
          # Create directory structure that install_native_deps.sh expects
          # Result: pipeline-kit/{binary}
          mkdir -p dist/pipeline-kit
          cp pipeline-kit-rs/target/${{ matrix.target }}/release/${{ matrix.binary }} \
             dist/pipeline-kit/${{ matrix.binary }}

      - name: Create tar.gz archive
        shell: bash
        run: |
          cd dist
          tar -czf pipeline-kit-${{ matrix.platform-name }}.tar.gz pipeline-kit/

      - name: Generate checksum
        shell: bash
        run: |
          cd dist
          if command -v sha256sum &> /dev/null; then
            sha256sum pipeline-kit-${{ matrix.platform-name }}.tar.gz \
              > pipeline-kit-${{ matrix.platform-name }}.tar.gz.sha256
          else
            shasum -a 256 pipeline-kit-${{ matrix.platform-name }}.tar.gz \
              > pipeline-kit-${{ matrix.platform-name }}.tar.gz.sha256
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.platform-name }}
          path: |
            dist/pipeline-kit-${{ matrix.platform-name }}.tar.gz
            dist/pipeline-kit-${{ matrix.platform-name }}.tar.gz.sha256

  # ==========================================
  # Job 3: Create GitHub Release
  # ==========================================
  release:
    needs: build
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - uses: actions/checkout@v5
        with:
          token: ${{ secrets.RELEASE_TOKEN || github.token }}

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: dist

      - name: Flatten artifacts
        run: |
          # Move all files from subdirectories to dist/
          find dist -type f -exec mv {} dist/ \;
          # Remove empty subdirectories
          find dist -type d -empty -delete
          # List final structure
          ls -lh dist/

      - name: Extract version
        id: version
        run: |
          version="${GITHUB_REF_NAME#v}"
          echo "number=${version}" >> $GITHUB_OUTPUT

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          name: Pipeline Kit v${{ steps.version.outputs.number }}
          tag_name: ${{ github.ref_name }}
          files: dist/*
          token: ${{ secrets.RELEASE_TOKEN || github.token }}
          body: |
            ## Installation

            ```bash
            npm install -g pipeline-kit@${{ steps.version.outputs.number }}
            ```

            ## Platform Binaries

            - macOS (Intel): `pipeline-kit-macos-x64.tar.gz`
            - macOS (Apple Silicon): `pipeline-kit-macos-arm64.tar.gz`
            - Linux (x64): `pipeline-kit-linux-x64.tar.gz`
            - Linux (ARM64): `pipeline-kit-linux-arm64.tar.gz`
            - Windows (x64): `pipeline-kit-windows-x64.tar.gz`
            - Windows (ARM64): `pipeline-kit-windows-arm64.tar.gz`

            Each archive includes a `.sha256` checksum file for verification.

            ## Verification

            Download and verify checksums:
            ```bash
            # Download
            gh release download v${{ steps.version.outputs.number }} \
              --repo ${{ github.repository }} \
              --pattern "pipeline-kit-*.tar.gz*"

            # Verify (Linux/macOS)
            sha256sum -c pipeline-kit-*.tar.gz.sha256

            # Verify (macOS alternative)
            shasum -a 256 -c pipeline-kit-*.tar.gz.sha256
            ```
          prerelease: ${{ contains(steps.version.outputs.number, '-alpha') || contains(steps.version.outputs.number, '-beta') }}

  # ==========================================
  # Job 4: Publish to npm
  # ==========================================
  npm-publish:
    needs: release
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-node@v4
        with:
          node-version: 18
          registry-url: "https://registry.npmjs.org"

      - name: Publish to npm
        working-directory: pipeline-kit-cli
        run: npm publish --access public
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
</file>

<file path=".github/workflows/security.yml">
name: Security Audit

on:
  pull_request:
    paths:
      - '**/Cargo.lock'
      - '**/Cargo.toml'
  push:
    branches: [main]
  schedule:
    # Run daily at 00:00 UTC
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  audit:
    name: Cargo Audit
    runs-on: ubuntu-latest
    timeout-minutes: 10
    defaults:
      run:
        working-directory: pipeline-kit-rs

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Setup Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo-audit
        uses: actions/cache@v4
        with:
          path: ~/.cargo/bin/cargo-audit
          key: ${{ runner.os }}-cargo-audit-${{ hashFiles('~/.cargo/bin/cargo-audit') }}

      - name: Install cargo-audit
        run: cargo install cargo-audit --locked || true

      - name: Run security audit
        run: cargo audit --deny warnings

      - name: Check for yanked crates
        run: cargo audit --deny yanked

      - name: Security report summary
        if: always()
        run: |
          echo "### üîí Security Audit Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cargo audit 2>&1 | tee -a $GITHUB_STEP_SUMMARY || true
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Checked against [RustSec Advisory Database](https://rustsec.org/)" >> $GITHUB_STEP_SUMMARY

  dependency-review:
    name: Dependency Review
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Dependency Review
        uses: actions/dependency-review-action@v4
        with:
          fail-on-severity: moderate
          deny-licenses: GPL-3.0, AGPL-3.0
          comment-summary-in-pr: always
</file>

<file path=".github/dependabot.yaml">
# https://docs.github.com/en/code-security/dependabot/working-with-dependabot/dependabot-options-reference#package-ecosystem

version: 2
updates:
  # Rust workspace dependencies
  - package-ecosystem: cargo
    directory: /pipeline-kit-rs
    schedule:
      interval: weekly
    groups:
      rust-dependencies:
        patterns:
          - "*"

  # TypeScript CLI dependencies
  - package-ecosystem: npm
    directory: /pipeline-kit-cli
    schedule:
      interval: weekly
    groups:
      npm-dependencies:
        patterns:
          - "*"

  # Root pnpm workspace
  - package-ecosystem: npm
    directory: /
    schedule:
      interval: weekly

  # GitHub Actions
  - package-ecosystem: github-actions
    directory: /
    schedule:
      interval: weekly

  # Rust toolchain
  - package-ecosystem: rust-toolchain
    directory: /pipeline-kit-rs
    schedule:
      interval: weekly
</file>

<file path=".pipeline-kit/pipelines/code-review.yaml">
name: code-review
master:
  model: claude-sonnet-4.5
  system-prompt: |
    You are the master orchestrator for a code review pipeline.
    Your job is to coordinate between the developer and reviewer agents
    to ensure high-quality code is produced.
  process:
    - developer
    - reviewer
    - HUMAN_REVIEW
sub-agents:
  - developer
  - reviewer
</file>

<file path=".pipeline-kit/pipelines/simple-task.yaml">
name: simple-task
master:
  model: claude-sonnet-4.5
  system-prompt: |
    You orchestrate a simple single-agent task.
  process:
    - developer
sub-agents:
  - developer
</file>

<file path=".ruler/ruler.toml">
# Ruler Configuration File
# See https://ai.intellectronica.net/ruler for documentation.

# To specify which agents are active by default when --agents is not used,
# uncomment and populate the following line. If omitted, all agents are active.
default_agents = ["codex", "claude", "cursor"]

# Enable nested rule loading from nested .ruler directories
# When enabled, ruler will search for and process .ruler directories throughout the project hierarchy
# nested = false

# --- Agent Specific Configurations ---
# You can enable/disable agents and override their default output paths here.
# Use lowercase agent identifiers: amp, copilot, claude, codex, cursor, windsurf, cline, aider, kilocode

# [agents.copilot]
# enabled = true
# output_path = ".github/copilot-instructions.md"

# [agents.aider]
# enabled = true
# output_path_instructions = "AGENTS.md"
# output_path_config = ".aider.conf.yml"

# [agents.gemini-cli]
# enabled = true

# --- MCP Servers ---
# Define Model Context Protocol servers here. Two examples:
# 1. A stdio server (local executable)
# 2. A remote server (HTTP-based)

# [mcp_servers.example_stdio]
# command = "node"
# args = ["scripts/your-mcp-server.js"]
# env = { API_KEY = "replace_me" }

# [mcp_servers.example_remote]
# url = "https://api.example.com/mcp"
# headers = { Authorization = "Bearer REPLACE_ME" }
</file>

<file path="pipeline-kit-cli/lib/platform.js">
/**
 * Platform detection and binary path resolution utilities.
 * This module is extracted to be testable separately from the main CLI script.
 */

import path from "path";

// Platform mapping from Node.js platform/arch to user-friendly names
// This matches the structure created by install_native_deps.sh
export const platformMap = {
  'darwin-x64': 'macos-x64',
  'darwin-arm64': 'macos-arm64',
  'linux-x64': 'linux-x64',
  'linux-arm64': 'linux-arm64',
  'android-arm64': 'linux-arm64', // Android uses Linux binaries
  'win32-x64': 'windows-x64',
  'win32-arm64': 'windows-arm64'
};

/**
 * Maps Node.js platform and architecture to user-friendly platform names.
 * @param {string} platform - process.platform value (e.g., 'darwin', 'linux', 'win32')
 * @param {string} arch - process.arch value (e.g., 'x64', 'arm64')
 * @returns {string | null} - User-friendly platform name or null if unsupported
 */
export function getPlatformName(platform, arch) {
  const platformKey = `${platform}-${arch}`;
  return platformMap[platformKey] || null;
}

/**
 * Legacy function for backward compatibility.
 * Maps Node.js platform and architecture to Rust target triples.
 * @param {string} platform - process.platform value (e.g., 'darwin', 'linux', 'win32')
 * @param {string} arch - process.arch value (e.g., 'x64', 'arm64')
 * @returns {string | null} - Rust target triple or null if unsupported
 */
export function getTargetTriple(platform, arch) {
  switch (platform) {
    case "linux":
    case "android":
      switch (arch) {
        case "x64":
          return "x86_64-unknown-linux-musl";
        case "arm64":
          return "aarch64-unknown-linux-musl";
        default:
          return null;
      }
    case "darwin":
      switch (arch) {
        case "x64":
          return "x86_64-apple-darwin";
        case "arm64":
          return "aarch64-apple-darwin";
        default:
          return null;
      }
    case "win32":
      switch (arch) {
        case "x64":
          return "x86_64-pc-windows-msvc";
        case "arm64":
          return "aarch64-pc-windows-msvc";
        default:
          return null;
      }
    default:
      return null;
  }
}

/**
 * Determines the binary filename based on platform.
 * @param {string} platform - process.platform value
 * @returns {string} - Binary filename ('pipeline.exe' on Windows, 'pipeline' elsewhere)
 */
export function getBinaryName(platform) {
  return platform === "win32" ? "pipeline.exe" : "pipeline";
}

/**
 * Constructs the vendor binary path using new platform naming.
 * @param {string} vendorRoot - Path to the vendor directory
 * @param {string} platformName - User-friendly platform name (e.g., 'macos-arm64')
 * @param {string} binaryName - Binary filename
 * @returns {string} - Full path to the vendor binary
 */
export function getVendorBinaryPath(vendorRoot, platformName, binaryName) {
  const platformRoot = path.join(vendorRoot, platformName);
  return path.join(platformRoot, "pipeline-kit", binaryName);
}

/**
 * Legacy function: Constructs the vendor binary path using target triple.
 * @param {string} vendorRoot - Path to the vendor directory
 * @param {string} targetTriple - Rust target triple
 * @param {string} binaryName - Binary filename
 * @returns {string} - Full path to the vendor binary
 */
export function getVendorBinaryPathLegacy(vendorRoot, targetTriple, binaryName) {
  const archRoot = path.join(vendorRoot, targetTriple);
  return path.join(archRoot, "pipeline-kit", binaryName);
}

/**
 * Constructs the development binary path.
 * @param {string} cliDir - Path to the pipeline-kit-cli directory
 * @param {string} binaryName - Binary filename
 * @returns {string} - Full path to the development binary
 */
export function getDevBinaryPath(cliDir, binaryName) {
  return path.join(cliDir, "..", "pipeline-kit-rs", "target", "release", binaryName);
}
</file>

<file path="pipeline-kit-cli/scripts/install.js">
#!/usr/bin/env node

/**
 * Install Pipeline Kit native binary
 * This script is called as a postinstall hook by npm.
 *
 * It supports two modes:
 * 1. Production mode (NODE_ENV=production): Downloads binaries from GitHub Releases
 * 2. Development mode (default): Copies binaries from local Rust build
 */

import fs from 'fs';
import path from 'path';
import os from 'os';
import { fileURLToPath } from 'url';
import axios from 'axios';
import { extract as tarExtract } from 'tar';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Detects the current platform and returns platform name and binary name
 * @param {string} platform - process.platform value (e.g., 'darwin', 'linux', 'win32')
 * @param {string} arch - process.arch value (e.g., 'x64', 'arm64')
 * @returns {{ platformName: string, binaryName: string }}
 * @throws {Error} If platform is unsupported
 */
export function detectPlatform(platform, arch) {
  const platformKey = `${platform}-${arch}`;

  const platformMap = {
    'darwin-x64': 'macos-x64',
    'darwin-arm64': 'macos-arm64',
    'linux-x64': 'linux-x64',
    'linux-arm64': 'linux-arm64',
    'android-arm64': 'linux-arm64', // Android uses Linux binaries
    'win32-x64': 'windows-x64',
    'win32-arm64': 'windows-arm64'
  };

  const platformName = platformMap[platformKey];

  if (!platformName) {
    throw new Error(
      `Unsupported platform: ${platform} (${arch})\n\n` +
      'Supported platforms:\n' +
      '  - macOS x64 (Intel)\n' +
      '  - macOS ARM64 (Apple Silicon)\n' +
      '  - Linux x64\n' +
      '  - Linux ARM64\n' +
      '  - Windows x64\n' +
      '  - Windows ARM64\n'
    );
  }

  const binaryName = platform === 'win32' ? 'pipeline.exe' : 'pipeline';

  return { platformName, binaryName };
}

/**
 * Builds the GitHub release download URL
 * @param {object} options
 * @param {string} options.repo - GitHub repository (e.g., 'Vooster-AI/pipeline-kit')
 * @param {string} options.version - Release version ('latest' or specific tag like 'v0.1.0')
 * @param {string} options.platformName - Platform name (e.g., 'macos-arm64')
 * @returns {string} - Download URL
 */
export function buildDownloadUrl({ repo, version, platformName }) {
  const archiveName = `pipeline-kit-${platformName}.tar.gz`;

  if (version === 'latest') {
    return `https://github.com/${repo}/releases/latest/download/${archiveName}`;
  } else {
    return `https://github.com/${repo}/releases/download/${version}/${archiveName}`;
  }
}

/**
 * Downloads and extracts a binary from GitHub releases
 * @param {object} options
 * @param {string} options.url - Download URL
 * @param {string} options.vendorDir - Vendor directory path
 * @param {string} options.platformName - Platform name
 * @param {string} options.binaryName - Binary filename
 * @param {boolean} [options.showProgress] - Whether to show download progress
 * @returns {Promise<void>}
 */
export async function downloadAndExtract({ url, vendorDir, platformName, binaryName, showProgress = true }) {
  const platformDir = path.join(vendorDir, platformName, 'pipeline-kit');
  const binaryPath = path.join(platformDir, binaryName);

  console.log('Production mode: Downloading Pipeline Kit binary from GitHub Releases...');
  console.log(`  URL: ${url}`);
  console.log(`  Platform: ${platformName}`);

  try {
    // Download the tar.gz file
    const response = await axios.get(url, {
      responseType: 'stream',
      timeout: 120000, // 2 minutes timeout
      maxRedirects: 5
    });

    if (!response.data) {
      throw new Error('No data received from download');
    }

    // Create a temporary file for the download
    const tempDir = fs.mkdtempSync(path.join(os.tmpdir(), 'pipeline-kit-'));
    const tempFile = path.join(tempDir, `pipeline-kit-${platformName}.tar.gz`);

    // Show progress if enabled
    const totalLength = response.headers['content-length'];
    let downloadedLength = 0;

    if (showProgress && totalLength) {
      console.log(`  Downloading: 0%`);
    }

    // Write to temporary file
    const writer = fs.createWriteStream(tempFile);

    response.data.on('data', (chunk) => {
      downloadedLength += chunk.length;
      if (showProgress && totalLength) {
        const percentage = Math.round((downloadedLength / totalLength) * 100);
        process.stdout.write(`\r  Downloading: ${percentage}%`);
      }
    });

    await new Promise((resolve, reject) => {
      response.data.pipe(writer);
      writer.on('finish', resolve);
      writer.on('error', reject);
    });

    if (showProgress && totalLength) {
      console.log(''); // New line after progress
    }

    console.log('  Extracting archive...');

    // Create platform directory
    fs.mkdirSync(platformDir, { recursive: true });

    // Extract tar.gz to platform directory
    await tarExtract({
      file: tempFile,
      cwd: platformDir
    });

    // Verify binary exists
    if (!fs.existsSync(binaryPath)) {
      throw new Error(
        `Binary not found in archive at expected location: ${binaryPath}\n` +
        'Please check that the archive contains the correct structure.'
      );
    }

    // Make binary executable (Unix systems)
    if (process.platform !== 'win32') {
      fs.chmodSync(binaryPath, 0o755);
    }

    // Clean up temp file
    fs.rmSync(tempDir, { recursive: true, force: true });

    console.log(`  Binary installed successfully at: ${binaryPath}`);

  } catch (error) {
    if (error.response) {
      // HTTP error
      throw new Error(
        `Failed to download binary from ${url}\n` +
        `HTTP Status: ${error.response.status}\n` +
        `\n` +
        `Possible reasons:\n` +
        `  - No releases have been published yet\n` +
        `  - The release doesn't include binaries for ${platformName}\n` +
        `  - Network connectivity issues\n` +
        `\n` +
        `For development, build locally:\n` +
        `  cd pipeline-kit-rs && cargo build --release\n` +
        `  Then run: npm install (without NODE_ENV=production)`
      );
    } else {
      // Other error (network, extraction, etc.)
      throw error;
    }
  }
}

/**
 * Copies a local binary from Rust build directory to vendor directory
 * @param {object} options
 * @param {string} options.sourcePath - Source binary path
 * @param {string} options.vendorDir - Vendor directory path
 * @param {string} options.platformName - Platform name
 * @param {string} options.binaryName - Binary filename
 * @returns {Promise<void>}
 */
export async function copyLocalBinary({ sourcePath, vendorDir, platformName, binaryName }) {
  const platformDir = path.join(vendorDir, platformName, 'pipeline-kit');
  const destPath = path.join(platformDir, binaryName);

  console.log('Development mode: Installing Pipeline Kit binary from local build...');
  console.log(`  Source: ${sourcePath}`);
  console.log(`  Destination: ${destPath}`);
  console.log(`  Platform: ${platformName}`);

  // Check if source binary exists
  if (!fs.existsSync(sourcePath)) {
    throw new Error(
      `Binary not found at ${sourcePath}\n` +
      `\n` +
      `For development, build the Rust binary first:\n` +
      `  cd pipeline-kit-rs\n` +
      `  cargo build --release\n` +
      `\n` +
      `For production installation, use: NODE_ENV=production npm install`
    );
  }

  // Create platform directory
  fs.mkdirSync(platformDir, { recursive: true });

  // Copy binary
  fs.copyFileSync(sourcePath, destPath);

  // Make binary executable (Unix systems)
  if (process.platform !== 'win32') {
    fs.chmodSync(destPath, 0o755);
  }

  console.log('Binary installed successfully.');
}

/**
 * Main install function
 * @param {object} [options] - Optional configuration for testing
 * @param {string} [options.mode] - Installation mode ('production' or 'development')
 * @param {string} [options.cliRoot] - CLI root directory path
 * @param {string} [options.platform] - Platform override (for testing)
 * @param {string} [options.arch] - Architecture override (for testing)
 * @returns {Promise<void>}
 */
export async function install(options = {}) {
  // Determine installation mode
  const mode = options.mode || (process.env.NODE_ENV === 'production' ? 'production' : 'development');

  // Determine paths
  const cliRoot = options.cliRoot || path.resolve(__dirname, '..');
  const vendorDir = path.join(cliRoot, 'vendor');

  // Detect platform
  const platform = options.platform || process.platform;
  const arch = options.arch || process.arch;
  const { platformName, binaryName } = detectPlatform(platform, arch);

  if (mode === 'production') {
    // Production mode: download from GitHub Releases
    const repo = process.env.PIPELINE_KIT_REPO || 'Vooster-AI/pipeline-kit';
    const version = process.env.PIPELINE_KIT_VERSION || 'latest';

    const url = buildDownloadUrl({ repo, version, platformName });

    await downloadAndExtract({
      url,
      vendorDir,
      platformName,
      binaryName,
      showProgress: !options.mode // Only show progress in real execution, not tests
    });
  } else {
    // Development mode: copy from local Rust build
    const rustBuildDir = path.join(cliRoot, '..', 'pipeline-kit-rs', 'target', 'release');
    const sourcePath = path.join(rustBuildDir, binaryName);

    try {
      await copyLocalBinary({
        sourcePath,
        vendorDir,
        platformName,
        binaryName
      });
    } catch (error) {
      // In development mode, show warning but don't fail
      // This allows npm install to complete even without a built binary
      console.warn('\nWarning:', error.message);
      console.warn('Installation will continue, but the binary will not be available until built.\n');
    }
  }
}

// Run install if this script is executed directly (not imported as a module)
if (import.meta.url === `file://${process.argv[1]}`) {
  install().catch((error) => {
    console.error('\nError during installation:', error.message);
    process.exit(1);
  });
}
</file>

<file path="pipeline-kit-cli/scripts/test_install_native_deps.sh">
#!/usr/bin/env bash
# Test script for install_native_deps.sh
# This script validates that the production mode downloads binaries from GitHub Releases

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CLI_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
VENDOR_DIR="$CLI_ROOT/vendor"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo "=== Testing install_native_deps.sh ==="
echo ""

# Test 1: Helper function for platform name detection
echo -e "${YELLOW}Test 1: Platform name detection${NC}"
source "$SCRIPT_DIR/install_native_deps.sh" 2>/dev/null || true

# Test platform detection for various inputs
test_platform_detection() {
  local platform=$1
  local arch=$2
  local expected=$3

  result=$(get_platform_name "$platform" "$arch" || echo "FAILED")
  if [ "$result" = "$expected" ]; then
    echo -e "${GREEN}PASS: get_platform_name($platform, $arch) = $result${NC}"
    return 0
  else
    echo -e "${RED}FAIL: get_platform_name($platform, $arch) = $result (expected $expected)${NC}"
    return 1
  fi
}

test_platform_detection "darwin" "arm64" "macos-aarch64"
test_platform_detection "darwin" "x86_64" "macos-x86_64"
test_platform_detection "linux" "x86_64" "linux-x86_64"
test_platform_detection "linux" "aarch64" "linux-aarch64"

echo ""

# Test 2: Development mode (existing behavior)
echo -e "${YELLOW}Test 2: Development mode${NC}"
echo "Expected: Script should use local build if available"

# Check if local build exists
RUST_BUILD_DIR="$CLI_ROOT/../pipeline-kit-rs/target/release"
PLATFORM="$(uname -s | tr '[:upper:]' '[:lower:]')"
if [ "$PLATFORM" = "darwin" ]; then
  BINARY_NAME="pipeline"
else
  BINARY_NAME="pipeline"
fi

if [ -f "$RUST_BUILD_DIR/$BINARY_NAME" ]; then
  # Clean vendor directory
  rm -rf "$VENDOR_DIR"
  mkdir -p "$VENDOR_DIR"

  # Run in development mode
  unset NODE_ENV
  bash "$SCRIPT_DIR/install_native_deps.sh"

  # Check if binary was copied
  if [ -f "$VENDOR_DIR/"*"/pipeline-kit/$BINARY_NAME" ]; then
    echo -e "${GREEN}PASS: Binary installed from local build${NC}"
  else
    echo -e "${RED}FAIL: Binary not found in vendor directory${NC}"
  fi
else
  echo -e "${YELLOW}SKIP: No local build available${NC}"
fi

echo ""

# Test 3: Production mode structure validation
echo -e "${YELLOW}Test 3: Production mode logic validation${NC}"
echo "This test validates the script structure without requiring actual GitHub releases"

# Validate script has production mode logic
if grep -q "MODE=\"production\"" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: Production mode variable defined${NC}"
else
  echo -e "${RED}FAIL: Production mode variable not found${NC}"
fi

if grep -q "gh release download" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: GitHub Release download command present${NC}"
else
  echo -e "${RED}FAIL: GitHub Release download command not found${NC}"
fi

if grep -q "NODE_ENV" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: NODE_ENV check present${NC}"
else
  echo -e "${RED}FAIL: NODE_ENV check not found${NC}"
fi

if grep -q "tar -xzf" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: Archive extraction logic present${NC}"
else
  echo -e "${RED}FAIL: Archive extraction logic not found${NC}"
fi

echo ""

# Test 4: Error handling
echo -e "${YELLOW}Test 4: Error handling validation${NC}"

if grep -q "command -v gh" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: gh CLI availability check present${NC}"
else
  echo -e "${RED}FAIL: gh CLI check not found${NC}"
fi

if grep -q "Failed to download" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: Download failure handling present${NC}"
else
  echo -e "${RED}FAIL: Download failure handling not found${NC}"
fi

if grep -q "Binary not found in archive" "$SCRIPT_DIR/install_native_deps.sh"; then
  echo -e "${GREEN}PASS: Archive validation present${NC}"
else
  echo -e "${RED}FAIL: Archive validation not found${NC}"
fi

echo ""

# Test 5: Mock production mode (requires gh CLI and mock setup)
echo -e "${YELLOW}Test 5: Production mode execution (integration)${NC}"
echo "Note: This test requires gh CLI and a published release"

if command -v gh &> /dev/null; then
  echo -e "${GREEN}INFO: gh CLI is installed${NC}"

  # Check if we can access the repository (but don't actually download)
  # This is just a connectivity check
  if gh release view --repo pipeline-kit/pipeline-kit 2>&1 | grep -q "release not found\|could not resolve"; then
    echo -e "${YELLOW}INFO: No releases published yet (expected for initial implementation)${NC}"
    echo "To test production mode fully:"
    echo "1. Publish a release with binaries using GitHub Actions"
    echo "2. Run: NODE_ENV=production bash scripts/install_native_deps.sh"
  else
    echo -e "${GREEN}INFO: Releases are accessible${NC}"
  fi
else
  echo -e "${YELLOW}SKIP: gh CLI not installed${NC}"
  echo "Install gh CLI to test production mode: https://cli.github.com/"
fi

echo ""
echo "=== Test Summary ==="
echo "All structure and logic tests completed successfully!"
echo ""
echo "Implementation status: GREEN (ready for use)"
echo ""
echo "Next steps for production validation:"
echo "1. Set up GitHub Actions to build and publish release binaries"
echo "2. Publish a test release with pipeline-kit-*.tar.gz files"
echo "3. Test actual download: NODE_ENV=production npm install"
echo ""
</file>

<file path="pipeline-kit-cli/test/install.test.js">
/**
 * Tests for the install.js script
 * These tests verify that the installation script correctly:
 * 1. Detects the platform and architecture
 * 2. Downloads binaries from GitHub releases in production mode
 * 3. Copies local binaries in development mode
 * 4. Extracts archives correctly
 * 5. Sets proper file permissions
 */

import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';
import fs from 'fs';
import path from 'path';
import os from 'os';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Import functions that will be tested
// These will be exported from install.js
let detectPlatform, buildDownloadUrl, downloadAndExtract, copyLocalBinary;

// Mock setup
let nock;

beforeEach(async () => {
  // Dynamically import nock for HTTP mocking
  const nockModule = await import('nock');
  nock = nockModule.default;

  // Clean all HTTP mocks
  nock.cleanAll();
});

afterEach(() => {
  // Clean all HTTP mocks
  if (nock) {
    nock.cleanAll();
  }

  // Clear module cache to get fresh imports
  vi.resetModules();
});

describe('install.js - Platform Detection', () => {
  it('should detect macOS ARM64 platform correctly', async () => {
    // This test will fail until install.js is created
    const installModule = await import('../scripts/install.js');
    detectPlatform = installModule.detectPlatform;

    const platform = detectPlatform('darwin', 'arm64');
    expect(platform).toEqual({
      platformName: 'macos-arm64',
      binaryName: 'pipeline'
    });
  });

  it('should detect macOS x64 platform correctly', async () => {
    const installModule = await import('../scripts/install.js');
    detectPlatform = installModule.detectPlatform;

    const platform = detectPlatform('darwin', 'x64');
    expect(platform).toEqual({
      platformName: 'macos-x64',
      binaryName: 'pipeline'
    });
  });

  it('should detect Linux x64 platform correctly', async () => {
    const installModule = await import('../scripts/install.js');
    detectPlatform = installModule.detectPlatform;

    const platform = detectPlatform('linux', 'x64');
    expect(platform).toEqual({
      platformName: 'linux-x64',
      binaryName: 'pipeline'
    });
  });

  it('should detect Windows x64 platform correctly', async () => {
    const installModule = await import('../scripts/install.js');
    detectPlatform = installModule.detectPlatform;

    const platform = detectPlatform('win32', 'x64');
    expect(platform).toEqual({
      platformName: 'windows-x64',
      binaryName: 'pipeline.exe'
    });
  });

  it('should throw error for unsupported platform', async () => {
    const installModule = await import('../scripts/install.js');
    detectPlatform = installModule.detectPlatform;

    expect(() => detectPlatform('freebsd', 'x64')).toThrow('Unsupported platform');
  });
});

describe('install.js - Download URL Construction', () => {
  it('should build correct GitHub release URL for latest version', async () => {
    const installModule = await import('../scripts/install.js');
    buildDownloadUrl = installModule.buildDownloadUrl;

    const url = buildDownloadUrl({
      repo: 'Vooster-AI/pipeline-kit',
      version: 'latest',
      platformName: 'macos-arm64'
    });

    expect(url).toBe(
      'https://github.com/Vooster-AI/pipeline-kit/releases/latest/download/pipeline-kit-macos-arm64.tar.gz'
    );
  });

  it('should build correct GitHub release URL for specific version', async () => {
    const installModule = await import('../scripts/install.js');
    buildDownloadUrl = installModule.buildDownloadUrl;

    const url = buildDownloadUrl({
      repo: 'Vooster-AI/pipeline-kit',
      version: 'v0.1.0',
      platformName: 'linux-x64'
    });

    expect(url).toBe(
      'https://github.com/Vooster-AI/pipeline-kit/releases/download/v0.1.0/pipeline-kit-linux-x64.tar.gz'
    );
  });
});

describe('install.js - Binary Download and Extraction (Production Mode)', () => {
  it('should handle download errors gracefully', async () => {
    // Mock HTTP request that fails
    nock('https://github.com')
      .get('/Vooster-AI/pipeline-kit/releases/latest/download/pipeline-kit-macos-arm64.tar.gz')
      .reply(404, 'Not Found');

    const installModule = await import('../scripts/install.js');
    downloadAndExtract = installModule.downloadAndExtract;

    // Use a real temp directory for this test (mock-fs doesn't work well with tar extraction)
    const tempDir = fs.mkdtempSync(path.join(os.tmpdir(), 'test-install-'));

    try {
      await expect(async () => {
        await downloadAndExtract({
          url: 'https://github.com/Vooster-AI/pipeline-kit/releases/latest/download/pipeline-kit-macos-arm64.tar.gz',
          vendorDir: tempDir,
          platformName: 'macos-arm64',
          binaryName: 'pipeline',
          showProgress: false
        });
      }).rejects.toThrow();
    } finally {
      // Clean up
      fs.rmSync(tempDir, { recursive: true, force: true });
    }
  });
});

describe('install.js - Local Binary Copy (Development Mode)', () => {
  it('should copy local binary from Rust build directory', async () => {
    // Use real temp directories (mock-fs doesn't work well with tar and axios)
    const tempRoot = fs.mkdtempSync(path.join(os.tmpdir(), 'test-install-'));
    const sourceDir = path.join(tempRoot, 'pipeline-kit-rs', 'target', 'release');
    const vendorDir = path.join(tempRoot, 'vendor');

    try {
      // Create source directory and fake binary
      fs.mkdirSync(sourceDir, { recursive: true });
      fs.writeFileSync(path.join(sourceDir, 'pipeline'), 'fake-binary-content');

      const installModule = await import('../scripts/install.js');
      copyLocalBinary = installModule.copyLocalBinary;

      await copyLocalBinary({
        sourcePath: path.join(sourceDir, 'pipeline'),
        vendorDir,
        platformName: 'macos-arm64',
        binaryName: 'pipeline'
      });

      // Verify binary was copied
      const destPath = path.join(vendorDir, 'macos-arm64', 'pipeline-kit', 'pipeline');
      expect(fs.existsSync(destPath)).toBe(true);

      // Verify file permissions (should be executable on Unix)
      if (process.platform !== 'win32') {
        const stats = fs.statSync(destPath);
        // Check if user execute bit is set (0o100)
        expect((stats.mode & 0o100) !== 0).toBe(true);
      }
    } finally {
      // Clean up
      fs.rmSync(tempRoot, { recursive: true, force: true });
    }
  });

  it('should create vendor directory if it does not exist', async () => {
    const tempRoot = fs.mkdtempSync(path.join(os.tmpdir(), 'test-install-'));
    const sourceDir = path.join(tempRoot, 'source');
    const vendorDir = path.join(tempRoot, 'vendor');

    try {
      // Create source directory with binary, but NOT vendor directory
      fs.mkdirSync(sourceDir, { recursive: true });
      fs.writeFileSync(path.join(sourceDir, 'pipeline'), 'fake-binary-content');

      const installModule = await import('../scripts/install.js');
      copyLocalBinary = installModule.copyLocalBinary;

      await copyLocalBinary({
        sourcePath: path.join(sourceDir, 'pipeline'),
        vendorDir,
        platformName: 'macos-arm64',
        binaryName: 'pipeline'
      });

      // Verify directory structure was created
      expect(fs.existsSync(path.join(vendorDir, 'macos-arm64', 'pipeline-kit'))).toBe(true);
    } finally {
      // Clean up
      fs.rmSync(tempRoot, { recursive: true, force: true });
    }
  });

  it('should handle missing source binary gracefully', async () => {
    const tempRoot = fs.mkdtempSync(path.join(os.tmpdir(), 'test-install-'));

    try {
      const installModule = await import('../scripts/install.js');
      copyLocalBinary = installModule.copyLocalBinary;

      await expect(async () => {
        await copyLocalBinary({
          sourcePath: path.join(tempRoot, 'nonexistent', 'pipeline'),
          vendorDir: path.join(tempRoot, 'vendor'),
          platformName: 'macos-arm64',
          binaryName: 'pipeline'
        });
      }).rejects.toThrow();
    } finally {
      // Clean up
      fs.rmSync(tempRoot, { recursive: true, force: true });
    }
  });
});

describe('install.js - Integration Test', () => {
  it('should successfully install binary in development mode (acceptance test)', async () => {
    // This is the main acceptance test that validates the entire flow
    // Use real temp directories
    const tempRoot = fs.mkdtempSync(path.join(os.tmpdir(), 'test-install-'));
    const cliRoot = path.join(tempRoot, 'pipeline-kit-cli');
    const rustBuildDir = path.join(tempRoot, 'pipeline-kit-rs', 'target', 'release');

    try {
      // Setup directory structure with source binary
      fs.mkdirSync(rustBuildDir, { recursive: true });
      fs.writeFileSync(path.join(rustBuildDir, 'pipeline'), 'mock-binary-content');
      fs.mkdirSync(cliRoot, { recursive: true });

      // Import the main install function
      const installModule = await import('../scripts/install.js');
      const install = installModule.install;

      // Run installation in development mode
      await install({
        mode: 'development',
        cliRoot,
        platform: 'darwin',
        arch: 'arm64'
      });

      // Verify binary exists in correct location
      const binaryPath = path.join(
        cliRoot,
        'vendor',
        'macos-arm64',
        'pipeline-kit',
        'pipeline'
      );

      expect(fs.existsSync(binaryPath)).toBe(true);

      // Verify binary is executable on Unix
      if (process.platform !== 'win32') {
        const stats = fs.statSync(binaryPath);
        expect((stats.mode & 0o100) !== 0).toBe(true);
      }
    } finally {
      // Clean up
      fs.rmSync(tempRoot, { recursive: true, force: true });
    }
  });
});
</file>

<file path="pipeline-kit-cli/test/integration.test.js">
/**
 * Integration tests for the CLI wrapper.
 * RED phase: These tests verify end-to-end binary execution.
 */

import { describe, it, expect, beforeAll } from 'vitest';
import { spawn } from 'child_process';
import { existsSync } from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Path to the CLI entry point
const cliPath = path.join(__dirname, '..', 'bin', 'pipeline-kit.js');

// Helper function to execute the CLI and capture output
// Note: The CLI uses stdio: 'inherit', so we need to modify the approach
function executeCli(args = [], options = {}) {
  return new Promise((resolve, reject) => {
    // We need to use pipe for stdio to capture output
    const child = spawn('node', [cliPath, ...args], {
      ...options,
      stdio: ['pipe', 'pipe', 'pipe'], // Override inherit to capture output
      env: { ...process.env, ...options.env },
    });

    let stdout = '';
    let stderr = '';

    if (child.stdout) {
      child.stdout.on('data', (data) => {
        stdout += data.toString();
      });
    }

    if (child.stderr) {
      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });
    }

    child.on('error', (err) => {
      reject(err);
    });

    child.on('exit', (code, signal) => {
      resolve({
        exitCode: code,
        signal,
        stdout,
        stderr,
      });
    });
  });
}

describe('CLI Integration Tests', () => {
  beforeAll(() => {
    // Verify the CLI script exists
    expect(existsSync(cliPath)).toBe(true);
  });

  describe('Binary execution', () => {
    it('should successfully spawn and execute the binary', async () => {
      // Test that the binary can be spawned
      const result = await executeCli(['--help']);

      // The binary should execute (exit code might vary based on implementation)
      expect(result.exitCode).toBeDefined();
      expect(typeof result.exitCode).toBe('number');
    }, 10000);

    it('should handle command-line arguments', async () => {
      // Test that arguments are passed through
      const result = await executeCli(['--help']);

      // Should complete execution
      expect(result.exitCode !== null).toBe(true);
    }, 10000);
  });

  describe('Error handling', () => {
    it('should handle invalid commands gracefully', async () => {
      const result = await executeCli(['--invalid-flag-that-does-not-exist']);

      // Should exit (with success or error code)
      expect(result.exitCode !== null).toBe(true);
    }, 10000);

    it('should provide error message if binary is not found', async () => {
      // This test verifies the error handling structure
      // Skip if the binary actually exists (which is the normal case)
      const vendorRoot = path.join(__dirname, '..', 'vendor');
      const devBinaryPath = path.join(__dirname, '..', '..', 'pipeline-kit-rs', 'target', 'release', 'pipeline');

      if (!existsSync(vendorRoot) && !existsSync(devBinaryPath)) {
        const result = await executeCli([]);

        // Should exit with error
        expect(result.exitCode).not.toBe(0);

        // Should have error message
        expect(result.stderr).toContain('Error');
      } else {
        // Binary exists, so this test doesn't apply
        expect(true).toBe(true);
      }
    }, 10000);
  });

  describe('Signal forwarding', () => {
    it('should handle process termination gracefully', async () => {
      // Start a long-running command (if available) and terminate it
      // This is a simplified test - in practice, the CLI should forward signals

      const child = spawn('node', [cliPath, '--help'], {
        stdio: ['pipe', 'pipe', 'pipe'],
        env: process.env,
      });

      let exited = false;
      child.on('exit', () => {
        exited = true;
      });

      // Wait a bit and then kill
      await new Promise(resolve => setTimeout(resolve, 100));

      if (!exited) {
        child.kill('SIGTERM');
        await new Promise(resolve => setTimeout(resolve, 500));
      }

      // Should have exited
      expect(exited || child.killed).toBe(true);
    }, 10000);
  });

  describe('Environment variable passing', () => {
    it('should set PIPELINE_KIT_MANAGED_BY_NPM environment variable', async () => {
      // We can't directly test this without modifying the binary,
      // but we can verify the CLI script is structured correctly
      // by checking that it spawns with environment variables

      // This is more of a structural test
      expect(cliPath).toBeTruthy();
      expect(existsSync(cliPath)).toBe(true);
    });
  });
});

describe('Platform detection in real environment', () => {
  it('should correctly detect current platform', () => {
    const { platform, arch } = process;

    // Just verify we're running on a supported configuration
    const supportedPlatforms = ['darwin', 'linux', 'win32', 'android'];
    const supportedArchs = ['x64', 'arm64'];

    if (supportedPlatforms.includes(platform)) {
      expect(supportedArchs).toContain(arch);
    }
  });
});

describe('Binary path resolution', () => {
  it('should resolve to either vendor or dev binary', () => {
    const vendorRoot = path.join(__dirname, '..', 'vendor');
    const devBinaryPath = path.join(__dirname, '..', '..', 'pipeline-kit-rs', 'target', 'release', 'pipeline');

    // At least one should exist for tests to pass
    const hasVendor = existsSync(vendorRoot);
    const hasDevBinary = existsSync(devBinaryPath);

    expect(hasVendor || hasDevBinary).toBe(true);
  });
});
</file>

<file path="pipeline-kit-cli/.npmignore">
# Development files
test/
*.test.js
vitest.config.js
.vitest/

# Dependencies
node_modules/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Misc
.env
.env.local
.env.*.local
</file>

<file path="pipeline-kit-cli/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "lib": ["ES2020"],
    "allowJs": true,
    "checkJs": false,
    "noEmit": true,
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true
  },
  "include": [
    "bin/**/*",
    "lib/**/*",
    "src/**/*",
    "test/**/*"
  ],
  "exclude": [
    "node_modules",
    "coverage",
    "vendor"
  ]
}
</file>

<file path="pipeline-kit-rs/.cargo/audit.toml">
# cargo-audit configuration for pipeline-kit-rs
#
# This file configures which advisories to ignore during security audits.
# Only ignore advisories that are:
# 1. Non-critical (e.g., "unmaintained" warnings)
# 2. From transitive dependencies we cannot easily replace
# 3. Have no actual security vulnerabilities

[advisories]
# Ignore RUSTSEC-2024-0436: paste crate marked as unmaintained
#
# Rationale:
# - This is a "warning" level advisory, not a security vulnerability
# - The paste crate is a transitive dependency from ratatui 0.29
# - No security issues have been identified, only maintenance status
# - Upgrading to ratatui 0.30 requires using alpha releases (not stable)
# - The paste crate is a proc-macro used only at compile time
#
# Decision: Safe to ignore until ratatui 0.30 stable is released or
# a security vulnerability is discovered in paste.
#
# Tracking: Will revisit when ratatui 0.30 stable is available
ignore = [
    "RUSTSEC-2024-0436",  # paste - unmaintained (proc-macro, compile-time only)
]
</file>

<file path="pipeline-kit-rs/.config/nextest.toml">
# Nextest configuration for pipeline-kit-rs
#
# This file configures cargo-nextest test runner behavior.
# See: https://nexte.st/book/configuration.html

# Default profile (used for local development)
[profile.default]
# Continue running tests even if some fail
fail-fast = false
# Show output for failing tests immediately
failure-output = "immediate-final"
# Don't show output for passing tests
success-output = "never"

# CI profile (used in GitHub Actions)
[profile.ci]
# Never stop early, run all tests to collect complete results
fail-fast = false
# Hide output from successful tests to reduce CI log noise
success-output = "never"
# Show failure output immediately as tests fail for faster debugging
failure-output = "immediate-final"
# Number of retries for flaky tests (0 = no retries in CI)
retries = 0

# JUnit XML output configuration for CI test reporting
[profile.ci.junit]
# Path where JUnit XML report will be generated
# This is uploaded as an artifact in GitHub Actions for test result visualization
path = "target/nextest/ci/junit.xml"
# Store output for failed tests in the XML report
store-success-output = false
store-failure-output = true
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/qwen_adapter.rs">
//! Qwen adapter implementation using Qwen CLI via JSON-RPC (ACP Protocol).
//!
//! This is an MVP implementation following a simplified architecture:
//! - Uses notification-only JSON-RPC (no response matching)
//! - No reverse request handlers (permissions ignored for now)
//! - No buffering (immediate event yielding)
//! - Fresh session per execution
//!
//! Future phases will add:
//! - Phase 2: Permission auto-approval, session reuse
//! - Phase 3: Buffer management, complete reverse request handlers

use crate::agents::base::{Agent, AgentError, AgentEvent, ExecutionContext};
use async_trait::async_trait;
// Allow: Serialize will be used in Phase 2 for JSON-RPC request serialization
#[allow(unused_imports)]
use serde::{Deserialize, Serialize};
use std::pin::Pin;
// Allow: Stdio, tokio types will be used in Phase 2 for process spawning and I/O
#[allow(unused_imports)]
use std::process::Stdio;
#[allow(unused_imports)]
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader};
#[allow(unused_imports)]
use tokio::process::{Child, Command};
use tokio_stream::Stream;

/// Minimal JSON-RPC notification structure (no id field = notification).
// Allow: Will be used in Phase 2 for parsing stdout notifications
#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct Notification {
    method: String,
    params: serde_json::Value,
}

/// Session update parameters from session/update notifications.
// Allow: Will be used in Phase 2 for parsing session/update notifications
#[allow(dead_code)]
#[derive(Debug, Deserialize)]
struct SessionUpdate {
    #[serde(rename = "sessionUpdate")]
    kind: Option<String>,
    #[serde(rename = "type")]
    type_field: Option<String>,
    text: Option<String>,
    content: Option<serde_json::Value>,
}

/// Qwen adapter for executing instructions using Qwen CLI.
///
/// This adapter spawns the `qwen` CLI process with `--experimental-acp` flag
/// and communicates via JSON-RPC protocol over stdin/stdout.
pub struct QwenAdapter {
    #[allow(dead_code)]
    // Allow: Public API field, may be used by external consumers
    name: String,
    // Allow: Will be used in Phase 2 for qwen CLI invocation
    #[allow(dead_code)]
    model: String,
    // Allow: Will be used in Phase 2 for QWEN.md content and initialization
    #[allow(dead_code)]
    system_prompt: String,
}

impl QwenAdapter {
    /// Create a new Qwen adapter.
    ///
    /// # Arguments
    ///
    /// * `name` - The agent name from configuration
    /// * `model` - The Qwen model to use (e.g., "qwen-coder", "qwen2.5-coder")
    /// * `system_prompt` - The system prompt for the agent
    pub fn new(name: String, model: String, system_prompt: String) -> Result<Self, AgentError> {
        Ok(Self {
            name,
            model,
            system_prompt,
        })
    }

    /// Resolve the qwen CLI command.
    ///
    /// Checks in the following order:
    /// 1. QWEN_CMD environment variable
    /// 2. `qwen` in PATH
    /// 3. `qwen-code` in PATH
    fn resolve_qwen_command(&self) -> Result<String, AgentError> {
        // Check QWEN_CMD environment variable
        if let Ok(cmd) = std::env::var("QWEN_CMD") {
            if which::which(&cmd).is_ok() {
                return Ok(cmd);
            }
        }

        // Check standard command names
        for cmd in &["qwen", "qwen-code"] {
            if which::which(cmd).is_ok() {
                return Ok(cmd.to_string());
            }
        }

        Err(AgentError::NotAvailable(
            "Qwen CLI not found. Install 'qwen' or set QWEN_CMD environment variable".to_string(),
        ))
    }

    /// Ensure QWEN.md file exists in the project root.
    ///
    /// Qwen CLI uses QWEN.md for system prompts.
    async fn ensure_qwen_md(&self, project_path: &str) -> Result<(), AgentError> {
        let qwen_md_path = std::path::Path::new(project_path).join("QWEN.md");

        // Skip if already exists
        if qwen_md_path.exists() {
            return Ok(());
        }

        // Write system prompt to QWEN.md with a header
        let content = format!("# QWEN\n\n{}", self.system_prompt);
        tokio::fs::write(&qwen_md_path, content)
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to create QWEN.md: {}", e)))?;

        Ok(())
    }

    /// Send a JSON-RPC request to stdin (response is not awaited in MVP).
    async fn send_request(
        stdin: &mut tokio::process::ChildStdin,
        request: &serde_json::Value,
    ) -> Result<(), AgentError> {
        let json = serde_json::to_string(request)
            .map_err(|e| AgentError::ExecutionError(format!("JSON serialize error: {}", e)))?;

        stdin
            .write_all(json.as_bytes())
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write to stdin: {}", e)))?;
        stdin
            .write_all(b"\n")
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write newline: {}", e)))?;
        stdin
            .flush()
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to flush stdin: {}", e)))?;

        Ok(())
    }

    /// Create a stream that reads notifications from stdout and converts them to AgentEvents.
    fn create_notification_stream(
        stdout: tokio::process::ChildStdout,
    ) -> impl Stream<Item = Result<AgentEvent, AgentError>> {
        async_stream::stream! {
            let reader = BufReader::new(stdout);
            let mut lines = reader.lines();

            while let Ok(Some(line)) = lines.next_line().await {
                if line.trim().is_empty() {
                    continue;
                }

                // Parse JSON
                let value: serde_json::Value = match serde_json::from_str(&line) {
                    Ok(v) => v,
                    Err(e) => {
                        eprintln!("[qwen] Failed to parse JSON: {} (line: {})", e, line);
                        continue;
                    }
                };

                // Only process notifications (has method, no id)
                if value.get("method").is_some() && value.get("id").is_none() {
                    let notification: Notification = match serde_json::from_value(value) {
                        Ok(n) => n,
                        Err(e) => {
                            eprintln!("[qwen] Failed to parse notification: {}", e);
                            continue;
                        }
                    };

                    // Only handle session/update notifications
                    if notification.method == "session/update" {
                        if let Some(event) = Self::convert_update(notification.params) {
                            yield Ok(event);
                        }
                    }
                }
                // Ignore requests/responses (reverse requests are not handled in MVP)
            }

            yield Ok(AgentEvent::Completed);
        }
    }

    /// Convert a session/update notification to an AgentEvent.
    fn convert_update(params: serde_json::Value) -> Option<AgentEvent> {
        // Clone params before parsing to allow reuse for tool_call
        let params_clone = params.clone();
        let update: SessionUpdate = serde_json::from_value(params).ok()?;

        // Extract kind from either `sessionUpdate` or `type` field
        let kind = update.kind.or(update.type_field)?;

        match kind.as_str() {
            "agent_message_chunk" | "agent_thought_chunk" => {
                // Extract text content
                let text = update.text.or_else(|| {
                    update
                        .content
                        .as_ref()
                        .and_then(|c| c.get("text"))
                        .and_then(|t| t.as_str())
                        .map(|s| s.to_string())
                })?;

                Some(AgentEvent::MessageChunk(text))
            }
            "tool_call" => {
                // For tool calls, return the entire params as JSON string
                Some(AgentEvent::ToolCall(params_clone.to_string()))
            }
            _ => {
                // Ignore other event types (tool_call_update, plan, etc.)
                None
            }
        }
    }
}

#[async_trait]
impl Agent for QwenAdapter {
    async fn check_availability(&self) -> bool {
        self.resolve_qwen_command().is_ok()
    }

    async fn execute(
        &self,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        // 1. Ensure QWEN.md exists
        self.ensure_qwen_md(&context.project_path).await?;

        // 2. Resolve qwen command
        let cmd = self.resolve_qwen_command()?;

        // 3. Spawn qwen process with --experimental-acp flag
        let mut child = Command::new(&cmd)
            .arg("--experimental-acp")
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(&context.project_path)
            .spawn()
            .map_err(|e| {
                AgentError::ExecutionError(format!("Failed to spawn qwen process: {}", e))
            })?;

        let mut stdin = child
            .stdin
            .take()
            .ok_or_else(|| AgentError::ExecutionError("Failed to capture stdin".to_string()))?;

        let stdout = child
            .stdout
            .take()
            .ok_or_else(|| AgentError::ExecutionError("Failed to capture stdout".to_string()))?;

        // 4. Send initialize request
        let init_req = serde_json::json!({
            "jsonrpc": "2.0",
            "id": 1,
            "method": "initialize",
            "params": {
                "clientCapabilities": {
                    "fs": {"readTextFile": false, "writeTextFile": false}
                },
                "protocolVersion": 1
            }
        });
        Self::send_request(&mut stdin, &init_req).await?;

        // 5. Send session/prompt request
        let prompt_req = serde_json::json!({
            "jsonrpc": "2.0",
            "id": 2,
            "method": "session/prompt",
            "params": {
                "prompt": [{
                    "type": "text",
                    "text": context.instruction.clone()
                }]
            }
        });
        Self::send_request(&mut stdin, &prompt_req).await?;

        // 6. Create and return notification stream
        let stream = Self::create_notification_stream(stdout);
        Ok(Box::pin(stream))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_qwen_adapter_new() {
        let adapter = QwenAdapter::new(
            "test-qwen".to_string(),
            "qwen-coder".to_string(),
            "You are a helpful assistant.".to_string(),
        );
        assert!(adapter.is_ok());

        let adapter = adapter.unwrap();
        assert_eq!(adapter.model, "qwen-coder");
        assert_eq!(adapter.system_prompt, "You are a helpful assistant.");
    }

    #[test]
    fn test_resolve_qwen_command() {
        let adapter = QwenAdapter::new(
            "test".to_string(),
            "qwen-coder".to_string(),
            "prompt".to_string(),
        )
        .unwrap();

        // Result depends on environment, just ensure it doesn't panic
        let _ = adapter.resolve_qwen_command();
    }

    #[tokio::test]
    async fn test_check_availability() {
        let adapter = QwenAdapter::new(
            "test".to_string(),
            "qwen-coder".to_string(),
            "prompt".to_string(),
        )
        .unwrap();

        // Will return false unless qwen CLI is installed
        // Don't assert the result, just ensure it doesn't panic
        let _ = adapter.check_availability().await;
    }

    #[tokio::test]
    async fn test_ensure_qwen_md() {
        let adapter = QwenAdapter::new(
            "test".to_string(),
            "qwen-coder".to_string(),
            "You are a test assistant.".to_string(),
        )
        .unwrap();

        let temp_dir = tempfile::tempdir().unwrap();
        let project_path = temp_dir.path().to_str().unwrap();

        // First call should create the file
        let result = adapter.ensure_qwen_md(project_path).await;
        assert!(result.is_ok());

        // Verify file was created
        let qwen_md_path = temp_dir.path().join("QWEN.md");
        assert!(qwen_md_path.exists());

        let content = tokio::fs::read_to_string(&qwen_md_path).await.unwrap();
        assert!(content.contains("# QWEN"));
        assert!(content.contains("You are a test assistant."));

        // Second call should be idempotent (no error)
        let result2 = adapter.ensure_qwen_md(project_path).await;
        assert!(result2.is_ok());
    }

    #[test]
    fn test_notification_deserialization() {
        let json = r#"{
            "method": "session/update",
            "params": {
                "sessionUpdate": "agent_message_chunk",
                "text": "Hello"
            }
        }"#;

        let notification: Result<Notification, _> = serde_json::from_str(json);
        assert!(notification.is_ok());

        let notification = notification.unwrap();
        assert_eq!(notification.method, "session/update");
    }

    #[test]
    fn test_session_update_deserialization() {
        let json = r#"{
            "sessionUpdate": "agent_message_chunk",
            "text": "Hello, world!"
        }"#;

        let update: Result<SessionUpdate, _> = serde_json::from_str(json);
        assert!(update.is_ok());

        let update = update.unwrap();
        assert_eq!(update.kind, Some("agent_message_chunk".to_string()));
        assert_eq!(update.text, Some("Hello, world!".to_string()));
    }

    #[test]
    fn test_session_update_with_type_field() {
        let json = r#"{
            "type": "tool_call",
            "content": {"name": "write"}
        }"#;

        let update: Result<SessionUpdate, _> = serde_json::from_str(json);
        assert!(update.is_ok());

        let update = update.unwrap();
        assert_eq!(update.type_field, Some("tool_call".to_string()));
        assert!(update.content.is_some());
    }

    #[test]
    fn test_convert_update_message_chunk() {
        let params = serde_json::json!({
            "sessionUpdate": "agent_message_chunk",
            "text": "Hello from Qwen!"
        });

        let event = QwenAdapter::convert_update(params);
        assert!(event.is_some());

        match event.unwrap() {
            AgentEvent::MessageChunk(text) => assert_eq!(text, "Hello from Qwen!"),
            _ => panic!("Expected MessageChunk"),
        }
    }

    #[test]
    fn test_convert_update_thought_chunk() {
        let params = serde_json::json!({
            "type": "agent_thought_chunk",
            "text": "Thinking..."
        });

        let event = QwenAdapter::convert_update(params);
        assert!(event.is_some());

        match event.unwrap() {
            AgentEvent::MessageChunk(text) => assert_eq!(text, "Thinking..."),
            _ => panic!("Expected MessageChunk"),
        }
    }

    #[test]
    fn test_convert_update_tool_call() {
        let params = serde_json::json!({
            "type": "tool_call",
            "name": "write",
            "input": {"path": "test.txt", "content": "Hello"}
        });

        let event = QwenAdapter::convert_update(params.clone());
        assert!(event.is_some());

        match event.unwrap() {
            AgentEvent::ToolCall(json) => {
                assert!(json.contains("tool_call"));
            }
            _ => panic!("Expected ToolCall"),
        }
    }

    #[test]
    fn test_convert_update_unknown_type() {
        let params = serde_json::json!({
            "type": "unknown_event",
            "data": "something"
        });

        let event = QwenAdapter::convert_update(params);
        assert!(event.is_none());
    }

    #[tokio::test]
    #[cfg(feature = "integration-tests")]
    async fn test_qwen_adapter_execute_integration() {
        use tokio_stream::StreamExt;

        let adapter = QwenAdapter::new(
            "test-qwen".to_string(),
            "qwen-coder".to_string(),
            "You are a helpful coding assistant.".to_string(),
        )
        .unwrap();

        // Skip test if qwen CLI is not installed
        if !adapter.check_availability().await {
            eprintln!("[test] Skipping integration test: qwen CLI not installed");
            return;
        }

        let temp_dir = tempfile::tempdir().unwrap();
        let project_path = temp_dir.path().to_str().unwrap();

        let context = ExecutionContext::new("Say hello in one word".to_string())
            .with_project_path(project_path.to_string());

        let mut stream = adapter.execute(&context).await.unwrap();
        let mut events = Vec::new();

        // Collect events with timeout to avoid hanging
        let timeout_duration = std::time::Duration::from_secs(30);
        let start = std::time::Instant::now();

        while let Some(result) = stream.next().await {
            if start.elapsed() > timeout_duration {
                eprintln!("[test] Timeout after 30 seconds");
                break;
            }

            match result {
                Ok(event) => {
                    eprintln!("[test] Event: {:?}", event);
                    let is_completed = matches!(event, AgentEvent::Completed);
                    events.push(event);
                    if is_completed {
                        break;
                    }
                }
                Err(e) => {
                    eprintln!("[test] Error: {}", e);
                    break;
                }
            }
        }

        // Verify that we got at least a Completed event
        assert!(!events.is_empty(), "Should receive at least one event");
        assert!(
            events.iter().any(|e| matches!(e, AgentEvent::Completed)),
            "Should receive a Completed event"
        );

        // Verify QWEN.md was created
        let qwen_md_path = temp_dir.path().join("QWEN.md");
        assert!(qwen_md_path.exists(), "QWEN.md should be created");
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/cli_executor.rs">
//! Common CLI subprocess executor for agent adapters.
//!
//! This module provides a unified interface for executing CLI-based agents
//! and parsing their JSON Lines / NDJSON output streams.

use crate::agents::base::AgentError;
use std::pin::Pin;
use std::process::Stdio;
use tokio::io::AsyncBufReadExt;
use tokio::io::BufReader;
use tokio::process::Command;
use tokio_stream::Stream;

/// CLI executor for agent adapters.
///
/// This struct provides common functionality for spawning subprocess-based
/// CLI tools and parsing their JSON Lines output.
pub struct CliExecutor;

impl CliExecutor {
    /// Execute a CLI command and parse its stdout as JSON Lines/NDJSON.
    ///
    /// # Arguments
    ///
    /// * `command` - The command to execute (e.g., "python3", "claude")
    /// * `args` - Command line arguments
    /// * `working_dir` - Working directory for the command
    ///
    /// # Returns
    ///
    /// A stream of `serde_json::Value` objects, one per line of JSON output.
    /// Empty lines are automatically filtered out. Lines that fail to parse
    /// as JSON will yield `AgentError::StreamParseError`.
    ///
    /// # Example
    ///
    /// ```no_run
    /// use pk_core::agents::cli_executor::CliExecutor;
    /// use tokio_stream::StreamExt;
    ///
    /// #[tokio::main]
    /// async fn main() {
    ///     let stream = CliExecutor::execute(
    ///         "echo".to_string(),
    ///         vec![r#"{"type":"test"}"#.to_string()],
    ///         ".".to_string(),
    ///     );
    ///
    ///     let values: Vec<_> = stream.collect().await;
    ///     println!("Got {} values", values.len());
    /// }
    /// ```
    pub fn execute(
        command: String,
        args: Vec<String>,
        working_dir: String,
    ) -> Pin<Box<dyn Stream<Item = Result<serde_json::Value, AgentError>> + Send>> {
        // Create async stream using async_stream::stream macro
        let stream = async_stream::stream! {
            // Build and spawn the command
            let mut cmd = Command::new(&command);
            cmd.args(&args);
            cmd.current_dir(&working_dir);
            cmd.stdout(Stdio::piped());
            cmd.stderr(Stdio::piped());

            let mut child = match cmd.spawn() {
                Ok(child) => child,
                Err(e) => {
                    yield Err(AgentError::ExecutionError(format!(
                        "Failed to spawn command '{}': {}",
                        command, e
                    )));
                    return;
                }
            };

            // Capture stdout
            let stdout = match child.stdout.take() {
                Some(stdout) => stdout,
                None => {
                    yield Err(AgentError::ExecutionError(
                        "Failed to capture stdout".to_string()
                    ));
                    return;
                }
            };

            // Create buffered reader for line-by-line processing
            let reader = BufReader::new(stdout);
            let mut lines = reader.lines();

            // Read lines and parse as JSON
            while let Ok(Some(line)) = lines.next_line().await {
                // Skip empty lines
                if line.trim().is_empty() {
                    continue;
                }

                // Try to parse as JSON
                match serde_json::from_str::<serde_json::Value>(&line) {
                    Ok(value) => {
                        yield Ok(value);
                    }
                    Err(e) => {
                        yield Err(AgentError::StreamParseError(format!(
                            "Failed to parse JSON: {} (line: {})",
                            e, line
                        )));
                    }
                }
            }

            // Wait for the process to complete
            // We don't need to check exit status here, as any output errors
            // will have been captured above
            let _ = child.wait().await;
        };

        Box::pin(stream)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio_stream::StreamExt;

    #[tokio::test]
    async fn test_execute_echo_json() {
        // Use echo to output a JSON line
        let stream = CliExecutor::execute(
            "echo".to_string(),
            vec![r#"{"type":"test","value":42}"#.to_string()],
            ".".to_string(),
        );

        let values: Vec<_> = stream
            .collect::<Vec<_>>()
            .await
            .into_iter()
            .collect::<Result<Vec<_>, _>>()
            .expect("Should parse JSON successfully");

        assert_eq!(values.len(), 1);
        assert_eq!(values[0].get("type").and_then(|v| v.as_str()), Some("test"));
        assert_eq!(values[0].get("value").and_then(|v| v.as_i64()), Some(42));
    }

    #[tokio::test]
    async fn test_execute_invalid_command() {
        let stream = CliExecutor::execute(
            "nonexistent-command-xyz".to_string(),
            vec![],
            ".".to_string(),
        );

        let results: Vec<_> = stream.collect::<Vec<_>>().await;

        assert!(!results.is_empty());
        assert!(results[0].is_err());

        if let Err(AgentError::ExecutionError(msg)) = &results[0] {
            assert!(msg.contains("Failed to spawn command"));
        } else {
            panic!("Expected ExecutionError");
        }
    }

    #[tokio::test]
    async fn test_execute_filters_empty_lines() {
        // Create a temporary script that outputs empty lines
        let temp_dir = tempfile::tempdir().unwrap();
        let script_path = temp_dir.path().join("test.py");
        std::fs::write(
            &script_path,
            r#"#!/usr/bin/env python3
import json
print()
print(json.dumps({"num": 1}))
print()
print(json.dumps({"num": 2}))
print()
"#,
        )
        .unwrap();

        #[cfg(unix)]
        {
            use std::os::unix::fs::PermissionsExt;
            std::fs::set_permissions(&script_path, std::fs::Permissions::from_mode(0o755)).unwrap();
        }

        let stream = CliExecutor::execute(
            "python3".to_string(),
            vec![script_path.to_str().unwrap().to_string()],
            temp_dir.path().to_str().unwrap().to_string(),
        );

        let values: Vec<_> = stream
            .collect::<Vec<_>>()
            .await
            .into_iter()
            .collect::<Result<Vec<_>, _>>()
            .expect("Should parse JSON successfully");

        // Should only get 2 values (empty lines filtered)
        assert_eq!(values.len(), 2);
        assert_eq!(values[0].get("num").and_then(|v| v.as_i64()), Some(1));
        assert_eq!(values[1].get("num").and_then(|v| v.as_i64()), Some(2));
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/config/mod.rs">
//! Configuration loading and management.
//!
//! This module provides functionality to load and parse all configuration files
//! from the `.pipeline-kit/` directory structure.

pub mod error;
pub mod loader;
pub mod models;
</file>

<file path="pipeline-kit-rs/crates/core/src/init/error.rs">
//! Error types for initialization operations.

use std::path::PathBuf;
use thiserror::Error;

/// Result type for initialization operations.
pub type InitResult<T> = Result<T, InitError>;

/// Errors that can occur during initialization.
#[derive(Debug, Error)]
pub enum InitError {
    /// The .pipeline-kit directory already exists and force flag was not set.
    #[error(".pipeline-kit directory already exists at {0:?}. Use --force to overwrite.")]
    DirectoryExists(PathBuf),

    /// A required template file was not found in embedded assets.
    #[error("Template file not found: {0}")]
    TemplateNotFound(String),

    /// Failed to create a directory.
    #[error("Failed to create directory {path:?}: {source}")]
    DirectoryCreate {
        path: PathBuf,
        source: std::io::Error,
    },

    /// Failed to write a file.
    #[error("Failed to write file {path:?}: {source}")]
    FileWrite {
        path: PathBuf,
        source: std::io::Error,
    },

    /// Generic I/O error.
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
}
</file>

<file path="pipeline-kit-rs/crates/core/src/init/generator.rs">
//! Directory structure and file generation for .pipeline-kit initialization.

use super::error::{InitError, InitResult};
use super::templates::{get_template, list_templates};
use std::fs;
use std::path::{Path, PathBuf};

/// Options for initializing a .pipeline-kit directory.
#[derive(Debug, Clone)]
pub struct InitOptions {
    /// Target directory where .pipeline-kit will be created.
    pub target_dir: PathBuf,

    /// Overwrite existing .pipeline-kit directory if it exists.
    pub force: bool,

    /// Create minimal template (only 1 agent and 1 pipeline).
    pub minimal: bool,
}

impl Default for InitOptions {
    fn default() -> Self {
        Self {
            target_dir: std::env::current_dir().unwrap_or_else(|_| PathBuf::from(".")),
            force: false,
            minimal: false,
        }
    }
}

/// Generate a complete .pipeline-kit directory structure with templates.
///
/// This function creates the following structure:
/// ```text
/// .pipeline-kit/
/// ‚îú‚îÄ‚îÄ config.toml
/// ‚îú‚îÄ‚îÄ agents/
/// ‚îÇ   ‚îú‚îÄ‚îÄ developer.md
/// ‚îÇ   ‚îî‚îÄ‚îÄ reviewer.md (unless minimal)
/// ‚îî‚îÄ‚îÄ pipelines/
///     ‚îú‚îÄ‚îÄ simple-task.yaml
///     ‚îî‚îÄ‚îÄ code-review.yaml (unless minimal)
/// ```
///
/// # Arguments
/// * `options` - Configuration for the initialization process
///
/// # Returns
/// `Ok(())` if successful, or an `InitError` if:
/// - The .pipeline-kit directory already exists (without force flag)
/// - A template file cannot be found
/// - File system operations fail
///
/// # Example
/// ```no_run
/// use pk_core::init::{InitOptions, generate_pipeline_kit_structure};
/// use std::path::PathBuf;
///
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// let options = InitOptions {
///     target_dir: PathBuf::from("."),
///     force: false,
///     minimal: false,
/// };
///
/// generate_pipeline_kit_structure(options).await?;
/// # Ok(())
/// # }
/// ```
pub async fn generate_pipeline_kit_structure(options: InitOptions) -> InitResult<()> {
    let pk_dir = options.target_dir.join(".pipeline-kit");

    // Check if directory exists
    if pk_dir.exists() && !options.force {
        return Err(InitError::DirectoryExists(pk_dir));
    }

    // Create directory structure
    fs::create_dir_all(pk_dir.join("agents")).map_err(|source| InitError::DirectoryCreate {
        path: pk_dir.join("agents"),
        source,
    })?;

    fs::create_dir_all(pk_dir.join("pipelines")).map_err(|source| InitError::DirectoryCreate {
        path: pk_dir.join("pipelines"),
        source,
    })?;

    // Generate config.toml
    write_template_file(&pk_dir, "config.toml")?;

    // Generate agent templates
    if options.minimal {
        // Only create developer agent
        write_template_file(&pk_dir, "agents/developer.md")?;
    } else {
        // Create all agent templates
        for agent_path in list_templates("agents/") {
            write_template_file(&pk_dir, &agent_path)?;
        }
    }

    // Generate pipeline templates
    if options.minimal {
        // Only create simple-task pipeline
        write_template_file(&pk_dir, "pipelines/simple-task.yaml")?;
    } else {
        // Create all pipeline templates
        for pipeline_path in list_templates("pipelines/") {
            write_template_file(&pk_dir, &pipeline_path)?;
        }
    }

    Ok(())
}

/// Helper function to write a template file to the target directory.
///
/// # Arguments
/// * `pk_dir` - The .pipeline-kit directory path
/// * `template_path` - Relative path of the template (e.g., "agents/developer.md")
///
/// # Returns
/// `Ok(())` if successful, or an `InitError` if the template is not found or writing fails.
fn write_template_file(pk_dir: &Path, template_path: &str) -> InitResult<()> {
    let content = get_template(template_path)
        .ok_or_else(|| InitError::TemplateNotFound(template_path.to_string()))?;

    let target_path = pk_dir.join(template_path);

    // Ensure parent directory exists
    if let Some(parent) = target_path.parent() {
        fs::create_dir_all(parent).map_err(|source| InitError::DirectoryCreate {
            path: parent.to_path_buf(),
            source,
        })?;
    }

    fs::write(&target_path, content).map_err(|source| InitError::FileWrite {
        path: target_path,
        source,
    })?;

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::tempdir;

    /// RED: This test defines our acceptance criteria and will fail initially.
    ///
    /// We create a complete `.pipeline-kit/` structure and verify all files are correct.
    #[tokio::test]
    async fn test_generate_structure_success() {
        let dir = tempdir().unwrap();
        let options = InitOptions {
            target_dir: dir.path().to_path_buf(),
            force: false,
            minimal: false,
        };

        let result = generate_pipeline_kit_structure(options).await;
        assert!(result.is_ok(), "Failed: {:?}", result.err());

        // Verify directory structure
        let pk_dir = dir.path().join(".pipeline-kit");
        assert!(pk_dir.exists(), ".pipeline-kit directory should exist");
        assert!(
            pk_dir.join("agents").exists(),
            "agents directory should exist"
        );
        assert!(
            pk_dir.join("pipelines").exists(),
            "pipelines directory should exist"
        );

        // Verify config.toml
        assert!(
            pk_dir.join("config.toml").exists(),
            "config.toml should exist"
        );
        let config = fs::read_to_string(pk_dir.join("config.toml")).unwrap();
        assert!(
            config.contains("git ="),
            "config should contain git setting"
        );

        // Verify agents
        assert!(
            pk_dir.join("agents/developer.md").exists(),
            "developer.md should exist"
        );
        assert!(
            pk_dir.join("agents/reviewer.md").exists(),
            "reviewer.md should exist"
        );

        let developer = fs::read_to_string(pk_dir.join("agents/developer.md")).unwrap();
        assert!(
            developer.contains("name: developer"),
            "developer should have correct frontmatter"
        );

        // Verify pipelines
        assert!(
            pk_dir.join("pipelines/simple-task.yaml").exists(),
            "simple-task.yaml should exist"
        );
        assert!(
            pk_dir.join("pipelines/code-review.yaml").exists(),
            "code-review.yaml should exist"
        );

        let simple_task = fs::read_to_string(pk_dir.join("pipelines/simple-task.yaml")).unwrap();
        assert!(
            simple_task.contains("name: simple-task"),
            "simple-task should have correct name"
        );
    }

    /// Test minimal mode generates only essential files.
    #[tokio::test]
    async fn test_generate_structure_minimal() {
        let dir = tempdir().unwrap();
        let options = InitOptions {
            target_dir: dir.path().to_path_buf(),
            force: false,
            minimal: true,
        };

        generate_pipeline_kit_structure(options).await.unwrap();

        let pk_dir = dir.path().join(".pipeline-kit");

        // Should have developer agent
        assert!(
            pk_dir.join("agents/developer.md").exists(),
            "developer.md should exist in minimal mode"
        );

        // Should NOT have reviewer agent
        assert!(
            !pk_dir.join("agents/reviewer.md").exists(),
            "reviewer.md should not exist in minimal mode"
        );

        // Should have simple-task pipeline
        assert!(
            pk_dir.join("pipelines/simple-task.yaml").exists(),
            "simple-task.yaml should exist in minimal mode"
        );

        // Should NOT have code-review pipeline
        assert!(
            !pk_dir.join("pipelines/code-review.yaml").exists(),
            "code-review.yaml should not exist in minimal mode"
        );
    }

    /// Test that existing directory without force flag returns error.
    #[tokio::test]
    async fn test_generate_structure_exists_without_force() {
        let dir = tempdir().unwrap();
        fs::create_dir_all(dir.path().join(".pipeline-kit")).unwrap();

        let options = InitOptions {
            target_dir: dir.path().to_path_buf(),
            force: false,
            minimal: false,
        };

        let result = generate_pipeline_kit_structure(options).await;
        assert!(result.is_err(), "Should fail when directory exists");
        assert!(
            matches!(result.unwrap_err(), InitError::DirectoryExists(_)),
            "Should return DirectoryExists error"
        );
    }

    /// Test that existing directory with force flag succeeds.
    #[tokio::test]
    async fn test_generate_structure_exists_with_force() {
        let dir = tempdir().unwrap();
        let pk_dir = dir.path().join(".pipeline-kit");
        fs::create_dir_all(&pk_dir).unwrap();

        // Write a file that will be overwritten
        fs::write(pk_dir.join("old-file.txt"), "old content").unwrap();

        let options = InitOptions {
            target_dir: dir.path().to_path_buf(),
            force: true,
            minimal: false,
        };

        let result = generate_pipeline_kit_structure(options).await;
        assert!(result.is_ok(), "Should succeed with force flag");

        // Verify new structure exists
        assert!(
            pk_dir.join("config.toml").exists(),
            "config.toml should be created"
        );
    }

    /// Test default InitOptions.
    #[test]
    fn test_default_init_options() {
        let options = InitOptions::default();
        assert!(!options.force, "Default force should be false");
        assert!(!options.minimal, "Default minimal should be false");
        assert!(
            options.target_dir.is_absolute() || options.target_dir == PathBuf::from("."),
            "Default target_dir should be current directory"
        );
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/init/mod.rs">
//! Initialization module for creating .pipeline-kit directory structures.
//!
//! This module provides functionality to initialize a new Pipeline Kit project
//! by generating a `.pipeline-kit/` directory with pre-configured templates for:
//! - Global configuration (`config.toml`)
//! - Agent definitions (`agents/*.md`)
//! - Pipeline workflows (`pipelines/*.yaml`)
//!
//! # Example
//!
//! ```no_run
//! use pk_core::init::{InitOptions, generate_pipeline_kit_structure};
//! use std::path::PathBuf;
//!
//! # async fn example() -> Result<(), Box<dyn std::error::Error>> {
//! let options = InitOptions {
//!     target_dir: PathBuf::from("."),
//!     force: false,
//!     minimal: false,
//! };
//!
//! generate_pipeline_kit_structure(options).await?;
//! println!("Pipeline Kit initialized successfully!");
//! # Ok(())
//! # }
//! ```

pub mod error;
pub mod generator;
pub mod templates;

// Re-export commonly used types for convenience
pub use error::{InitError, InitResult};
pub use generator::{generate_pipeline_kit_structure, InitOptions};
pub use templates::{get_template, list_templates};
</file>

<file path="pipeline-kit-rs/crates/core/src/init/templates.rs">
//! Embedded template files for .pipeline-kit initialization.
//!
//! This module uses `rust-embed` to embed template files from the project root
//! `templates/` directory into the binary at compile time. This allows the CLI
//! to generate `.pipeline-kit/` structures without external file dependencies.

use rust_embed::RustEmbed;

/// Embedded template files from the `templates/` directory.
///
/// At compile time, all files in the project root `templates/` directory are
/// embedded into the binary. The path is calculated relative to the crate root:
/// - `CARGO_MANIFEST_DIR` = `pipeline-kit-rs/crates/core`
/// - `../../../templates` = project root `templates/`
///
/// During development with the `debug-embed` feature, files are read from the
/// filesystem at runtime, allowing for quick iteration without recompilation.
#[derive(RustEmbed)]
#[folder = "$CARGO_MANIFEST_DIR/../../../templates"]
pub struct TemplateAssets;

/// Get template file content by path.
///
/// # Arguments
/// * `path` - Relative path from templates root (e.g., "config.toml", "agents/developer.md")
///
/// # Returns
/// The file content as a String, or None if the file doesn't exist.
///
/// # Example
/// ```
/// use pk_core::init::templates::get_template;
///
/// let config = get_template("config.toml").expect("config.toml should exist");
/// assert!(config.contains("git ="));
/// ```
pub fn get_template(path: &str) -> Option<String> {
    TemplateAssets::get(path).map(|file| String::from_utf8_lossy(file.data.as_ref()).to_string())
}

/// List all template files in a directory.
///
/// # Arguments
/// * `prefix` - Directory prefix (e.g., "agents/", "pipelines/")
///
/// # Returns
/// A vector of file paths that match the prefix.
///
/// # Example
/// ```
/// use pk_core::init::templates::list_templates;
///
/// let agents = list_templates("agents/");
/// assert!(agents.contains(&"agents/developer.md".to_string()));
/// ```
pub fn list_templates(prefix: &str) -> Vec<String> {
    TemplateAssets::iter()
        .filter(|path| path.starts_with(prefix))
        .map(|path| path.to_string())
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_get_config_template() {
        let config = get_template("config.toml");
        assert!(config.is_some(), "config.toml should be embedded");
        let content = config.unwrap();
        assert!(
            content.contains("git ="),
            "config.toml should contain git setting"
        );
    }

    #[test]
    fn test_get_agent_template() {
        let developer = get_template("agents/developer.md");
        assert!(
            developer.is_some(),
            "agents/developer.md should be embedded"
        );
        let content = developer.unwrap();
        assert!(
            content.contains("name: developer"),
            "developer.md should have correct frontmatter"
        );
    }

    #[test]
    fn test_get_reviewer_template() {
        let reviewer = get_template("agents/reviewer.md");
        assert!(reviewer.is_some(), "agents/reviewer.md should be embedded");
        let content = reviewer.unwrap();
        assert!(
            content.contains("name: reviewer"),
            "reviewer.md should have correct frontmatter"
        );
    }

    #[test]
    fn test_get_simple_task_pipeline() {
        let pipeline = get_template("pipelines/simple-task.yaml");
        assert!(
            pipeline.is_some(),
            "pipelines/simple-task.yaml should be embedded"
        );
        let content = pipeline.unwrap();
        assert!(
            content.contains("name: simple-task"),
            "simple-task.yaml should have correct name"
        );
    }

    #[test]
    fn test_get_code_review_pipeline() {
        let pipeline = get_template("pipelines/code-review.yaml");
        assert!(
            pipeline.is_some(),
            "pipelines/code-review.yaml should be embedded"
        );
        let content = pipeline.unwrap();
        assert!(
            content.contains("name: code-review"),
            "code-review.yaml should have correct name"
        );
    }

    #[test]
    fn test_get_nonexistent_template() {
        let result = get_template("nonexistent.txt");
        assert!(result.is_none(), "Nonexistent files should return None");
    }

    #[test]
    fn test_list_agent_templates() {
        let agents = list_templates("agents/");
        assert!(!agents.is_empty(), "Should find agent templates");
        assert!(
            agents.contains(&"agents/developer.md".to_string()),
            "Should contain developer.md"
        );
        assert!(
            agents.contains(&"agents/reviewer.md".to_string()),
            "Should contain reviewer.md"
        );
    }

    #[test]
    fn test_list_pipeline_templates() {
        let pipelines = list_templates("pipelines/");
        assert!(!pipelines.is_empty(), "Should find pipeline templates");
        assert!(
            pipelines.contains(&"pipelines/simple-task.yaml".to_string()),
            "Should contain simple-task.yaml"
        );
        assert!(
            pipelines.contains(&"pipelines/code-review.yaml".to_string()),
            "Should contain code-review.yaml"
        );
    }

    #[test]
    fn test_list_empty_prefix() {
        let all = list_templates("");
        assert!(!all.is_empty(), "Should find all templates");
        // Should contain at least config.toml, 2 agents, 2 pipelines
        assert!(all.len() >= 5, "Should have at least 5 template files");
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/state/mod.rs">
//! State management for pipeline processes.
//!
//! This module provides:
//! - Process state machine logic
//! - StateManager for coordinating multiple processes

pub mod manager;
pub mod process;
</file>

<file path="pipeline-kit-rs/crates/core/tests/common/mock_cli.py">
#!/usr/bin/env python3
"""Mock CLI that outputs predefined JSON Lines for testing CliExecutor."""

import json
import sys
import time

def main():
    # Output predefined JSON Lines
    events = [
        {"type": "system", "message": "Starting mock CLI"},
        {"type": "assistant", "content": [{"type": "text", "text": "Hello from mock"}]},
        {"type": "tool_call", "name": "Read", "input": {"file_path": "/test.txt"}},
        {"type": "assistant", "content": [{"type": "text", "text": "Processing complete"}]},
        {"type": "result", "session_id": "mock-session-123", "duration_ms": 100}
    ]

    for event in events:
        print(json.dumps(event), flush=True)
        # Small delay to simulate streaming
        time.sleep(0.01)

if __name__ == "__main__":
    main()
</file>

<file path="pipeline-kit-rs/crates/core/tests/cli_executor.rs">
//! Integration tests for CliExecutor.

use pk_core::agents::cli_executor::CliExecutor;
use tokio_stream::StreamExt;

#[tokio::test]
async fn test_cli_executor_execute_success() {
    // Get the path to the mock CLI script
    let manifest_dir = env!("CARGO_MANIFEST_DIR");
    let mock_cli_path = format!("{}/tests/common/mock_cli.py", manifest_dir);

    // Execute the mock CLI
    let stream = CliExecutor::execute(
        "python3".to_string(),
        vec![mock_cli_path],
        manifest_dir.to_string(),
    );

    // Collect all JSON values from the stream
    let values: Vec<_> = stream
        .collect::<Vec<_>>()
        .await
        .into_iter()
        .collect::<Result<Vec<_>, _>>()
        .expect("Stream should not contain errors");

    // Verify we got 5 events
    assert_eq!(values.len(), 5, "Expected 5 JSON events from mock CLI");

    // Verify first event (system)
    assert_eq!(
        values[0].get("type").and_then(|v| v.as_str()),
        Some("system")
    );
    assert_eq!(
        values[0].get("message").and_then(|v| v.as_str()),
        Some("Starting mock CLI")
    );

    // Verify second event (assistant)
    assert_eq!(
        values[1].get("type").and_then(|v| v.as_str()),
        Some("assistant")
    );
    let content = values[1].get("content").and_then(|v| v.as_array());
    assert!(content.is_some());
    assert_eq!(content.unwrap().len(), 1);

    // Verify third event (tool_call)
    assert_eq!(
        values[2].get("type").and_then(|v| v.as_str()),
        Some("tool_call")
    );
    assert_eq!(values[2].get("name").and_then(|v| v.as_str()), Some("Read"));

    // Verify fourth event (assistant)
    assert_eq!(
        values[3].get("type").and_then(|v| v.as_str()),
        Some("assistant")
    );

    // Verify fifth event (result)
    assert_eq!(
        values[4].get("type").and_then(|v| v.as_str()),
        Some("result")
    );
    assert_eq!(
        values[4].get("session_id").and_then(|v| v.as_str()),
        Some("mock-session-123")
    );
}

#[tokio::test]
async fn test_cli_executor_invalid_json() {
    // Create a temporary script that outputs invalid JSON
    let temp_dir = tempfile::tempdir().unwrap();
    let script_path = temp_dir.path().join("bad_cli.py");
    std::fs::write(
        &script_path,
        r#"#!/usr/bin/env python3
print("not json")
print("{\"valid\": true}")
"#,
    )
    .unwrap();

    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        std::fs::set_permissions(&script_path, std::fs::Permissions::from_mode(0o755)).unwrap();
    }

    let stream = CliExecutor::execute(
        "python3".to_string(),
        vec![script_path.to_str().unwrap().to_string()],
        temp_dir.path().to_str().unwrap().to_string(),
    );

    let results: Vec<_> = stream.collect::<Vec<_>>().await;

    // First line should be an error (invalid JSON)
    assert!(results[0].is_err());

    // Second line should be valid
    assert!(results[1].is_ok());
}

#[tokio::test]
async fn test_cli_executor_empty_lines() {
    // Create a script with empty lines
    let temp_dir = tempfile::tempdir().unwrap();
    let script_path = temp_dir.path().join("empty_lines.py");
    std::fs::write(
        &script_path,
        r#"#!/usr/bin/env python3
import json

print()
print(json.dumps({"type": "test"}))
print()
print(json.dumps({"type": "test2"}))
"#,
    )
    .unwrap();

    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        std::fs::set_permissions(&script_path, std::fs::Permissions::from_mode(0o755)).unwrap();
    }

    let stream = CliExecutor::execute(
        "python3".to_string(),
        vec![script_path.to_str().unwrap().to_string()],
        temp_dir.path().to_str().unwrap().to_string(),
    );

    let values: Vec<_> = stream
        .collect::<Vec<_>>()
        .await
        .into_iter()
        .collect::<Result<Vec<_>, _>>()
        .expect("Stream should not contain errors");

    // Empty lines should be filtered out
    assert_eq!(values.len(), 2);
    assert_eq!(values[0].get("type").and_then(|v| v.as_str()), Some("test"));
    assert_eq!(
        values[1].get("type").and_then(|v| v.as_str()),
        Some("test2")
    );
}

#[tokio::test]
async fn test_cli_executor_command_not_found() {
    let stream = CliExecutor::execute(
        "nonexistent-command-xyz123".to_string(),
        vec![],
        ".".to_string(),
    );

    // Collect results - should get an error
    let results: Vec<_> = stream.collect::<Vec<_>>().await;

    // Should have at least one error
    assert!(!results.is_empty());
    assert!(results[0].is_err());
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/qwen_integration.rs">
//! Integration tests for Qwen adapter with the full system.

use pk_core::agents::{AgentFactory, AgentManager, AgentType};
use pk_protocol::agent_models::Agent as AgentConfig;

#[tokio::test]
async fn test_qwen_agent_from_config() {
    // Create Qwen agent config
    let config = AgentConfig {
        name: "qwen-dev".to_string(),
        model: "qwen-coder".to_string(),
        description: "Qwen development agent".to_string(),
        color: "purple".to_string(),
        system_prompt: "You are a helpful Qwen assistant.".to_string(),
    };

    // Test factory creation
    let agent = AgentFactory::create(&config);
    assert!(
        agent.is_ok(),
        "Factory should create QwenAdapter successfully"
    );

    let agent = agent.unwrap();

    // Check availability (will be false unless qwen CLI is installed)
    let _available = agent.check_availability().await;

    println!("‚úÖ Qwen agent created successfully from config");
}

#[tokio::test]
async fn test_qwen_in_agent_manager() {
    // Create multiple agent configs including Qwen
    let configs = vec![
        AgentConfig {
            name: "qwen-agent".to_string(),
            model: "qwen-coder".to_string(),
            description: "Qwen agent".to_string(),
            color: "purple".to_string(),
            system_prompt: "Qwen prompt".to_string(),
        },
        AgentConfig {
            name: "mock-agent".to_string(),
            model: "test-model".to_string(),
            description: "Mock agent".to_string(),
            color: "blue".to_string(),
            system_prompt: "Mock prompt".to_string(),
        },
    ];

    // Create AgentManager
    let manager = AgentManager::new(configs);

    // Verify both agents are registered
    assert!(
        manager.has_agent("qwen-agent"),
        "Qwen agent should be registered"
    );
    assert!(
        manager.has_agent("mock-agent"),
        "Mock agent should be registered"
    );

    // Verify agent list
    let agents = manager.list_agents();
    assert_eq!(agents.len(), 2, "Should have 2 agents");
    assert!(
        agents.contains(&"qwen-agent".to_string()),
        "Should contain qwen-agent"
    );

    println!("‚úÖ Qwen agent integrated with AgentManager successfully");
}

#[test]
fn test_qwen_agent_type_detection() {
    // Test various Qwen model names
    assert_eq!(AgentType::from_model_name("qwen-coder"), AgentType::Qwen);
    assert_eq!(
        AgentType::from_model_name("Qwen3-Coder-Plus"),
        AgentType::Qwen
    );
    assert_eq!(AgentType::from_model_name("qwen2.5-coder"), AgentType::Qwen);
    assert_eq!(AgentType::from_model_name("QWEN-turbo"), AgentType::Qwen);

    // Test that non-Qwen models don't get detected as Qwen
    assert_ne!(AgentType::from_model_name("claude-3"), AgentType::Qwen);
    assert_ne!(AgentType::from_model_name("gpt-4"), AgentType::Qwen);

    println!("‚úÖ Qwen model name detection working correctly");
}

#[tokio::test]
async fn test_qwen_with_fallback() {
    // Create configs with Qwen as primary and Mock as fallback
    let configs = vec![
        AgentConfig {
            name: "qwen-primary".to_string(),
            model: "qwen-coder".to_string(),
            description: "Primary Qwen agent".to_string(),
            color: "purple".to_string(),
            system_prompt: "Primary prompt".to_string(),
        },
        AgentConfig {
            name: "mock-fallback".to_string(),
            model: "test-model".to_string(),
            description: "Fallback mock agent".to_string(),
            color: "blue".to_string(),
            system_prompt: "Fallback prompt".to_string(),
        },
    ];

    // Create manager with fallback
    let manager = AgentManager::new(configs).with_fallback("mock-fallback".to_string());

    // Verify agents are registered
    assert!(manager.has_agent("qwen-primary"));
    assert!(manager.has_agent("mock-fallback"));

    println!("‚úÖ Qwen agent works with fallback mechanism");
}

#[test]
fn test_multiple_qwen_models() {
    // Test that different Qwen model variants all map to Qwen type
    let models = vec![
        "qwen-coder",
        "qwen2.5-coder",
        "Qwen3-Coder-Plus",
        "qwen-turbo",
        "QWEN-MAX",
    ];

    for model in models {
        assert_eq!(
            AgentType::from_model_name(model),
            AgentType::Qwen,
            "Model '{}' should be detected as Qwen",
            model
        );
    }

    println!("‚úÖ All Qwen model variants detected correctly");
}
</file>

<file path="pipeline-kit-rs/crates/tui/src/widgets/mod.rs">
//! TUI widgets module.
//!
//! This module contains reusable widgets for the TUI.

pub mod command_composer;
pub mod dashboard;
pub mod detail_view;

pub use command_composer::CommandComposer;
pub use detail_view::DetailView;
</file>

<file path="pipeline-kit-rs/crates/tui/src/event.rs">
//! Event handling types for the TUI.
//!
//! This module defines the types used for event handling in the TUI,
//! including the EventStatus enum that widgets use to indicate whether
//! they consumed an event or not.

/// Status of an event after being handled by a widget.
///
/// Widgets return this enum from their `handle_key_event` methods to indicate
/// whether the event was consumed or should be passed to the next handler in
/// the chain of responsibility.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum EventStatus {
    /// The event was handled by the widget and should not be propagated further.
    Consumed,
    /// The event was not handled by the widget and should be passed to the next handler.
    NotConsumed,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_event_status_equality() {
        assert_eq!(EventStatus::Consumed, EventStatus::Consumed);
        assert_eq!(EventStatus::NotConsumed, EventStatus::NotConsumed);
        assert_ne!(EventStatus::Consumed, EventStatus::NotConsumed);
    }

    #[test]
    fn test_event_status_copy() {
        let status1 = EventStatus::Consumed;
        let status2 = status1; // Should copy, not move
        assert_eq!(status1, status2);
    }
}
</file>

<file path="scripts/verify-ts-types.sh">
#!/bin/bash
set -e

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo "üîç Verifying TypeScript type generation..."
echo ""

# 1. Generate TypeScript types from Rust
echo -e "${YELLOW}1/2 Generating TypeScript types from Rust...${NC}"
cd pipeline-kit-rs
if cargo test --package pk-protocol --lib -- --nocapture 2>&1 | grep -q "TypeScript bindings generated"; then
    echo -e "${GREEN}‚úÖ TypeScript types generated successfully${NC}"
else
    # Run the test and check if it passes (it generates types as a side effect)
    if cargo test --package pk-protocol --lib -- --nocapture > /dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ TypeScript types generated successfully${NC}"
    else
        echo -e "${RED}‚ùå TypeScript type generation failed${NC}"
        exit 1
    fi
fi
echo ""

# 2. Verify TypeScript type checking
echo -e "${YELLOW}2/2 Running TypeScript type check...${NC}"
cd ../pipeline-kit-cli
if pnpm type-check; then
    echo -e "${GREEN}‚úÖ TypeScript type check passed${NC}"
else
    echo -e "${RED}‚ùå TypeScript type check failed${NC}"
    exit 1
fi
echo ""

echo -e "${GREEN}üéâ TypeScript type verification passed!${NC}"
</file>

<file path="templates/pipelines/code-review.yaml">
name: code-review
master:
  model: claude-sonnet-4.5
  system-prompt: |
    You are the master orchestrator for a code review pipeline.
    Your job is to coordinate between the developer and reviewer agents
    to ensure high-quality code is produced.
  process:
    - developer
    - reviewer
    - HUMAN_REVIEW
sub-agents:
  - developer
  - reviewer
</file>

<file path="templates/pipelines/simple-task.yaml">
name: simple-task
master:
  model: claude-sonnet-4.5
  system-prompt: |
    You are the master orchestrator for a simple task execution.
    Guide the developer agent to complete the given task efficiently.
  process:
    - developer
sub-agents:
  - developer
</file>

<file path="templates/config.toml">
# Pipeline Kit Global Configuration
# Documentation: https://github.com/Vooster-AI/pipeline-kit

# Enable git integration (auto-commit after pipeline completion)
git = false

# Default timeout for agent execution (seconds)
# timeout = 300
</file>

<file path=".codespellrc">
[codespell]
skip = target,*.lock,pnpm-lock.yaml,Cargo.lock,.git,node_modules,dist,vendor
ignore-words = .codespellignore
</file>

<file path=".mcp.json">
{
  "mcpServers": {
    "codex-github-fetcher": {
      "type": "stdio",
      "command": "npx",
      "args": [
        "-y",
        "github-fetcher-mcp",
        "--repoIdentifier",
        "openai/codex/main"
      ],
      "env": {}
    }
  }
}
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Pipeline Kit Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pnpm-workspace.yaml">
packages:
  - "pipeline-kit-cli"
</file>

<file path="pipeline-kit-cli/test/binary-path.test.js">
/**
 * Binary path resolution tests.
 * RED phase: These tests verify correct path construction.
 */

import { describe, it, expect } from 'vitest';
import { getVendorBinaryPath, getVendorBinaryPathLegacy, getDevBinaryPath, getPlatformName, platformMap } from '../lib/platform.js';
import path from 'path';

describe('getPlatformName', () => {
  it('should map darwin-x64 to macos-x64', () => {
    expect(getPlatformName('darwin', 'x64')).toBe('macos-x64');
  });

  it('should map darwin-arm64 to macos-arm64', () => {
    expect(getPlatformName('darwin', 'arm64')).toBe('macos-arm64');
  });

  it('should map linux-x64 to linux-x64', () => {
    expect(getPlatformName('linux', 'x64')).toBe('linux-x64');
  });

  it('should map linux-arm64 to linux-arm64', () => {
    expect(getPlatformName('linux', 'arm64')).toBe('linux-arm64');
  });

  it('should map win32-x64 to windows-x64', () => {
    expect(getPlatformName('win32', 'x64')).toBe('windows-x64');
  });

  it('should map win32-arm64 to windows-arm64', () => {
    expect(getPlatformName('win32', 'arm64')).toBe('windows-arm64');
  });

  it('should return null for unsupported platform', () => {
    expect(getPlatformName('freebsd', 'x64')).toBeNull();
    expect(getPlatformName('sunos', 'x64')).toBeNull();
  });
});

describe('getVendorBinaryPath', () => {
  it('should construct correct path for macos-arm64', () => {
    const vendorRoot = '/fake/vendor';
    const platformName = 'macos-arm64';
    const binaryName = 'pipeline';

    const result = getVendorBinaryPath(vendorRoot, platformName, binaryName);

    expect(result).toBe(path.join('/fake/vendor', 'macos-arm64', 'pipeline-kit', 'pipeline'));
  });

  it('should construct correct path for macos-x64', () => {
    const vendorRoot = '/fake/vendor';
    const platformName = 'macos-x64';
    const binaryName = 'pipeline';

    const result = getVendorBinaryPath(vendorRoot, platformName, binaryName);

    expect(result).toBe(path.join('/fake/vendor', 'macos-x64', 'pipeline-kit', 'pipeline'));
  });

  it('should construct correct path for linux-x64', () => {
    const vendorRoot = '/fake/vendor';
    const platformName = 'linux-x64';
    const binaryName = 'pipeline';

    const result = getVendorBinaryPath(vendorRoot, platformName, binaryName);

    expect(result).toBe(path.join('/fake/vendor', 'linux-x64', 'pipeline-kit', 'pipeline'));
  });

  it('should construct correct path for windows-x64 with .exe extension', () => {
    const vendorRoot = 'C:\\fake\\vendor';
    const platformName = 'windows-x64';
    const binaryName = 'pipeline.exe';

    const result = getVendorBinaryPath(vendorRoot, platformName, binaryName);

    expect(result).toBe(path.join('C:\\fake\\vendor', 'windows-x64', 'pipeline-kit', 'pipeline.exe'));
  });

  it('should handle relative vendor paths', () => {
    const vendorRoot = 'vendor';
    const platformName = 'macos-arm64';
    const binaryName = 'pipeline';

    const result = getVendorBinaryPath(vendorRoot, platformName, binaryName);

    expect(result).toBe(path.join('vendor', 'macos-arm64', 'pipeline-kit', 'pipeline'));
  });
});

describe('getDevBinaryPath', () => {
  it('should construct correct dev path for Unix binary', () => {
    const cliDir = '/fake/pipeline-kit-cli';
    const binaryName = 'pipeline';

    const result = getDevBinaryPath(cliDir, binaryName);

    expect(result).toBe(path.join('/fake/pipeline-kit-cli', '..', 'pipeline-kit-rs', 'target', 'release', 'pipeline'));
  });

  it('should construct correct dev path for Windows binary', () => {
    const cliDir = 'C:\\fake\\pipeline-kit-cli';
    const binaryName = 'pipeline.exe';

    const result = getDevBinaryPath(cliDir, binaryName);

    expect(result).toBe(path.join('C:\\fake\\pipeline-kit-cli', '..', 'pipeline-kit-rs', 'target', 'release', 'pipeline.exe'));
  });

  it('should handle relative cli directory paths', () => {
    const cliDir = 'pipeline-kit-cli';
    const binaryName = 'pipeline';

    const result = getDevBinaryPath(cliDir, binaryName);

    expect(result).toBe(path.join('pipeline-kit-cli', '..', 'pipeline-kit-rs', 'target', 'release', 'pipeline'));
  });

  it('should correctly navigate up one directory level', () => {
    const cliDir = '/project/pipeline-kit-cli';
    const binaryName = 'pipeline';

    const result = getDevBinaryPath(cliDir, binaryName);
    const normalized = path.normalize(result);

    // Should resolve to /project/pipeline-kit-rs/target/release/pipeline
    expect(normalized).toContain('pipeline-kit-rs');
    expect(normalized).toContain('target');
    expect(normalized).toContain('release');
  });
});

describe('Path format validation', () => {
  it('vendor paths should use correct path separators', () => {
    const vendorRoot = '/vendor';
    const result = getVendorBinaryPath(vendorRoot, 'macos-arm64', 'pipeline');

    // Should use platform-appropriate separators
    expect(result).toContain(path.sep);
  });

  it('dev paths should use correct path separators', () => {
    const cliDir = '/cli';
    const result = getDevBinaryPath(cliDir, 'pipeline');

    // Should use platform-appropriate separators
    expect(result).toContain(path.sep);
  });

  it('paths should not have double separators', () => {
    const vendorRoot = '/vendor/';
    const result = getVendorBinaryPath(vendorRoot, 'macos-arm64', 'pipeline');

    // path.join should handle this correctly
    expect(result).not.toMatch(/\/\//);
    expect(result).not.toMatch(/\\\\/);
  });
});

describe('platformMap export', () => {
  it('should export platformMap with all expected mappings', () => {
    expect(platformMap).toBeDefined();
    expect(platformMap['darwin-x64']).toBe('macos-x64');
    expect(platformMap['darwin-arm64']).toBe('macos-arm64');
    expect(platformMap['linux-x64']).toBe('linux-x64');
    expect(platformMap['linux-arm64']).toBe('linux-arm64');
    expect(platformMap['win32-x64']).toBe('windows-x64');
    expect(platformMap['win32-arm64']).toBe('windows-arm64');
    expect(platformMap['android-arm64']).toBe('linux-arm64');
  });
});

describe('getVendorBinaryPathLegacy (backward compatibility)', () => {
  it('should construct correct path using target triple', () => {
    const vendorRoot = '/fake/vendor';
    const targetTriple = 'x86_64-apple-darwin';
    const binaryName = 'pipeline';

    const result = getVendorBinaryPathLegacy(vendorRoot, targetTriple, binaryName);

    expect(result).toBe(path.join('/fake/vendor', 'x86_64-apple-darwin', 'pipeline-kit', 'pipeline'));
  });

  it('should work with different target triples', () => {
    const vendorRoot = '/vendor';

    const linux = getVendorBinaryPathLegacy(vendorRoot, 'x86_64-unknown-linux-musl', 'pipeline');
    expect(linux).toContain('x86_64-unknown-linux-musl');

    const windows = getVendorBinaryPathLegacy(vendorRoot, 'x86_64-pc-windows-msvc', 'pipeline.exe');
    expect(windows).toContain('x86_64-pc-windows-msvc');
  });
});
</file>

<file path="pipeline-kit-cli/test/platform.test.js">
/**
 * Platform detection tests.
 * RED phase: These tests define the expected behavior.
 */

import { describe, it, expect } from 'vitest';
import { getTargetTriple, getBinaryName } from '../lib/platform.js';

describe('getTargetTriple', () => {
  describe('Linux platforms', () => {
    it('should return correct triple for linux x64', () => {
      expect(getTargetTriple('linux', 'x64')).toBe('x86_64-unknown-linux-musl');
    });

    it('should return correct triple for linux arm64', () => {
      expect(getTargetTriple('linux', 'arm64')).toBe('aarch64-unknown-linux-musl');
    });

    it('should return correct triple for android x64', () => {
      expect(getTargetTriple('android', 'x64')).toBe('x86_64-unknown-linux-musl');
    });

    it('should return correct triple for android arm64', () => {
      expect(getTargetTriple('android', 'arm64')).toBe('aarch64-unknown-linux-musl');
    });

    it('should return null for unsupported linux arch', () => {
      expect(getTargetTriple('linux', 'ia32')).toBeNull();
      expect(getTargetTriple('linux', 'arm')).toBeNull();
    });
  });

  describe('macOS (darwin) platforms', () => {
    it('should return correct triple for darwin x64', () => {
      expect(getTargetTriple('darwin', 'x64')).toBe('x86_64-apple-darwin');
    });

    it('should return correct triple for darwin arm64', () => {
      expect(getTargetTriple('darwin', 'arm64')).toBe('aarch64-apple-darwin');
    });

    it('should return null for unsupported darwin arch', () => {
      expect(getTargetTriple('darwin', 'ia32')).toBeNull();
    });
  });

  describe('Windows platforms', () => {
    it('should return correct triple for win32 x64', () => {
      expect(getTargetTriple('win32', 'x64')).toBe('x86_64-pc-windows-msvc');
    });

    it('should return correct triple for win32 arm64', () => {
      expect(getTargetTriple('win32', 'arm64')).toBe('aarch64-pc-windows-msvc');
    });

    it('should return null for unsupported win32 arch', () => {
      expect(getTargetTriple('win32', 'ia32')).toBeNull();
    });
  });

  describe('Unsupported platforms', () => {
    it('should return null for freebsd', () => {
      expect(getTargetTriple('freebsd', 'x64')).toBeNull();
    });

    it('should return null for sunos', () => {
      expect(getTargetTriple('sunos', 'x64')).toBeNull();
    });

    it('should return null for unknown platform', () => {
      expect(getTargetTriple('unknown', 'x64')).toBeNull();
    });
  });
});

describe('getBinaryName', () => {
  it('should return pipeline.exe for win32', () => {
    expect(getBinaryName('win32')).toBe('pipeline.exe');
  });

  it('should return pipeline for darwin', () => {
    expect(getBinaryName('darwin')).toBe('pipeline');
  });

  it('should return pipeline for linux', () => {
    expect(getBinaryName('linux')).toBe('pipeline');
  });

  it('should return pipeline for android', () => {
    expect(getBinaryName('android')).toBe('pipeline');
  });

  it('should return pipeline for any non-Windows platform', () => {
    expect(getBinaryName('freebsd')).toBe('pipeline');
    expect(getBinaryName('sunos')).toBe('pipeline');
  });
});

describe('getTargetTriple (legacy)', () => {
  it('should return correct triple for android platforms', () => {
    expect(getTargetTriple('android', 'x64')).toBe('x86_64-unknown-linux-musl');
    expect(getTargetTriple('android', 'arm64')).toBe('aarch64-unknown-linux-musl');
  });

  it('should maintain backward compatibility with target triple naming', () => {
    // Verify the legacy function still works
    expect(getTargetTriple('darwin', 'arm64')).toBe('aarch64-apple-darwin');
    expect(getTargetTriple('linux', 'x64')).toBe('x86_64-unknown-linux-musl');
    expect(getTargetTriple('win32', 'x64')).toBe('x86_64-pc-windows-msvc');
  });
});
</file>

<file path="pipeline-kit-cli/.gitignore">
coverage/
vendor/
</file>

<file path="pipeline-kit-cli/vitest.config.js">
import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        'scripts/',
        'vendor/',
        'vitest.config.js',
        'bin/', // Exclude bin from coverage as it's tested via integration tests
      ],
      // Set thresholds for lib/ directory only
      thresholds: {
        lines: 90,
        functions: 80,
        branches: 90,
        statements: 90,
      },
    },
  },
});
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/codex_adapter.rs">
//! Codex adapter implementation using OpenAI Codex CLI via JSON-RPC.
//!
//! This adapter communicates with the OpenAI Codex CLI using JSON-RPC protocol
//! over stdin/stdout pipes, following the Python reference implementation.

use crate::agents::base::Agent;
use crate::agents::base::AgentError;
use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use async_trait::async_trait;
use serde::Deserialize;
use serde::Serialize;
use std::collections::HashMap;
use std::pin::Pin;
use std::process::Stdio;
use std::sync::Arc;
use std::sync::Mutex;
use tokio::fs;
use tokio::io::AsyncBufReadExt;
use tokio::io::AsyncWriteExt;
use tokio::io::BufReader;
use tokio::process::Command;
use tokio_stream::Stream;
use tokio_stream::StreamExt;

/// Codex adapter for executing instructions using OpenAI Codex CLI.
///
/// This adapter spawns the `codex` CLI process and communicates via JSON-RPC.
pub struct CodexAdapter {
    #[allow(dead_code)]
    name: String,
    model: String,
    system_prompt: String,
    /// Session mapping: project_id -> rollout_file_path
    session_mapping: Arc<Mutex<HashMap<String, String>>>,
}

impl CodexAdapter {
    /// Create a new Codex adapter.
    ///
    /// # Arguments
    ///
    /// * `name` - The agent name from configuration
    /// * `model` - The Codex model to use (e.g., "gpt-4", "codex")
    /// * `system_prompt` - The system prompt for the agent
    pub fn new(name: String, model: String, system_prompt: String) -> Result<Self, AgentError> {
        Ok(Self {
            name,
            model,
            system_prompt,
            session_mapping: Arc::new(Mutex::new(HashMap::new())),
        })
    }

    /// Extract project ID from project path.
    fn extract_project_id(project_path: &str) -> String {
        std::path::Path::new(project_path)
            .file_name()
            .and_then(|s| s.to_str())
            .unwrap_or(project_path)
            .to_string()
    }

    /// Ensure AGENTS.md file exists in the project root.
    ///
    /// Codex uses AGENTS.md for system prompts.
    async fn ensure_agent_md(&self, project_path: &str) -> Result<(), AgentError> {
        let agent_md_path = std::path::Path::new(project_path).join("AGENTS.md");

        // Skip if already exists
        if agent_md_path.exists() {
            return Ok(());
        }

        // Write system prompt to AGENTS.md
        fs::write(&agent_md_path, &self.system_prompt)
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write AGENTS.md: {}", e)))?;

        Ok(())
    }

    /// Get or create rollout file path for session management.
    async fn get_rollout_path(
        &self,
        project_path: &str,
        project_id: &str,
    ) -> Result<String, AgentError> {
        // Create rollout directory: .pipeline-kit/codex_rollouts/
        let rollout_dir = std::path::Path::new(project_path)
            .join(".pipeline-kit")
            .join("codex_rollouts");

        fs::create_dir_all(&rollout_dir).await.map_err(|e| {
            AgentError::ExecutionError(format!("Failed to create rollout directory: {}", e))
        })?;

        // Rollout file: <project_id>.yaml
        let rollout_file = rollout_dir.join(format!("{}.yaml", project_id));
        Ok(rollout_file.to_string_lossy().to_string())
    }

    /// Determine the codex executable name based on platform.
    fn get_executable_name() -> &'static str {
        if cfg!(target_os = "windows") {
            "codex.cmd"
        } else {
            "codex"
        }
    }
}

#[async_trait]
impl Agent for CodexAdapter {
    async fn check_availability(&self) -> bool {
        // Check if codex CLI is installed
        let cli_available = Command::new(Self::get_executable_name())
            .arg("--version")
            .stdout(Stdio::null())
            .stderr(Stdio::null())
            .status()
            .await
            .map(|s| s.success())
            .unwrap_or(false);

        // Check if OPENAI_API_KEY is set
        let api_key_available = std::env::var("OPENAI_API_KEY").is_ok();

        cli_available && api_key_available
    }

    async fn execute(
        &self,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        // 1. Ensure AGENTS.md exists
        self.ensure_agent_md(&context.project_path).await?;

        // 2. Get rollout file path
        let project_id = Self::extract_project_id(&context.project_path);
        let rollout_path = self
            .get_rollout_path(&context.project_path, &project_id)
            .await?;

        // 3. Build command
        let mut cmd = Command::new(Self::get_executable_name());
        cmd.arg("--model").arg(&self.model);
        cmd.arg("--approval-policy").arg("allow-all"); // Auto-approve all actions
        cmd.arg("--rollout").arg(&rollout_path); // Session persistence
        cmd.arg("--output-format").arg("jsonrpc"); // JSON-RPC output

        // API key from environment
        if let Ok(api_key) = std::env::var("OPENAI_API_KEY") {
            cmd.env("OPENAI_API_KEY", api_key);
        }

        // Set working directory
        cmd.current_dir(&context.project_path);
        cmd.stdin(Stdio::piped());
        cmd.stdout(Stdio::piped());
        cmd.stderr(Stdio::piped());

        // 4. Spawn process
        let mut child = cmd
            .spawn()
            .map_err(|e| AgentError::ExecutionError(format!("Failed to spawn codex CLI: {}", e)))?;

        let mut stdin = child
            .stdin
            .take()
            .ok_or_else(|| AgentError::ExecutionError("Failed to capture stdin".to_string()))?;

        let stdout = child
            .stdout
            .take()
            .ok_or_else(|| AgentError::ExecutionError("Failed to capture stdout".to_string()))?;

        // 5. Create JSON-RPC request
        let request = JsonRpcRequest {
            jsonrpc: "2.0".to_string(),
            id: 1,
            method: "execute".to_string(),
            params: ExecuteParams {
                prompt: context.instruction.clone(),
                system: self.system_prompt.clone(),
            },
        };

        // 6. Send request to Codex CLI
        let request_str = serde_json::to_string(&request).map_err(|e| {
            AgentError::ExecutionError(format!("Failed to serialize request: {}", e))
        })?;

        stdin
            .write_all(request_str.as_bytes())
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write to stdin: {}", e)))?;
        stdin
            .write_all(b"\n")
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write newline: {}", e)))?;
        stdin
            .flush()
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to flush stdin: {}", e)))?;

        // Close stdin to signal end of input
        drop(stdin);

        // 7. Create stream from stdout
        let reader = BufReader::new(stdout);
        let lines = reader.lines();
        let lines_stream = tokio_stream::wrappers::LinesStream::new(lines);

        // 8. Save rollout path for this session
        {
            let mut mapping = self.session_mapping.lock().unwrap();
            mapping.insert(project_id.clone(), rollout_path);
        }

        // 9. Parse JSON-RPC responses and convert to AgentEvents
        let events_stream = lines_stream
            .then(|line_result| async move {
                match line_result {
                    Ok(line) => {
                        if line.trim().is_empty() {
                            return None;
                        }

                        match serde_json::from_str::<JsonRpcResponse>(&line) {
                            Ok(response) => convert_codex_response(response),
                            Err(e) => Some(Err(AgentError::StreamParseError(format!(
                                "Failed to parse JSON-RPC response: {} (line: {})",
                                e, line
                            )))),
                        }
                    }
                    Err(e) => Some(Err(AgentError::StreamParseError(e.to_string()))),
                }
            })
            .filter_map(|opt| opt);

        Ok(Box::pin(events_stream))
    }
}

/// JSON-RPC request structure.
#[derive(Debug, Serialize)]
struct JsonRpcRequest {
    jsonrpc: String,
    id: u32,
    method: String,
    params: ExecuteParams,
}

/// Parameters for the execute method.
#[derive(Debug, Serialize)]
struct ExecuteParams {
    prompt: String,
    system: String,
}

/// JSON-RPC response structure.
#[derive(Debug, Deserialize)]
struct JsonRpcResponse {
    #[allow(dead_code)]
    jsonrpc: String,
    #[allow(dead_code)]
    id: u32,
    result: Option<CodexResult>,
    error: Option<JsonRpcError>,
}

/// Codex result structure.
#[derive(Debug, Deserialize)]
struct CodexResult {
    #[serde(rename = "type")]
    event_type: String,
    #[serde(default)]
    content: Option<String>,
    #[serde(default)]
    tool_event: Option<ToolEvent>,
}

/// Tool event structure.
#[derive(Debug, Deserialize)]
struct ToolEvent {
    #[serde(rename = "type")]
    tool_type: String,
    #[serde(default)]
    command: Option<String>,
    #[serde(default)]
    patch: Option<String>,
    #[serde(default)]
    query: Option<String>,
}

/// JSON-RPC error structure.
#[derive(Debug, Deserialize)]
struct JsonRpcError {
    code: i32,
    message: String,
}

/// Convert Codex JSON-RPC response to AgentEvent.
fn convert_codex_response(response: JsonRpcResponse) -> Option<Result<AgentEvent, AgentError>> {
    // Check for errors
    if let Some(error) = response.error {
        return Some(Err(AgentError::ApiError(format!(
            "Codex API error (code {}): {}",
            error.code, error.message
        ))));
    }

    // Extract result
    if let Some(result) = response.result {
        match result.event_type.as_str() {
            "message" => {
                // Text message from assistant
                if let Some(content) = result.content {
                    if !content.trim().is_empty() {
                        return Some(Ok(AgentEvent::MessageChunk(content)));
                    }
                }
            }
            "tool_event" => {
                // Tool execution event
                if let Some(tool_event) = result.tool_event {
                    let tool_json = match tool_event.tool_type.as_str() {
                        "exec_command" => {
                            let cmd = tool_event.command.unwrap_or_default();
                            serde_json::json!({
                                "type": "exec_command",
                                "command": cmd
                            })
                        }
                        "patch_apply" => {
                            let patch = tool_event.patch.unwrap_or_default();
                            serde_json::json!({
                                "type": "patch_apply",
                                "patch": patch
                            })
                        }
                        "web_search" => {
                            let query = tool_event.query.unwrap_or_default();
                            serde_json::json!({
                                "type": "web_search",
                                "query": query
                            })
                        }
                        "mcp_tool_call" => {
                            serde_json::json!({
                                "type": "mcp_tool_call"
                            })
                        }
                        _ => {
                            serde_json::json!({
                                "type": "unknown",
                                "tool_type": tool_event.tool_type
                            })
                        }
                    };

                    return Some(Ok(AgentEvent::ToolCall(tool_json.to_string())));
                }
            }
            "done" | "completed" => {
                // Execution completed
                return Some(Ok(AgentEvent::Completed));
            }
            _ => {
                // Unknown event type, skip
            }
        }
    }

    // No meaningful event
    None
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_codex_adapter_new() {
        let adapter = CodexAdapter::new(
            "test".to_string(),
            "gpt-4".to_string(),
            "test prompt".to_string(),
        );
        assert!(adapter.is_ok());
    }

    #[test]
    fn test_extract_project_id() {
        assert_eq!(
            CodexAdapter::extract_project_id("/path/to/project"),
            "project"
        );
        assert_eq!(CodexAdapter::extract_project_id("/tmp/test"), "test");
    }

    #[tokio::test]
    async fn test_check_availability_without_cli() {
        let adapter = CodexAdapter::new(
            "test".to_string(),
            "gpt-4".to_string(),
            "test prompt".to_string(),
        )
        .unwrap();

        // Will return false unless codex CLI is installed AND OPENAI_API_KEY is set
        let available = adapter.check_availability().await;
        let _ = available;
    }

    #[test]
    fn test_json_rpc_request_serialization() {
        let request = JsonRpcRequest {
            jsonrpc: "2.0".to_string(),
            id: 1,
            method: "execute".to_string(),
            params: ExecuteParams {
                prompt: "Hello".to_string(),
                system: "You are helpful".to_string(),
            },
        };

        let json = serde_json::to_string(&request).unwrap();
        assert!(json.contains("jsonrpc"));
        assert!(json.contains("execute"));
        assert!(json.contains("Hello"));
    }

    #[test]
    fn test_convert_codex_response_with_message() {
        let response = JsonRpcResponse {
            jsonrpc: "2.0".to_string(),
            id: 1,
            result: Some(CodexResult {
                event_type: "message".to_string(),
                content: Some("Hello, world!".to_string()),
                tool_event: None,
            }),
            error: None,
        };

        let event = convert_codex_response(response);
        assert!(event.is_some());

        let event = event.unwrap();
        assert!(event.is_ok());

        match event.unwrap() {
            AgentEvent::MessageChunk(text) => assert_eq!(text, "Hello, world!"),
            _ => panic!("Expected MessageChunk"),
        }
    }

    #[test]
    fn test_convert_codex_response_with_tool_event() {
        let response = JsonRpcResponse {
            jsonrpc: "2.0".to_string(),
            id: 1,
            result: Some(CodexResult {
                event_type: "tool_event".to_string(),
                content: None,
                tool_event: Some(ToolEvent {
                    tool_type: "exec_command".to_string(),
                    command: Some("ls -la".to_string()),
                    patch: None,
                    query: None,
                }),
            }),
            error: None,
        };

        let event = convert_codex_response(response);
        assert!(event.is_some());

        let event = event.unwrap();
        assert!(event.is_ok());

        match event.unwrap() {
            AgentEvent::ToolCall(json) => {
                assert!(json.contains("exec_command"));
                assert!(json.contains("ls -la"));
            }
            _ => panic!("Expected ToolCall"),
        }
    }

    #[test]
    fn test_convert_codex_response_with_completion() {
        let response = JsonRpcResponse {
            jsonrpc: "2.0".to_string(),
            id: 1,
            result: Some(CodexResult {
                event_type: "done".to_string(),
                content: None,
                tool_event: None,
            }),
            error: None,
        };

        let event = convert_codex_response(response);
        assert!(event.is_some());

        let event = event.unwrap();
        assert!(event.is_ok());

        match event.unwrap() {
            AgentEvent::Completed => {}
            _ => panic!("Expected Completed"),
        }
    }

    #[test]
    fn test_convert_codex_response_with_error() {
        let response = JsonRpcResponse {
            jsonrpc: "2.0".to_string(),
            id: 1,
            result: None,
            error: Some(JsonRpcError {
                code: 400,
                message: "Bad request".to_string(),
            }),
        };

        let event = convert_codex_response(response);
        assert!(event.is_some());

        let event = event.unwrap();
        assert!(event.is_err());
    }

    #[tokio::test]
    async fn test_ensure_agent_md() {
        let adapter = CodexAdapter::new(
            "test".to_string(),
            "gpt-4".to_string(),
            "test system prompt".to_string(),
        )
        .unwrap();

        let temp_dir = tempfile::tempdir().unwrap();
        let project_path = temp_dir.path().to_str().unwrap();

        let result = adapter.ensure_agent_md(project_path).await;
        assert!(result.is_ok());

        // Verify file was created
        let agent_md_path = temp_dir.path().join("AGENTS.md");
        assert!(agent_md_path.exists());

        let content = fs::read_to_string(&agent_md_path).await.unwrap();
        assert_eq!(content, "test system prompt");

        // Calling again should not error (idempotent)
        let result2 = adapter.ensure_agent_md(project_path).await;
        assert!(result2.is_ok());
    }

    #[tokio::test]
    async fn test_get_rollout_path() {
        let adapter = CodexAdapter::new(
            "test".to_string(),
            "gpt-4".to_string(),
            "test prompt".to_string(),
        )
        .unwrap();

        let temp_dir = tempfile::tempdir().unwrap();
        let project_path = temp_dir.path().to_str().unwrap();

        let rollout_path = adapter
            .get_rollout_path(project_path, "my-project")
            .await
            .unwrap();

        assert!(rollout_path.contains(".pipeline-kit"));
        assert!(rollout_path.contains("codex_rollouts"));
        assert!(rollout_path.contains("my-project.yaml"));

        // Verify directory was created
        let rollout_dir = temp_dir.path().join(".pipeline-kit").join("codex_rollouts");
        assert!(rollout_dir.exists());
    }

    #[test]
    fn test_get_executable_name() {
        let exe_name = CodexAdapter::get_executable_name();
        if cfg!(target_os = "windows") {
            assert_eq!(exe_name, "codex.cmd");
        } else {
            assert_eq!(exe_name, "codex");
        }
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/agent_type.rs">
//! Agent type enumeration for determining which adapter to use.

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum AgentType {
    Claude,
    Cursor,
    Gemini,
    Codex,
    Qwen,
    Mock,
}

impl AgentType {
    /// Infer the agent type from a model name.
    ///
    /// # Arguments
    ///
    /// * `model` - The model name from the agent configuration
    ///
    /// # Returns
    ///
    /// The inferred `AgentType`. Defaults to `Mock` if the model doesn't match any known pattern.
    ///
    /// # Examples
    ///
    /// ```
    /// use pk_core::agents::AgentType;
    ///
    /// assert_eq!(AgentType::from_model_name("claude-sonnet-4.5"), AgentType::Claude);
    /// assert_eq!(AgentType::from_model_name("gpt-5"), AgentType::Cursor);
    /// assert_eq!(AgentType::from_model_name("gemini-2.5-pro"), AgentType::Gemini);
    /// assert_eq!(AgentType::from_model_name("unknown-model"), AgentType::Mock);
    /// ```
    pub fn from_model_name(model: &str) -> Self {
        let model_lower = model.to_lowercase();

        if model_lower.contains("claude") {
            Self::Claude
        } else if model_lower.starts_with("gpt")
            || model_lower.contains("cursor")
            || model_lower.starts_with("sonnet")
            || model_lower.starts_with("opus")
        {
            // Cursor uses GPT models and also has cursor-specific names
            // Also handles sonnet-4.5, opus-4.1 which are Cursor shorthand
            Self::Cursor
        } else if model_lower.contains("gemini") {
            Self::Gemini
        } else if model_lower.contains("codex") {
            Self::Codex
        } else if model_lower.contains("qwen") {
            Self::Qwen
        } else {
            // Default to Mock for unknown models
            Self::Mock
        }
    }

    /// Get a human-readable name for the agent type.
    pub fn name(&self) -> &'static str {
        match self {
            Self::Claude => "Claude",
            Self::Cursor => "Cursor",
            Self::Gemini => "Gemini",
            Self::Codex => "Codex",
            Self::Qwen => "Qwen",
            Self::Mock => "Mock",
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_from_model_name_claude() {
        assert_eq!(
            AgentType::from_model_name("claude-sonnet-4.5"),
            AgentType::Claude
        );
        assert_eq!(
            AgentType::from_model_name("claude-opus-4.1"),
            AgentType::Claude
        );
        assert_eq!(
            AgentType::from_model_name("Claude-Haiku-3.5"),
            AgentType::Claude
        );
        assert_eq!(AgentType::from_model_name("claude"), AgentType::Claude);
    }

    #[test]
    fn test_from_model_name_cursor() {
        assert_eq!(AgentType::from_model_name("gpt-5"), AgentType::Cursor);
        assert_eq!(AgentType::from_model_name("gpt-4o"), AgentType::Cursor);
        assert_eq!(
            AgentType::from_model_name("cursor-model"),
            AgentType::Cursor
        );
        assert_eq!(AgentType::from_model_name("sonnet-4.5"), AgentType::Cursor);
        assert_eq!(AgentType::from_model_name("opus-4.1"), AgentType::Cursor);
    }

    #[test]
    fn test_from_model_name_gemini() {
        assert_eq!(
            AgentType::from_model_name("gemini-2.5-pro"),
            AgentType::Gemini
        );
        assert_eq!(
            AgentType::from_model_name("gemini-2.5-flash"),
            AgentType::Gemini
        );
        assert_eq!(AgentType::from_model_name("Gemini-Pro"), AgentType::Gemini);
    }

    #[test]
    fn test_from_model_name_codex() {
        assert_eq!(AgentType::from_model_name("codex-model"), AgentType::Codex);
        assert_eq!(AgentType::from_model_name("openai-codex"), AgentType::Codex);
    }

    #[test]
    fn test_from_model_name_qwen() {
        assert_eq!(AgentType::from_model_name("qwen-coder"), AgentType::Qwen);
        assert_eq!(
            AgentType::from_model_name("Qwen3-Coder-Plus"),
            AgentType::Qwen
        );
    }

    #[test]
    fn test_from_model_name_unknown() {
        assert_eq!(AgentType::from_model_name("unknown-model"), AgentType::Mock);
        assert_eq!(AgentType::from_model_name(""), AgentType::Mock);
        assert_eq!(AgentType::from_model_name("random-string"), AgentType::Mock);
    }

    #[test]
    fn test_agent_type_name() {
        assert_eq!(AgentType::Claude.name(), "Claude");
        assert_eq!(AgentType::Cursor.name(), "Cursor");
        assert_eq!(AgentType::Gemini.name(), "Gemini");
        assert_eq!(AgentType::Codex.name(), "Codex");
        assert_eq!(AgentType::Qwen.name(), "Qwen");
        assert_eq!(AgentType::Mock.name(), "Mock");
    }

    #[test]
    fn test_agent_type_eq() {
        assert_eq!(AgentType::Claude, AgentType::Claude);
        assert_ne!(AgentType::Claude, AgentType::Cursor);
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/config/models.rs">
//! Configuration models that aggregate all settings.
//!
//! This module provides the unified `AppConfig` structure that combines
//! global settings, agent definitions, and pipeline definitions into a
//! single configuration object.

use pk_protocol::agent_models::Agent;
use pk_protocol::config_models::GlobalConfig;
use pk_protocol::pipeline_models::Pipeline;

/// Unified application configuration loaded from `.pipeline-kit/` directory.
///
/// This structure aggregates all configuration sources:
/// - `config.toml`: Global settings
/// - `agents/*.md`: Agent definitions
/// - `pipelines/*.yaml`: Pipeline definitions
///
/// # Example
///
/// ```rust,no_run
/// use pk_core::config::loader::load_config;
/// use std::path::Path;
///
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// let config = load_config(Path::new(".")).await?;
/// println!("Loaded {} agents and {} pipelines",
///          config.agents.len(),
///          config.pipelines.len());
/// # Ok(())
/// # }
/// ```
#[derive(Debug, Clone)]
pub struct AppConfig {
    /// Global settings from `config.toml`.
    pub global: GlobalConfig,

    /// All agent definitions loaded from `agents/*.md`.
    pub agents: Vec<Agent>,

    /// All pipeline definitions loaded from `pipelines/*.yaml`.
    pub pipelines: Vec<Pipeline>,
}

impl Default for AppConfig {
    fn default() -> Self {
        Self {
            global: GlobalConfig { git: false },
            agents: Vec::new(),
            pipelines: Vec::new(),
        }
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/common/assertions.rs">
//! Custom assertion helpers for E2E tests.

use pk_protocol::ipc::Event;
use pk_protocol::process_models::ProcessStatus;

/// Assert that a sequence of events contains a ProcessStarted event.
pub fn assert_has_process_started(events: &[Event]) -> bool {
    events
        .iter()
        .any(|e| matches!(e, Event::ProcessStarted { .. }))
}

/// Assert that a sequence of events contains a ProcessCompleted event.
#[allow(dead_code)]
pub fn assert_has_process_completed(events: &[Event]) -> bool {
    events
        .iter()
        .any(|e| matches!(e, Event::ProcessCompleted { .. }))
}

/// Assert that a sequence of events contains a ProcessStatusUpdate with specific status.
pub fn assert_has_status_update(events: &[Event], status: ProcessStatus) -> bool {
    events.iter().any(|e| {
        matches!(
            e,
            Event::ProcessStatusUpdate {
                status: s,
                ..
            } if *s == status
        )
    })
}

/// Assert that a sequence of events contains log chunks.
#[allow(dead_code)]
pub fn assert_has_log_chunks(events: &[Event]) -> bool {
    events
        .iter()
        .any(|e| matches!(e, Event::ProcessLogChunk { .. }))
}

/// Assert that events are in the correct sequential order.
///
/// Checks that:
/// 1. ProcessStarted comes first
/// 2. ProcessStatusUpdate(Running) comes before completion
/// 3. ProcessCompleted or Failed comes last
#[allow(dead_code)]
pub fn assert_event_sequence(events: &[Event]) {
    if events.is_empty() {
        panic!("Event sequence is empty");
    }

    // First event should be ProcessStarted
    assert!(
        matches!(events[0], Event::ProcessStarted { .. }),
        "First event should be ProcessStarted, got: {:?}",
        events[0]
    );

    // Last event should be completion or killed
    let last = events.last().unwrap();
    assert!(
        matches!(
            last,
            Event::ProcessCompleted { .. } | Event::ProcessKilled { .. }
        ),
        "Last event should be ProcessCompleted or ProcessKilled, got: {:?}",
        last
    );
}

/// Extract process ID from the first ProcessStarted event.
#[allow(dead_code)]
pub fn extract_process_id(events: &[Event]) -> Option<uuid::Uuid> {
    events.iter().find_map(|e| match e {
        Event::ProcessStarted { process_id, .. } => Some(*process_id),
        _ => None,
    })
}

/// Count events of a specific type.
#[allow(dead_code)]
pub fn count_log_chunks(events: &[Event]) -> usize {
    events
        .iter()
        .filter(|e| matches!(e, Event::ProcessLogChunk { .. }))
        .count()
}

/// Assert that a string contains a substring (case-insensitive).
#[allow(dead_code)]
pub fn assert_contains_ci(haystack: &str, needle: &str) {
    let haystack_lower = haystack.to_lowercase();
    let needle_lower = needle.to_lowercase();
    assert!(
        haystack_lower.contains(&needle_lower),
        "Expected '{}' to contain '{}' (case-insensitive)",
        haystack,
        needle
    );
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/common/mock_agents.rs">
//! Mock agent implementations for deterministic testing.

use async_trait::async_trait;
use pk_core::agents::base::Agent;
use pk_core::agents::base::AgentError;
use pk_core::agents::base::AgentEvent;
use pk_core::agents::base::ExecutionContext;
use std::pin::Pin;
use tokio_stream::Stream;

/// A mock agent that always succeeds with a predefined response.
#[allow(dead_code)]
pub struct MockSuccessAgent {
    pub name: String,
    pub response: String,
}

impl MockSuccessAgent {
    #[allow(dead_code)]
    pub fn new(name: &str, response: &str) -> Self {
        Self {
            name: name.to_string(),
            response: response.to_string(),
        }
    }
}

#[async_trait]
impl Agent for MockSuccessAgent {
    async fn check_availability(&self) -> bool {
        true
    }

    async fn execute(
        &self,
        _context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        let response = self.response.clone();

        // Create a simple stream that emits the response and completes
        let stream = async_stream::stream! {
            yield Ok(AgentEvent::MessageChunk(response));
            yield Ok(AgentEvent::Completed);
        };

        Ok(Box::pin(stream))
    }
}

/// A mock agent that always fails with a predefined error.
#[allow(dead_code)]
pub struct MockFailureAgent {
    pub name: String,
    pub error_message: String,
}

impl MockFailureAgent {
    #[allow(dead_code)]
    pub fn new(name: &str, error_message: &str) -> Self {
        Self {
            name: name.to_string(),
            error_message: error_message.to_string(),
        }
    }
}

#[async_trait]
impl Agent for MockFailureAgent {
    async fn check_availability(&self) -> bool {
        true
    }

    async fn execute(
        &self,
        _context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        let error_message = self.error_message.clone();

        let stream = async_stream::stream! {
            yield Err(AgentError::ExecutionError(error_message));
        };

        Ok(Box::pin(stream))
    }
}

/// A mock agent that delays before responding.
#[allow(dead_code)]
pub struct MockDelayedAgent {
    pub name: String,
    pub response: String,
    pub delay_ms: u64,
}

impl MockDelayedAgent {
    #[allow(dead_code)]
    pub fn new(name: &str, response: &str, delay_ms: u64) -> Self {
        Self {
            name: name.to_string(),
            response: response.to_string(),
            delay_ms,
        }
    }
}

#[async_trait]
impl Agent for MockDelayedAgent {
    async fn check_availability(&self) -> bool {
        true
    }

    async fn execute(
        &self,
        _context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        let response = self.response.clone();
        let delay_ms = self.delay_ms;

        let stream = async_stream::stream! {
            tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
            yield Ok(AgentEvent::MessageChunk(response));
            yield Ok(AgentEvent::Completed);
        };

        Ok(Box::pin(stream))
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/agent_integration.rs">
//! Integration tests for agent adapters.
//!
//! These tests verify the real CLI integration with actual agent CLIs.
//! Run with: `cargo test --features e2e-cli-tests`
//!
//! Prerequisites:
//! - Claude CLI: `claude` must be installed and configured
//! - Cursor CLI: `cursor-agent` must be installed
//! - Gemini CLI: `gemini-cli` must be installed and `GEMINI_API_KEY` set
//! - Codex CLI: `codex` must be installed and `OPENAI_API_KEY` set

use pk_core::agents::AgentFactory;
use pk_core::agents::ExecutionContext;
use pk_protocol::agent_models;
use tokio_stream::StreamExt;

/// Helper to create test agent config.
fn create_agent_config(name: &str, model: &str, system_prompt: &str) -> agent_models::Agent {
    agent_models::Agent {
        name: name.to_string(),
        model: model.to_string(),
        description: format!("Test agent: {}", name),
        color: "blue".to_string(),
        system_prompt: system_prompt.to_string(),
    }
}

/// Helper to create test execution context.
fn create_test_context(instruction: &str, project_path: &str) -> ExecutionContext {
    ExecutionContext::new(instruction.to_string())
        .with_project_path(project_path.to_string())
        .with_initial_prompt(true)
}

// =============================================================================
// Feature-gated integration tests (require actual CLI installations)
// =============================================================================

#[cfg(feature = "e2e-cli-tests")]
mod real_cli_tests {
    use super::*;

    #[tokio::test]
    #[ignore] // Run explicitly with --ignored flag
    async fn test_claude_adapter_real_execution() {
        // Check if Claude CLI is available
        let config = create_agent_config(
            "test-claude",
            "claude-sonnet-4.5",
            "You are a helpful assistant. Be concise.",
        );

        let agent = AgentFactory::create(&config).expect("Failed to create Claude agent");

        if !agent.check_availability().await {
            eprintln!("Skipping test: Claude CLI not available");
            return;
        }

        // Create temporary test directory
        let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
        let project_path = temp_dir.path().to_str().unwrap();

        let context = create_test_context("Say 'Hello' in one word", project_path);

        // Execute and collect events
        let stream = agent
            .execute(&context)
            .await
            .expect("Failed to execute agent");

        let events: Vec<_> = stream.collect().await;

        // Verify we got some events
        assert!(!events.is_empty(), "Should receive at least one event");

        // Check for successful completion
        let has_completion = events
            .iter()
            .any(|e| matches!(e, Ok(pk_core::agents::AgentEvent::Completed)));
        assert!(has_completion, "Should complete successfully");

        println!("Claude execution test passed with {} events", events.len());
    }

    #[tokio::test]
    #[ignore]
    async fn test_cursor_adapter_real_execution() {
        let config = create_agent_config(
            "test-cursor",
            "gpt-5",
            "You are a helpful coding assistant.",
        );

        let agent = AgentFactory::create(&config).expect("Failed to create Cursor agent");

        if !agent.check_availability().await {
            eprintln!("Skipping test: Cursor CLI not available");
            return;
        }

        let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
        let project_path = temp_dir.path().to_str().unwrap();

        let context =
            create_test_context("What is 2+2? Answer with just the number.", project_path);

        let stream = agent
            .execute(&context)
            .await
            .expect("Failed to execute agent");

        let events: Vec<_> = stream.collect().await;

        assert!(!events.is_empty(), "Should receive at least one event");

        // Verify AGENTS.md was created
        let agents_md_path = temp_dir.path().join("AGENTS.md");
        assert!(
            agents_md_path.exists(),
            "AGENTS.md should be created by CursorAdapter"
        );

        println!("Cursor execution test passed with {} events", events.len());
    }

    #[tokio::test]
    #[ignore]
    async fn test_gemini_adapter_real_execution() {
        let config = create_agent_config(
            "test-gemini",
            "gemini-2.5-pro",
            "You are a helpful assistant.",
        );

        let agent = AgentFactory::create(&config).expect("Failed to create Gemini agent");

        if !agent.check_availability().await {
            eprintln!("Skipping test: Gemini CLI or API key not available");
            return;
        }

        let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
        let project_path = temp_dir.path().to_str().unwrap();

        let context = create_test_context(
            "What is the capital of France? One word answer.",
            project_path,
        );

        let stream = agent
            .execute(&context)
            .await
            .expect("Failed to execute agent");

        let events: Vec<_> = stream.collect().await;

        assert!(!events.is_empty(), "Should receive at least one event");

        let has_message = events
            .iter()
            .any(|e| matches!(e, Ok(pk_core::agents::AgentEvent::MessageChunk(_))));
        assert!(has_message, "Should receive at least one message chunk");

        println!("Gemini execution test passed with {} events", events.len());
    }

    #[tokio::test]
    #[ignore]
    async fn test_codex_adapter_real_execution() {
        let config =
            create_agent_config("test-codex", "codex", "You are a helpful coding assistant.");

        let agent = AgentFactory::create(&config).expect("Failed to create Codex agent");

        if !agent.check_availability().await {
            eprintln!("Skipping test: Codex CLI or API key not available");
            return;
        }

        let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
        let project_path = temp_dir.path().to_str().unwrap();

        let context = create_test_context("Write a hello world comment in Python", project_path);

        let stream = agent
            .execute(&context)
            .await
            .expect("Failed to execute agent");

        let events: Vec<_> = stream.collect().await;

        assert!(!events.is_empty(), "Should receive at least one event");

        // Verify AGENTS.md was created
        let agents_md_path = temp_dir.path().join("AGENTS.md");
        assert!(
            agents_md_path.exists(),
            "AGENTS.md should be created by CodexAdapter"
        );

        // Verify rollout directory was created
        let rollout_dir = temp_dir.path().join(".pipeline-kit").join("codex_rollouts");
        assert!(
            rollout_dir.exists(),
            "Rollout directory should be created by CodexAdapter"
        );

        println!("Codex execution test passed with {} events", events.len());
    }

    #[tokio::test]
    #[ignore]
    async fn test_session_persistence() {
        // Test that sessions are properly managed across multiple executions
        let config = create_agent_config(
            "test-session",
            "claude-sonnet-4.5",
            "You are a helpful assistant.",
        );

        let agent = AgentFactory::create(&config).expect("Failed to create agent");

        if !agent.check_availability().await {
            eprintln!("Skipping test: Claude CLI not available");
            return;
        }

        let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
        let project_path = temp_dir.path().to_str().unwrap();

        // First execution
        let context1 = create_test_context("Remember this number: 42", project_path);
        let stream1 = agent.execute(&context1).await.expect("Failed to execute");
        let _events1: Vec<_> = stream1.collect().await;

        // Second execution (should resume session)
        let context2 = ExecutionContext::new("What number did I tell you to remember?".to_string())
            .with_project_path(project_path.to_string())
            .with_initial_prompt(false); // Not initial prompt

        let stream2 = agent.execute(&context2).await.expect("Failed to execute");
        let events2: Vec<_> = stream2.collect().await;

        assert!(
            !events2.is_empty(),
            "Should receive events from resumed session"
        );

        println!(
            "Session persistence test passed with {} events",
            events2.len()
        );
    }
}

// =============================================================================
// Mock-based integration tests (always run, no CLI required)
// =============================================================================

#[tokio::test]
async fn test_agent_factory_integration() {
    // Test that factory correctly creates different agent types
    let configs = vec![
        create_agent_config("claude", "claude-sonnet-4.5", "Test"),
        create_agent_config("cursor", "gpt-5", "Test"),
        create_agent_config("gemini", "gemini-2.5-pro", "Test"),
        create_agent_config("codex", "codex", "Test"),
        create_agent_config("mock", "unknown-model", "Test"),
    ];

    for config in configs {
        let agent = AgentFactory::create(&config);
        assert!(
            agent.is_ok(),
            "Factory should create agent for model: {}",
            config.model
        );
    }
}

#[tokio::test]
async fn test_execution_context_integration() {
    // Test that execution context is properly constructed
    let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
    let project_path = temp_dir.path().to_str().unwrap();

    let context = ExecutionContext::new("test instruction".to_string())
        .with_project_path(project_path.to_string())
        .with_initial_prompt(true);

    assert_eq!(context.instruction, "test instruction");
    assert_eq!(context.project_path, project_path);
    assert!(context.is_initial_prompt);
    assert!(context.attachments.is_empty());
}

#[tokio::test]
async fn test_mock_agent_integration() {
    // Test mock agent for development/testing
    let config = create_agent_config("mock", "test-model", "Test system prompt");
    let agent = AgentFactory::create(&config).expect("Failed to create mock agent");

    assert!(
        agent.check_availability().await,
        "Mock agent should always be available"
    );

    let temp_dir = tempfile::tempdir().expect("Failed to create temp dir");
    let project_path = temp_dir.path().to_str().unwrap();
    let context = create_test_context("Test instruction", project_path);

    let stream = agent
        .execute(&context)
        .await
        .expect("Failed to execute mock agent");

    let events: Vec<_> = stream.collect().await;

    assert!(!events.is_empty(), "Mock agent should produce events");

    // Mock agent should complete successfully
    let has_completion = events
        .iter()
        .any(|e| matches!(e, Ok(pk_core::agents::AgentEvent::Completed)));
    assert!(has_completion, "Mock agent should complete");
}
</file>

<file path="pipeline-kit-rs/crates/protocol/src/agent_models.rs">
//! Agent configuration models for `.pipeline-kit/agents/*.md`.
//!
//! This module defines the structure of agent configuration files.
//! Agents are defined as Markdown files with YAML front matter.

use serde::Deserialize;
use serde::Serialize;
use ts_rs::TS;

/// Represents an AI agent's configuration and system prompt.
///
/// Agents are defined in `.pipeline-kit/agents/*.md` files with YAML front matter
/// containing metadata, and the file body containing the system prompt.
///
/// # Example
///
/// ```markdown
/// ---
/// name: code-reviewer
/// description: Reviews code for quality and best practices
/// model: claude-sonnet-4
/// color: blue
/// ---
///
/// You are an expert code reviewer. Analyze code for:
/// - Correctness
/// - Performance
/// - Security
/// - Best practices
/// ```
#[derive(Serialize, Deserialize, Debug, Clone, TS)]
pub struct Agent {
    /// Unique identifier for this agent.
    ///
    /// Used to reference the agent in pipeline process steps.
    pub name: String,

    /// Human-readable description of the agent's purpose.
    pub description: String,

    /// AI model to use for this agent (e.g., "claude-sonnet-4", "gpt-4").
    pub model: String,

    /// UI color hint for displaying this agent in the TUI.
    ///
    /// Defaults to empty string if not specified.
    #[serde(default)]
    pub color: String,

    /// The main content of the .md file, not part of the front matter.
    ///
    /// This contains the system prompt that defines the agent's behavior.
    /// Note: This field is skipped during JSON serialization as it's not
    /// part of the front matter metadata.
    #[serde(skip)]
    pub system_prompt: String,
}
</file>

<file path="pipeline-kit-rs/crates/protocol/src/config_models.rs">
//! Global configuration models for `.pipeline-kit/config.toml`.
//!
//! This module defines the structure of the global configuration file that
//! controls project-wide settings for pipeline-kit.

use serde::Deserialize;
use serde::Serialize;
use ts_rs::TS;

/// Represents global settings from `.pipeline-kit/config.toml`.
///
/// This structure contains project-wide configuration options that affect
/// all pipeline executions.
///
/// # Example
///
/// ```toml
/// # .pipeline-kit/config.toml
/// git = true
/// ```
#[derive(Serialize, Deserialize, Debug, Clone, TS)]
pub struct GlobalConfig {
    /// Enable git integration for tracking pipeline changes.
    ///
    /// When enabled, pipeline-kit will automatically track changes made
    /// during pipeline execution in git.
    #[serde(default)]
    pub git: bool,
}
</file>

<file path="pipeline-kit-rs/crates/protocol/src/lib.rs">
//! # pk-protocol
//!
//! Core protocol definitions and data models for pipeline-kit.
//!
//! This crate defines all shared data structures used for:
//! - Configuration file parsing (YAML pipelines, TOML config, Markdown agents)
//! - Runtime process state management
//! - Inter-process communication between TUI and Core
//!
//! ## Modules
//!
//! - [`agent_models`]: Agent configuration structures
//! - [`config_models`]: Global configuration from config.toml
//! - [`pipeline_models`]: Pipeline definitions and process steps
//! - [`process_models`]: Runtime process state and status
//! - [`ipc`]: Operations and Events for Core-TUI communication
//!
//! ## Design Principles
//!
//! - Minimal dependencies: Only serde, ts-rs, and uuid
//! - TypeScript generation: All types derive `TS` for client compatibility
//! - Independent compilation: No dependencies on other pipeline-kit crates

pub mod agent_models;
pub mod config_models;
pub mod ipc;
pub mod pipeline_models;
pub mod process_models;

// Re-export all public types for convenience
pub use agent_models::*;
pub use config_models::*;
pub use ipc::*;
pub use pipeline_models::*;
pub use process_models::*;
</file>

<file path="pipeline-kit-rs/crates/protocol/src/pipeline_models.rs">
//! Pipeline configuration models for `.pipeline-kit/pipelines/*.yaml`.
//!
//! This module defines the structure of pipeline definition files that
//! orchestrate multi-agent workflows.

use serde::Deserialize;
use serde::Serialize;
use std::collections::HashMap;
use ts_rs::TS;

/// Represents a single step in a pipeline's process.
///
/// A process step can be either:
/// - An agent execution (referenced by agent name)
/// - A special command like HUMAN_REVIEW that pauses for manual intervention
///
/// The enum uses `#[serde(untagged)]` to allow flexible YAML syntax where
/// steps can be simple strings.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, TS)]
#[serde(untagged)]
pub enum ProcessStep {
    /// A special step that pauses the pipeline for human review.
    ///
    /// When encountered, the pipeline execution stops and waits for
    /// manual approval before proceeding to the next step.
    HumanReview(HumanReviewMarker),

    /// Execute a specific agent by name.
    ///
    /// The string should match the `name` field of an agent defined
    /// in `.pipeline-kit/agents/*.md`.
    Agent(String),
}

/// Marker type for HUMAN_REVIEW step that deserializes from the literal string "HUMAN_REVIEW".
///
/// This type ensures that only the exact string "HUMAN_REVIEW" is accepted
/// when deserializing a human review step from YAML.
#[derive(Debug, Clone, PartialEq, Eq, TS)]
pub struct HumanReviewMarker;

impl<'de> Deserialize<'de> for HumanReviewMarker {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        if s == "HUMAN_REVIEW" {
            Ok(HumanReviewMarker)
        } else {
            Err(serde::de::Error::custom(format!(
                "expected HUMAN_REVIEW, got {}",
                s
            )))
        }
    }
}

impl Serialize for HumanReviewMarker {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        serializer.serialize_str("HUMAN_REVIEW")
    }
}

/// Defines the configuration for the master agent orchestrating the pipeline.
///
/// The master agent is responsible for coordinating the execution of all
/// sub-agents and managing the overall pipeline flow.
#[derive(Serialize, Deserialize, Debug, Clone, TS)]
#[serde(rename_all = "kebab-case")]
pub struct MasterAgentConfig {
    /// AI model to use for the master agent.
    pub model: String,

    /// System prompt defining the master agent's orchestration behavior.
    pub system_prompt: String,

    /// Sequential list of process steps to execute.
    ///
    /// Each step is either an agent name or a special command like HUMAN_REVIEW.
    pub process: Vec<ProcessStep>,
}

/// Defines a full pipeline, including its agents and process flow.
///
/// Pipelines are defined in `.pipeline-kit/pipelines/*.yaml` files and specify
/// the complete workflow for a multi-agent task.
///
/// # Example
///
/// ```yaml
/// name: code-review-pipeline
/// required-reference-file:
///   1: "docs/coding-standards.md"
///   2: "docs/security-checklist.md"
/// output-file:
///   1: "review-report.md"
/// master:
///   model: "claude-sonnet-4"
///   system-prompt: "Orchestrate a thorough code review process"
///   process:
///     - "static-analyzer"
///     - "security-reviewer"
///     - "HUMAN_REVIEW"
///     - "final-reporter"
/// sub-agents:
///   - "static-analyzer"
///   - "security-reviewer"
///   - "final-reporter"
/// ```
#[derive(Serialize, Deserialize, Debug, Clone, TS)]
#[serde(rename_all = "kebab-case")]
pub struct Pipeline {
    /// Unique name identifying this pipeline.
    pub name: String,

    /// Map of step index to required reference file paths.
    ///
    /// These files provide context to agents at specific pipeline steps.
    /// The key is the 1-based step index.
    #[serde(default)]
    pub required_reference_file: HashMap<u32, String>,

    /// Map of step index to output file paths.
    ///
    /// Specifies where agents should write their output at each step.
    /// The key is the 1-based step index.
    #[serde(default)]
    pub output_file: HashMap<u32, String>,

    /// Configuration for the master orchestrator agent.
    pub master: MasterAgentConfig,

    /// List of sub-agent names that can be used in the process.
    ///
    /// All agent names referenced in `master.process` should be listed here.
    pub sub_agents: Vec<String>,
}
</file>

<file path="pipeline-kit-rs/crates/protocol-ts/src/lib.rs">

</file>

<file path="pipeline-kit-rs/crates/protocol-ts/Cargo.toml">
[package]
name = "pk-protocol-ts"
version = { workspace = true }
edition = { workspace = true }
authors = { workspace = true }
license = { workspace = true }
repository = { workspace = true }
description = "TypeScript type generator for pk-protocol"
</file>

<file path="pipeline-kit-rs/crates/tui/src/widgets/detail_view.rs">
//! Detail view widget for displaying process logs with scrolling support.
//!
//! This widget displays the logs and details of a selected process in a scrollable view.
//! It supports keyboard navigation (j/k, PageUp/PageDown) and shows a scrollbar to indicate position.

use pk_protocol::Process;
use ratatui::layout::Rect;
use ratatui::widgets::Block;
use ratatui::widgets::Borders;
use ratatui::widgets::Paragraph;
use ratatui::widgets::Scrollbar;
use ratatui::widgets::ScrollbarOrientation;
use ratatui::widgets::ScrollbarState;
use ratatui::Frame;

/// Widget for displaying process details with scrolling support.
pub struct DetailView {
    /// Current scroll offset (number of lines scrolled from the top).
    pub scroll_offset: usize,
}

impl DetailView {
    /// Create a new DetailView with scroll offset at the top.
    pub fn new() -> Self {
        Self { scroll_offset: 0 }
    }

    /// Render the detail view for a given process.
    ///
    /// # Arguments
    ///
    /// * `frame` - The ratatui frame to render to
    /// * `area` - The area to render within
    /// * `process` - Optional reference to the process to display
    pub fn render(&self, frame: &mut Frame, area: Rect, process: Option<&Process>) {
        let block = Block::default()
            .borders(Borders::ALL)
            .title("Detail - Process Logs");

        let text = if let Some(process) = process {
            if process.logs.is_empty() {
                "No logs yet.".to_string()
            } else {
                process.logs.join("\n")
            }
        } else {
            "No process selected.".to_string()
        };

        let paragraph = Paragraph::new(text)
            .block(block)
            .scroll((self.scroll_offset as u16, 0));

        frame.render_widget(paragraph, area);

        // Render scrollbar if there is content to scroll
        if let Some(process) = process {
            if !process.logs.is_empty() {
                let total_lines = process.logs.len();
                let visible_lines = area.height.saturating_sub(2) as usize; // Subtract 2 for borders

                // Only show scrollbar if content exceeds visible area
                if total_lines > visible_lines {
                    let mut scrollbar_state = ScrollbarState::default()
                        .content_length(total_lines)
                        .viewport_content_length(visible_lines)
                        .position(self.scroll_offset);

                    let scrollbar = Scrollbar::default()
                        .orientation(ScrollbarOrientation::VerticalRight)
                        .begin_symbol(Some("‚Üë"))
                        .end_symbol(Some("‚Üì"));

                    frame.render_stateful_widget(scrollbar, area, &mut scrollbar_state);
                }
            }
        }
    }

    /// Scroll up by one line.
    pub fn scroll_up(&mut self) {
        self.scroll_offset = self.scroll_offset.saturating_sub(1);
    }

    /// Scroll down by one line.
    ///
    /// # Arguments
    ///
    /// * `max` - The maximum scroll offset (typically total_lines - visible_lines)
    pub fn scroll_down(&mut self, max: usize) {
        self.scroll_offset = (self.scroll_offset + 1).min(max);
    }

    /// Scroll up by a page (viewport height).
    ///
    /// # Arguments
    ///
    /// * `page_size` - Number of lines in a page
    pub fn page_up(&mut self, page_size: usize) {
        self.scroll_offset = self.scroll_offset.saturating_sub(page_size);
    }

    /// Scroll down by a page (viewport height).
    ///
    /// # Arguments
    ///
    /// * `page_size` - Number of lines in a page
    /// * `max` - The maximum scroll offset
    pub fn page_down(&mut self, page_size: usize, max: usize) {
        self.scroll_offset = (self.scroll_offset + page_size).min(max);
    }

    /// Reset scroll to the top.
    pub fn scroll_to_top(&mut self) {
        self.scroll_offset = 0;
    }

    /// Scroll to the bottom.
    ///
    /// # Arguments
    ///
    /// * `max` - The maximum scroll offset
    pub fn scroll_to_bottom(&mut self, max: usize) {
        self.scroll_offset = max;
    }
}

impl Default for DetailView {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use pk_protocol::ProcessStatus;
    use ratatui::backend::TestBackend;
    use ratatui::Terminal;
    use uuid::Uuid;

    fn create_test_process(logs: Vec<String>) -> Process {
        use std::sync::Arc;
        use tokio::sync::Notify;

        Process {
            id: Uuid::new_v4(),
            pipeline_name: "test-pipeline".to_string(),
            status: ProcessStatus::Running,
            current_step_index: 0,
            started_at: chrono::Utc::now(),
            completed_at: None,
            logs,
            resume_notifier: Arc::new(Notify::new()),
        }
    }

    #[test]
    fn test_detail_view_renders_empty_state() {
        let detail_view = DetailView::new();
        let backend = TestBackend::new(80, 24);
        let mut terminal = Terminal::new(backend).unwrap();

        terminal
            .draw(|frame| {
                detail_view.render(frame, frame.area(), None);
            })
            .unwrap();

        let buffer = terminal.backend().buffer();
        let content = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();

        assert!(content.contains("No process selected"));
        assert!(content.contains("Detail - Process Logs"));
    }

    #[test]
    fn test_detail_view_renders_process_logs() {
        let detail_view = DetailView::new();
        let process = create_test_process(vec![
            "Log line 1".to_string(),
            "Log line 2".to_string(),
            "Log line 3".to_string(),
        ]);

        let backend = TestBackend::new(80, 24);
        let mut terminal = Terminal::new(backend).unwrap();

        terminal
            .draw(|frame| {
                detail_view.render(frame, frame.area(), Some(&process));
            })
            .unwrap();

        let buffer = terminal.backend().buffer();
        let content = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();

        assert!(content.contains("Log line 1"));
        assert!(content.contains("Log line 2"));
        assert!(content.contains("Log line 3"));
    }

    #[test]
    fn test_detail_view_scroll_up() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 5;

        detail_view.scroll_up();
        assert_eq!(detail_view.scroll_offset, 4);

        detail_view.scroll_up();
        assert_eq!(detail_view.scroll_offset, 3);
    }

    #[test]
    fn test_detail_view_scroll_up_at_top() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 0;

        detail_view.scroll_up();
        assert_eq!(detail_view.scroll_offset, 0); // Should not go below 0
    }

    #[test]
    fn test_detail_view_scroll_down() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 0;

        detail_view.scroll_down(10);
        assert_eq!(detail_view.scroll_offset, 1);

        detail_view.scroll_down(10);
        assert_eq!(detail_view.scroll_offset, 2);
    }

    #[test]
    fn test_detail_view_scroll_down_at_max() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 10;

        detail_view.scroll_down(10);
        assert_eq!(detail_view.scroll_offset, 10); // Should not exceed max
    }

    #[test]
    fn test_detail_view_page_up() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 20;

        detail_view.page_up(10);
        assert_eq!(detail_view.scroll_offset, 10);

        detail_view.page_up(10);
        assert_eq!(detail_view.scroll_offset, 0);
    }

    #[test]
    fn test_detail_view_page_down() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 0;

        detail_view.page_down(10, 50);
        assert_eq!(detail_view.scroll_offset, 10);

        detail_view.page_down(10, 50);
        assert_eq!(detail_view.scroll_offset, 20);
    }

    #[test]
    fn test_detail_view_scroll_to_top() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 42;

        detail_view.scroll_to_top();
        assert_eq!(detail_view.scroll_offset, 0);
    }

    #[test]
    fn test_detail_view_scroll_to_bottom() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 0;

        detail_view.scroll_to_bottom(100);
        assert_eq!(detail_view.scroll_offset, 100);
    }

    #[test]
    fn test_detail_view_renders_with_scroll_offset() {
        let mut detail_view = DetailView::new();
        detail_view.scroll_offset = 2;

        let process = create_test_process(vec![
            "Line 0".to_string(),
            "Line 1".to_string(),
            "Line 2".to_string(),
            "Line 3".to_string(),
            "Line 4".to_string(),
        ]);

        let backend = TestBackend::new(80, 10);
        let mut terminal = Terminal::new(backend).unwrap();

        terminal
            .draw(|frame| {
                detail_view.render(frame, frame.area(), Some(&process));
            })
            .unwrap();

        let buffer = terminal.backend().buffer();
        let content = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();

        // With scroll_offset=2, we should see Line 2, 3, 4 but not Line 0, 1
        assert!(content.contains("Line 2"));
        assert!(content.contains("Line 3"));
        assert!(content.contains("Line 4"));
    }
}
</file>

<file path="pipeline-kit-rs/crates/tui/src/main.rs">
//! Main entry point for the pk-tui binary.
//!
//! This executable provides a standalone TUI for pipeline-kit.

use anyhow::Result;
use pk_tui::run_app;

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize and run the TUI application
    run_app().await
}
</file>

<file path="pipeline-kit-rs/crates/tui/src/tui.rs">
//! Terminal UI initialization and event handling.
//!
//! This module provides the `Tui` wrapper around ratatui's Terminal,
//! handling raw mode setup, event streaming, and frame scheduling.

use anyhow::Result;
use crossterm::event::DisableBracketedPaste;
use crossterm::event::EnableBracketedPaste;
use crossterm::event::Event;
use crossterm::event::KeyEvent;
use crossterm::execute;
use crossterm::terminal::disable_raw_mode;
use crossterm::terminal::enable_raw_mode;
use crossterm::terminal::EnterAlternateScreen;
use crossterm::terminal::LeaveAlternateScreen;
use ratatui::backend::CrosstermBackend;
use ratatui::Terminal;
use std::io::stdout;
use std::io::Stdout;
use std::pin::Pin;
use std::time::Duration;
use std::time::Instant;
use tokio::select;
use tokio_stream::Stream;
use tokio_stream::StreamExt;

/// Type alias for the terminal backend we're using.
pub type TerminalBackend = CrosstermBackend<Stdout>;

/// TUI events that can be emitted.
#[derive(Debug)]
pub enum TuiEvent {
    /// Keyboard event.
    Key(KeyEvent),
    /// Paste event (from bracketed paste).
    Paste(String),
    /// Draw event (triggered by frame scheduler).
    Draw,
}

/// Main TUI wrapper.
pub struct Tui {
    /// The underlying ratatui terminal.
    terminal: Terminal<TerminalBackend>,
    /// Channel for scheduling frames.
    frame_schedule_tx: tokio::sync::mpsc::UnboundedSender<Instant>,
    /// Broadcast channel for draw events.
    draw_tx: tokio::sync::broadcast::Sender<()>,
}

impl Tui {
    /// Initialize the terminal in raw mode.
    pub fn init() -> Result<Self> {
        // Enable raw mode and bracketed paste
        enable_raw_mode()?;
        execute!(stdout(), EnableBracketedPaste)?;
        execute!(stdout(), EnterAlternateScreen)?;

        // Set panic hook to restore terminal on panic
        set_panic_hook();

        // Create terminal backend
        let backend = CrosstermBackend::new(stdout());
        let terminal = Terminal::new(backend)?;

        // Create frame scheduling channels
        let (frame_schedule_tx, frame_schedule_rx) = tokio::sync::mpsc::unbounded_channel();
        let (draw_tx, _) = tokio::sync::broadcast::channel(1);

        // Spawn background task to coalesce frame requests
        let draw_tx_clone = draw_tx.clone();
        tokio::spawn(async move {
            use tokio::time::sleep_until;
            use tokio::time::Instant as TokioInstant;

            let mut rx = frame_schedule_rx;
            let mut next_deadline: Option<Instant> = None;

            loop {
                let target =
                    next_deadline.unwrap_or_else(|| Instant::now() + Duration::from_secs(3600));
                let sleep_fut = sleep_until(TokioInstant::from_std(target));
                tokio::pin!(sleep_fut);

                select! {
                    recv = rx.recv() => {
                        match recv {
                            Some(at) => {
                                if next_deadline.is_none() || at < next_deadline.unwrap() {
                                    next_deadline = Some(at);
                                }
                                continue;
                            }
                            None => break,
                        }
                    }
                    _ = &mut sleep_fut => {
                        if next_deadline.is_some() {
                            next_deadline = None;
                            let _ = draw_tx_clone.send(());
                        }
                    }
                }
            }
        });

        Ok(Self {
            terminal,
            frame_schedule_tx,
            draw_tx,
        })
    }

    /// Restore the terminal to its original state.
    pub fn restore(&mut self) -> Result<()> {
        disable_raw_mode()?;
        execute!(stdout(), DisableBracketedPaste)?;
        execute!(stdout(), LeaveAlternateScreen)?;
        Ok(())
    }

    /// Get a frame requester for scheduling draws.
    pub fn frame_requester(&self) -> FrameRequester {
        FrameRequester {
            frame_schedule_tx: self.frame_schedule_tx.clone(),
        }
    }

    /// Create an event stream for TUI events.
    pub fn event_stream(&self) -> Pin<Box<dyn Stream<Item = TuiEvent> + Send + 'static>> {
        let mut crossterm_events = crossterm::event::EventStream::new();
        let mut draw_rx = self.draw_tx.subscribe();

        let event_stream = async_stream::stream! {
            loop {
                select! {
                    Some(Ok(event)) = crossterm_events.next() => {
                        match event {
                            Event::Key(key_event) => {
                                yield TuiEvent::Key(key_event);
                            }
                            Event::Resize(_, _) => {
                                yield TuiEvent::Draw;
                            }
                            Event::Paste(pasted) => {
                                yield TuiEvent::Paste(pasted);
                            }
                            _ => {}
                        }
                    }
                    result = draw_rx.recv() => {
                        match result {
                            Ok(_) => {
                                yield TuiEvent::Draw;
                            }
                            Err(tokio::sync::broadcast::error::RecvError::Lagged(_)) => {
                                // Coalesce lagged events into a single draw
                                yield TuiEvent::Draw;
                            }
                            Err(tokio::sync::broadcast::error::RecvError::Closed) => {
                                // Sender dropped; stop
                                break;
                            }
                        }
                    }
                }
            }
        };

        Box::pin(event_stream)
    }

    /// Draw the UI with the provided function.
    pub fn draw<F>(&mut self, f: F) -> Result<()>
    where
        F: FnOnce(&mut ratatui::Frame),
    {
        self.terminal.draw(f)?;
        Ok(())
    }

    /// Clear the terminal.
    pub fn clear(&mut self) -> Result<()> {
        self.terminal.clear()?;
        Ok(())
    }
}

impl Drop for Tui {
    fn drop(&mut self) {
        let _ = self.restore();
    }
}

/// Handle for scheduling frame redraws.
#[derive(Clone, Debug)]
pub struct FrameRequester {
    frame_schedule_tx: tokio::sync::mpsc::UnboundedSender<Instant>,
}

impl FrameRequester {
    /// Schedule a frame to be drawn immediately.
    pub fn schedule_frame(&self) {
        let _ = self.frame_schedule_tx.send(Instant::now());
    }

    /// Schedule a frame to be drawn after a delay.
    pub fn schedule_frame_in(&self, dur: Duration) {
        let _ = self.frame_schedule_tx.send(Instant::now() + dur);
    }
}

/// Set a panic hook that restores the terminal before panicking.
fn set_panic_hook() {
    let original_hook = std::panic::take_hook();
    std::panic::set_hook(Box::new(move |panic_info| {
        let _ = disable_raw_mode();
        let _ = execute!(stdout(), DisableBracketedPaste);
        let _ = execute!(stdout(), LeaveAlternateScreen);
        original_hook(panic_info);
    }));
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_frame_requester_creation() {
        let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();
        let requester = FrameRequester {
            frame_schedule_tx: tx,
        };
        // Should not panic
        requester.schedule_frame();
    }
}
</file>

<file path="scripts/asciicheck.py">
#!/usr/bin/env python3
"""
ASCII checker for README files.
Ensures that README files contain only ASCII characters to prevent encoding issues.
Allows specific non-ASCII characters commonly used in documentation.
"""

import sys
import os


# Allowlist of non-ASCII characters permitted in README files
ALLOWED_CHARS = set([
    # Box drawing characters (for tree structures and diagrams)
    0x2500,  # ‚îÄ (BOX DRAWINGS LIGHT HORIZONTAL)
    0x2502,  # ‚îÇ (BOX DRAWINGS LIGHT VERTICAL)
    0x250C,  # ‚îå (BOX DRAWINGS LIGHT DOWN AND RIGHT)
    0x2510,  # ‚îê (BOX DRAWINGS LIGHT DOWN AND LEFT)
    0x2514,  # ‚îî (BOX DRAWINGS LIGHT UP AND RIGHT)
    0x2518,  # ‚îò (BOX DRAWINGS LIGHT UP AND LEFT)
    0x251C,  # ‚îú (BOX DRAWINGS LIGHT VERTICAL AND RIGHT)
    0x2524,  # ‚î§ (BOX DRAWINGS LIGHT VERTICAL AND LEFT)
    0x252C,  # ‚î¨ (BOX DRAWINGS LIGHT DOWN AND HORIZONTAL)
    0x2534,  # ‚î¥ (BOX DRAWINGS LIGHT UP AND HORIZONTAL)
    0x253C,  # ‚îº (BOX DRAWINGS LIGHT VERTICAL AND HORIZONTAL)

    # Double box drawing characters
    0x2550,  # ‚ïê (BOX DRAWINGS DOUBLE HORIZONTAL)
    0x2551,  # ‚ïë (BOX DRAWINGS DOUBLE VERTICAL)
    0x2554,  # ‚ïî (BOX DRAWINGS DOUBLE DOWN AND RIGHT)
    0x2557,  # ‚ïó (BOX DRAWINGS DOUBLE DOWN AND LEFT)
    0x255A,  # ‚ïö (BOX DRAWINGS DOUBLE UP AND RIGHT)
    0x255D,  # ‚ïù (BOX DRAWINGS DOUBLE UP AND LEFT)
    0x2560,  # ‚ï† (BOX DRAWINGS DOUBLE VERTICAL AND RIGHT)
    0x2563,  # ‚ï£ (BOX DRAWINGS DOUBLE VERTICAL AND LEFT)
    0x2566,  # ‚ï¶ (BOX DRAWINGS DOUBLE DOWN AND HORIZONTAL)
    0x2569,  # ‚ï© (BOX DRAWINGS DOUBLE UP AND HORIZONTAL)
    0x256C,  # ‚ï¨ (BOX DRAWINGS DOUBLE VERTICAL AND HORIZONTAL)

    # Block elements
    0x2580,  # ‚ñÄ (UPPER HALF BLOCK)
    0x2584,  # ‚ñÑ (LOWER HALF BLOCK)
    0x2588,  # ‚ñà (FULL BLOCK)
    0x258C,  # ‚ñå (LEFT HALF BLOCK)
    0x2590,  # ‚ñê (RIGHT HALF BLOCK)
    0x2591,  # ‚ñë (LIGHT SHADE)
    0x2592,  # ‚ñí (MEDIUM SHADE)
    0x2593,  # ‚ñì (DARK SHADE)

    # Arrows
    0x2190,  # ‚Üê (LEFTWARDS ARROW)
    0x2191,  # ‚Üë (UPWARDS ARROW)
    0x2192,  # ‚Üí (RIGHTWARDS ARROW)
    0x2193,  # ‚Üì (DOWNWARDS ARROW)
    0x2194,  # ‚Üî (LEFT RIGHT ARROW)
    0x2195,  # ‚Üï (UP DOWN ARROW)
    0x21D0,  # ‚áê (LEFTWARDS DOUBLE ARROW)
    0x21D1,  # ‚áë (UPWARDS DOUBLE ARROW)
    0x21D2,  # ‚áí (RIGHTWARDS DOUBLE ARROW)
    0x21D3,  # ‚áì (DOWNWARDS DOUBLE ARROW)
    0x21D4,  # ‚áî (LEFT RIGHT DOUBLE ARROW)

    # Triangle arrows
    0x25C0,  # ‚óÄ (BLACK LEFT-POINTING TRIANGLE)
    0x25B6,  # ‚ñ∂ (BLACK RIGHT-POINTING TRIANGLE)
    0x25B2,  # ‚ñ≤ (BLACK UP-POINTING TRIANGLE)
    0x25BC,  # ‚ñº (BLACK DOWN-POINTING TRIANGLE)
    0x25C4,  # ‚óÑ (BLACK LEFT-POINTING POINTER)
    0x25BA,  # ‚ñ∫ (BLACK RIGHT-POINTING POINTER)

    # Status symbols
    0x2713,  # ‚úì (CHECK MARK)
    0x2714,  # ‚úî (HEAVY CHECK MARK)
    0x2705,  # ‚úÖ (WHITE HEAVY CHECK MARK)
    0x2717,  # ‚úó (BALLOT X)
    0x2718,  # ‚úò (HEAVY BALLOT X)
    0x274C,  # ‚ùå (CROSS MARK)
    0x26A0,  # ‚ö† (WARNING SIGN)
    0x26A1,  # ‚ö° (HIGH VOLTAGE SIGN)
    0x2139,  # ‚Ñπ (INFORMATION SOURCE)

    # Common emojis for README
    0x2764,  # ‚ù§ (HEAVY BLACK HEART)
    0x1F4A1, # üí° (ELECTRIC LIGHT BULB)
    0x1F680, # üöÄ (ROCKET)
    0x1F4E6, # üì¶ (PACKAGE)
    0x1F4DD, # üìù (MEMO)
    0x1F527, # üîß (WRENCH)
    0x1F3AF, # üéØ (DIRECT HIT)
    0x1F525, # üî• (FIRE)
    0x2B50,  # ‚≠ê (WHITE MEDIUM STAR)
    0xFE0F,  # (VARIATION SELECTOR-16, for emoji rendering)

    # Bullets and symbols
    0x2022,  # ‚Ä¢ (BULLET)
    0x00B7,  # ¬∑ (MIDDLE DOT)
    0x2605,  # ‚òÖ (BLACK STAR)
    0x2606,  # ‚òÜ (WHITE STAR)
    0x25CB,  # ‚óã (WHITE CIRCLE)
    0x25CF,  # ‚óè (BLACK CIRCLE)
    0x2122,  # ‚Ñ¢ (TRADE MARK SIGN)
    0x00A9,  # ¬© (COPYRIGHT SIGN)
    0x00AE,  # ¬Æ (REGISTERED SIGN)
])


def check_ascii(file_path):
    """Check if a file contains only ASCII characters (or allowed non-ASCII chars)."""
    if not os.path.exists(file_path):
        print(f"‚ùå File not found: {file_path}")
        return False

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    non_ascii_chars = []
    for line_num, line in enumerate(content.split('\n'), 1):
        for char_pos, char in enumerate(line, 1):
            char_code = ord(char)
            if char_code > 127 and char_code not in ALLOWED_CHARS:
                non_ascii_chars.append({
                    'line': line_num,
                    'col': char_pos,
                    'char': char,
                    'code': char_code
                })

    if non_ascii_chars:
        print(f"‚ùå Non-ASCII characters found in {file_path}:")
        for item in non_ascii_chars[:10]:  # Show first 10
            print(f"   Line {item['line']}, Column {item['col']}: "
                  f"'{item['char']}' (U+{item['code']:04X})")
        if len(non_ascii_chars) > 10:
            print(f"   ... and {len(non_ascii_chars) - 10} more")
        return False

    print(f"‚úÖ {file_path} contains only ASCII characters (or allowed symbols)")
    return True


def main():
    if len(sys.argv) < 2:
        print("Usage: python3 asciicheck.py <file_path> [<file_path2> ...]")
        sys.exit(1)

    all_passed = True
    for file_path in sys.argv[1:]:
        if not check_ascii(file_path):
            all_passed = False

    if not all_passed:
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/pre-release-check.sh">
#!/bin/bash
# Pre-release validation script
# Runs all automated checks from the release checklist

set -e  # Exit on any error

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Track overall status
CHECKS_PASSED=0
CHECKS_TOTAL=5

print_step() {
    echo -e "\n${BLUE}==>${NC} $1"
}

print_success() {
    echo -e "${GREEN}‚úì${NC} $1"
    CHECKS_PASSED=$((CHECKS_PASSED + 1))
}

print_error() {
    echo -e "${RED}‚úó${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}!${NC} $1"
}

# Get project root (assuming script is in scripts/)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
cd "$PROJECT_ROOT"

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}Pipeline Kit - Pre-Release Check${NC}"
echo -e "${BLUE}========================================${NC}"

# 1. Code Formatting
print_step "Checking code formatting..."
cd pipeline-kit-rs
if cargo fmt --all --check; then
    print_success "Code formatting check passed"
else
    print_error "Code formatting check failed. Run 'cargo fmt --all' to fix."
    exit 1
fi
cd ..

# 2. Rust Tests
print_step "Running Rust tests..."
cd pipeline-kit-rs
if cargo test --workspace --quiet; then
    print_success "All Rust tests passed"
else
    print_error "Rust tests failed"
    exit 1
fi
cd ..

# 3. Clippy
print_step "Running Clippy linter..."
cd pipeline-kit-rs
if cargo clippy --all-targets --quiet -- -D warnings; then
    print_success "No Clippy warnings"
else
    print_error "Clippy found issues"
    exit 1
fi
cd ..

# 4. Release Build
print_step "Building release binary..."
cd pipeline-kit-rs
if cargo build --release --quiet; then
    print_success "Release build successful"
else
    print_error "Release build failed"
    exit 1
fi
cd ..

# 5. TypeScript Tests
print_step "Running TypeScript tests..."
cd pipeline-kit-cli
if [ -f "package.json" ]; then
    # Check if npm or pnpm is available
    if command -v pnpm &> /dev/null; then
        if pnpm test; then
            print_success "TypeScript tests passed"
        else
            print_error "TypeScript tests failed"
            exit 1
        fi
    elif command -v npm &> /dev/null; then
        if npm test; then
            print_success "TypeScript tests passed"
        else
            print_error "TypeScript tests failed"
            exit 1
        fi
    else
        print_warning "Neither npm nor pnpm found, skipping TypeScript tests"
        ((CHECKS_TOTAL--))
    fi
else
    print_warning "package.json not found, skipping TypeScript tests"
    ((CHECKS_TOTAL--))
fi
cd ..

# Summary
echo -e "\n${BLUE}========================================${NC}"
echo -e "${GREEN}Automated Checks: ${CHECKS_PASSED}/${CHECKS_TOTAL} passed${NC}"
echo -e "${BLUE}========================================${NC}"

# Manual checks reminder
echo -e "\n${YELLOW}Manual checks required:${NC}"
echo "  [ ] CHANGELOG.md updated with new version"
echo "  [ ] README.md version information verified"
echo ""

if [ $CHECKS_PASSED -eq $CHECKS_TOTAL ]; then
    echo -e "${GREEN}‚úì All automated checks passed!${NC}"
    echo -e "Please verify the manual checks above before releasing."
    exit 0
else
    echo -e "${RED}‚úó Some checks failed. Please fix the issues before releasing.${NC}"
    exit 1
fi
</file>

<file path=".codespellignore">
# Add words that codespell should ignore (one per line)
# Common technical terms that might be flagged
slowy
crate
ratatui
</file>

<file path=".pipeline-kit/config.toml">
# Pipeline Kit Global Configuration

# Enable git integration
git = true
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/gemini_adapter.rs">
//! Gemini adapter implementation using JSON-RPC via stdio.
//!
//! This adapter communicates with the Gemini CLI using JSON-RPC protocol
//! over stdin/stdout pipes.

use crate::agents::base::Agent;
use crate::agents::base::AgentError;
use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use async_trait::async_trait;
use serde::Deserialize;
use serde::Serialize;
use std::pin::Pin;
use std::process::Stdio;
use tokio::io::AsyncBufReadExt;
use tokio::io::AsyncWriteExt;
use tokio::io::BufReader;
use tokio::process::Command;
use tokio_stream::Stream;
use tokio_stream::StreamExt;

/// Gemini adapter for executing instructions using Gemini CLI.
///
/// This adapter spawns the `gemini-cli` process and communicates via JSON-RPC.
pub struct GeminiAdapter {
    #[allow(dead_code)]
    name: String,
    model: String,
    system_prompt: String,
}

impl GeminiAdapter {
    /// Create a new Gemini adapter.
    ///
    /// # Arguments
    ///
    /// * `name` - The agent name from configuration
    /// * `model` - The Gemini model to use (e.g., "gemini-2.5-pro")
    /// * `system_prompt` - The system prompt for the agent
    pub fn new(name: String, model: String, system_prompt: String) -> Result<Self, AgentError> {
        Ok(Self {
            name,
            model,
            system_prompt,
        })
    }
}

#[async_trait]
impl Agent for GeminiAdapter {
    async fn check_availability(&self) -> bool {
        // Check if gemini-cli is installed
        let cli_available = Command::new("gemini-cli")
            .arg("--version")
            .stdout(Stdio::null())
            .stderr(Stdio::null())
            .status()
            .await
            .map(|s| s.success())
            .unwrap_or(false);

        // Check if GEMINI_API_KEY is set
        let api_key_available = std::env::var("GEMINI_API_KEY").is_ok();

        cli_available && api_key_available
    }

    async fn execute(
        &self,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        // 1. Spawn gemini-cli process with stdin/stdout pipes
        let mut child = Command::new("gemini-cli")
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(&context.project_path)
            .spawn()
            .map_err(|e| {
                AgentError::ExecutionError(format!("Failed to spawn gemini-cli: {}", e))
            })?;

        let mut stdin = child
            .stdin
            .take()
            .ok_or_else(|| AgentError::ExecutionError("Failed to capture stdin".to_string()))?;

        let stdout = child
            .stdout
            .take()
            .ok_or_else(|| AgentError::ExecutionError("Failed to capture stdout".to_string()))?;

        // 2. Create JSON-RPC request
        let request = JsonRpcRequest {
            jsonrpc: "2.0".to_string(),
            id: 1,
            method: "generate".to_string(),
            params: GenerateParams {
                model: self.model.clone(),
                system: self.system_prompt.clone(),
                prompt: context.instruction.clone(),
            },
        };

        // 3. Send request to Gemini CLI
        let request_str = serde_json::to_string(&request).map_err(|e| {
            AgentError::ExecutionError(format!("Failed to serialize request: {}", e))
        })?;

        stdin
            .write_all(request_str.as_bytes())
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write to stdin: {}", e)))?;
        stdin
            .write_all(b"\n")
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write newline: {}", e)))?;
        stdin
            .flush()
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to flush stdin: {}", e)))?;

        // Close stdin to signal end of input
        drop(stdin);

        // 4. Create stream from stdout
        let reader = BufReader::new(stdout);
        let lines = reader.lines();
        let lines_stream = tokio_stream::wrappers::LinesStream::new(lines);

        // 5. Parse JSON-RPC responses and convert to AgentEvents
        let events_stream = lines_stream
            .then(|line_result| async move {
                match line_result {
                    Ok(line) => {
                        if line.trim().is_empty() {
                            return None;
                        }

                        match serde_json::from_str::<JsonRpcResponse>(&line) {
                            Ok(response) => convert_gemini_response(response),
                            Err(e) => Some(Err(AgentError::StreamParseError(format!(
                                "Failed to parse JSON-RPC response: {} (line: {})",
                                e, line
                            )))),
                        }
                    }
                    Err(e) => Some(Err(AgentError::StreamParseError(e.to_string()))),
                }
            })
            .filter_map(|opt| opt);

        Ok(Box::pin(events_stream))
    }
}

/// JSON-RPC request structure.
#[derive(Debug, Serialize)]
struct JsonRpcRequest {
    jsonrpc: String,
    id: u32,
    method: String,
    params: GenerateParams,
}

/// Parameters for the generate method.
#[derive(Debug, Serialize)]
struct GenerateParams {
    model: String,
    system: String,
    prompt: String,
}

/// JSON-RPC response structure.
#[derive(Debug, Deserialize)]
struct JsonRpcResponse {
    #[allow(dead_code)]
    jsonrpc: String,
    #[allow(dead_code)]
    id: u32,
    result: Option<serde_json::Value>,
    error: Option<JsonRpcError>,
}

/// JSON-RPC error structure.
#[derive(Debug, Deserialize)]
struct JsonRpcError {
    code: i32,
    message: String,
}

/// Convert Gemini JSON-RPC response to AgentEvent.
fn convert_gemini_response(response: JsonRpcResponse) -> Option<Result<AgentEvent, AgentError>> {
    // Check for errors
    if let Some(error) = response.error {
        return Some(Err(AgentError::ApiError(format!(
            "Gemini API error (code {}): {}",
            error.code, error.message
        ))));
    }

    // Extract result
    if let Some(result) = response.result {
        // Try to extract text from different possible structures

        // Try result.text
        if let Some(text) = result.get("text").and_then(|t| t.as_str()) {
            if !text.trim().is_empty() {
                return Some(Ok(AgentEvent::MessageChunk(text.to_string())));
            }
        }

        // Try result.parts[].text (Google AI format)
        if let Some(parts) = result.get("parts").and_then(|p| p.as_array()) {
            for part in parts {
                if let Some(text) = part.get("text").and_then(|t| t.as_str()) {
                    if !text.trim().is_empty() {
                        return Some(Ok(AgentEvent::MessageChunk(text.to_string())));
                    }
                }
            }
        }

        // Try result.content (alternative format)
        if let Some(content) = result.get("content").and_then(|c| c.as_str()) {
            if !content.trim().is_empty() {
                return Some(Ok(AgentEvent::MessageChunk(content.to_string())));
            }
        }

        // If no text found but result exists, signal completion
        return Some(Ok(AgentEvent::Completed));
    }

    // No result and no error - signal completion
    Some(Ok(AgentEvent::Completed))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_gemini_adapter_new() {
        let adapter = GeminiAdapter::new(
            "test".to_string(),
            "gemini-2.5-pro".to_string(),
            "test prompt".to_string(),
        );
        assert!(adapter.is_ok());
    }

    #[tokio::test]
    async fn test_check_availability_without_cli() {
        let adapter = GeminiAdapter::new(
            "test".to_string(),
            "gemini-2.5-pro".to_string(),
            "test prompt".to_string(),
        )
        .unwrap();

        // Will return false unless gemini-cli is installed AND GEMINI_API_KEY is set
        let available = adapter.check_availability().await;
        // We can't assert true/false as it depends on the environment
        let _ = available;
    }

    #[test]
    fn test_json_rpc_request_serialization() {
        let request = JsonRpcRequest {
            jsonrpc: "2.0".to_string(),
            id: 1,
            method: "generate".to_string(),
            params: GenerateParams {
                model: "gemini-2.5-pro".to_string(),
                system: "You are helpful".to_string(),
                prompt: "Hello".to_string(),
            },
        };

        let json = serde_json::to_string(&request).unwrap();
        assert!(json.contains("jsonrpc"));
        assert!(json.contains("generate"));
        assert!(json.contains("gemini-2.5-pro"));
    }

    #[test]
    fn test_convert_gemini_response_with_text() {
        let response = JsonRpcResponse {
            jsonrpc: "2.0".to_string(),
            id: 1,
            result: Some(serde_json::json!({
                "text": "Hello, world!"
            })),
            error: None,
        };

        let event = convert_gemini_response(response);
        assert!(event.is_some());

        let event = event.unwrap();
        assert!(event.is_ok());

        match event.unwrap() {
            AgentEvent::MessageChunk(text) => assert_eq!(text, "Hello, world!"),
            _ => panic!("Expected MessageChunk"),
        }
    }

    #[test]
    fn test_convert_gemini_response_with_error() {
        let response = JsonRpcResponse {
            jsonrpc: "2.0".to_string(),
            id: 1,
            result: None,
            error: Some(JsonRpcError {
                code: 400,
                message: "Bad request".to_string(),
            }),
        };

        let event = convert_gemini_response(response);
        assert!(event.is_some());

        let event = event.unwrap();
        assert!(event.is_err());
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/mock_agent.rs">
//! Mock agent implementation for testing.

use crate::agents::base::Agent;
use crate::agents::base::AgentError;
use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use async_trait::async_trait;
use std::pin::Pin;
use tokio_stream::Stream;

#[derive(Clone)]
pub struct MockAgent {
    available: bool,
    events: Vec<Result<AgentEvent, AgentError>>,
}

impl MockAgent {
    pub fn new(available: bool, events: Vec<Result<AgentEvent, AgentError>>) -> Self {
        Self { available, events }
    }

    pub fn success() -> Self {
        Self {
            available: true,
            events: vec![
                Ok(AgentEvent::Thought("Mock agent thinking".to_string())),
                Ok(AgentEvent::MessageChunk("Mock response".to_string())),
                Ok(AgentEvent::Completed),
            ],
        }
    }

    pub fn unavailable() -> Self {
        Self {
            available: false,
            events: vec![],
        }
    }

    pub fn failing() -> Self {
        Self {
            available: true,
            events: vec![
                Ok(AgentEvent::Thought("Starting...".to_string())),
                Err(AgentError::ExecutionError("Mock failure".to_string())),
            ],
        }
    }
}

#[async_trait]
impl Agent for MockAgent {
    async fn check_availability(&self) -> bool {
        self.available
    }

    async fn execute(
        &self,
        _context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        if !self.available {
            return Err(AgentError::NotAvailable(
                "Mock agent not available".to_string(),
            ));
        }

        let events = self.events.clone();
        let stream = tokio_stream::iter(events);
        Ok(Box::pin(stream))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio_stream::StreamExt;

    #[tokio::test]
    async fn test_mock_agent_success() {
        let agent = MockAgent::success();
        assert!(agent.check_availability().await);

        let context = ExecutionContext::new("test".to_string());

        let mut stream = agent.execute(&context).await.unwrap();
        let mut events = Vec::new();

        while let Some(event) = stream.next().await {
            events.push(event);
        }

        assert_eq!(events.len(), 3);
        assert!(matches!(events[0], Ok(AgentEvent::Thought(_))));
        assert!(matches!(events[1], Ok(AgentEvent::MessageChunk(_))));
        assert_eq!(events[2], Ok(AgentEvent::Completed));
    }

    #[tokio::test]
    async fn test_mock_agent_unavailable() {
        let agent = MockAgent::unavailable();
        assert!(!agent.check_availability().await);

        let context = ExecutionContext::new("test".to_string());

        let result = agent.execute(&context).await;
        assert!(result.is_err());
        if let Err(e) = result {
            assert!(matches!(e, AgentError::NotAvailable(_)));
        }
    }

    #[tokio::test]
    async fn test_mock_agent_failing() {
        let agent = MockAgent::failing();
        assert!(agent.check_availability().await);

        let context = ExecutionContext::new("test".to_string());

        let stream = agent.execute(&context).await.unwrap();
        let events: Vec<_> = stream.collect().await;

        assert_eq!(events.len(), 2);
        assert!(matches!(events[0], Ok(AgentEvent::Thought(_))));
        assert!(matches!(events[1], Err(AgentError::ExecutionError(_))));
    }

    #[tokio::test]
    async fn test_mock_agent_custom_events() {
        let custom_events = vec![
            Ok(AgentEvent::ToolCall("read_file".to_string())),
            Ok(AgentEvent::MessageChunk("File content".to_string())),
            Ok(AgentEvent::Completed),
        ];

        let agent = MockAgent::new(true, custom_events);
        let context = ExecutionContext::new("test".to_string());

        let stream = agent.execute(&context).await.unwrap();
        let events: Vec<_> = stream.collect().await;

        assert_eq!(events.len(), 3);
        assert!(matches!(events[0], Ok(AgentEvent::ToolCall(_))));
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/base.rs">
//! Base Agent trait and supporting types.

use async_trait::async_trait;
use std::pin::Pin;
use thiserror::Error;
use tokio_stream::Stream;

/// Attachment types that can be included with an instruction.
#[derive(Debug, Clone)]
pub enum Attachment {
    /// Image attachment with path and MIME type.
    Image { path: String, mime_type: String },
    /// File attachment with path and content.
    File { path: String, content: String },
}

/// Context information passed to agents during execution.
#[derive(Debug, Clone)]
pub struct ExecutionContext {
    /// The user instruction (prompt).
    pub instruction: String,

    /// The project path (working directory).
    pub project_path: String,

    /// Whether this is the initial prompt (used for tool filtering).
    pub is_initial_prompt: bool,

    /// Additional context (images, files, etc.).
    pub attachments: Vec<Attachment>,
}

impl ExecutionContext {
    /// Create a new ExecutionContext with the given instruction.
    ///
    /// Defaults:
    /// - project_path: current directory
    /// - is_initial_prompt: false
    /// - attachments: empty
    pub fn new(instruction: String) -> Self {
        Self {
            instruction,
            project_path: std::env::current_dir()
                .ok()
                .and_then(|p| p.to_str().map(|s| s.to_string()))
                .unwrap_or_else(|| ".".to_string()),
            is_initial_prompt: false,
            attachments: vec![],
        }
    }

    /// Set the project path.
    pub fn with_project_path(mut self, path: String) -> Self {
        self.project_path = path;
        self
    }

    /// Set whether this is an initial prompt.
    pub fn with_initial_prompt(mut self, is_initial: bool) -> Self {
        self.is_initial_prompt = is_initial;
        self
    }

    /// Add an attachment.
    pub fn with_attachment(mut self, attachment: Attachment) -> Self {
        self.attachments.push(attachment);
        self
    }

    /// Add multiple attachments.
    pub fn with_attachments(mut self, attachments: Vec<Attachment>) -> Self {
        self.attachments.extend(attachments);
        self
    }
}

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum AgentEvent {
    Thought(String),
    ToolCall(String),
    MessageChunk(String),
    Completed,
}

#[derive(Error, Debug, Clone, PartialEq, Eq)]
pub enum AgentError {
    #[error("Agent not available: {0}")]
    NotAvailable(String),
    #[error("API call failed: {0}")]
    ApiError(String),
    #[error("Stream parsing error: {0}")]
    StreamParseError(String),
    #[error("Execution failed: {0}")]
    ExecutionError(String),
}

#[async_trait]
pub trait Agent: Send + Sync {
    async fn check_availability(&self) -> bool;
    async fn execute(
        &self,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>;
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio_stream::StreamExt;

    struct TestAgent {
        available: bool,
    }

    #[async_trait]
    impl Agent for TestAgent {
        async fn check_availability(&self) -> bool {
            self.available
        }

        async fn execute(
            &self,
            context: &ExecutionContext,
        ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
        {
            if !self.available {
                return Err(AgentError::NotAvailable(
                    "Test agent not available".to_string(),
                ));
            }

            let instruction = context.instruction.clone();
            let stream = tokio_stream::iter(vec![
                Ok(AgentEvent::Thought(format!("Processing: {}", instruction))),
                Ok(AgentEvent::MessageChunk("Response: ".to_string())),
                Ok(AgentEvent::MessageChunk("Test complete".to_string())),
                Ok(AgentEvent::Completed),
            ]);

            Ok(Box::pin(stream))
        }
    }

    #[tokio::test]
    async fn test_agent_check_availability() {
        let available_agent = TestAgent { available: true };
        assert!(available_agent.check_availability().await);

        let unavailable_agent = TestAgent { available: false };
        assert!(!unavailable_agent.check_availability().await);
    }

    #[tokio::test]
    async fn test_agent_execute_success() {
        let agent = TestAgent { available: true };
        let context = ExecutionContext::new("test instruction".to_string());

        let mut stream = agent.execute(&context).await.unwrap();
        let mut events = Vec::new();

        while let Some(event) = stream.next().await {
            events.push(event.unwrap());
        }

        assert_eq!(events.len(), 4);
        assert!(matches!(events[0], AgentEvent::Thought(_)));
        assert!(matches!(events[1], AgentEvent::MessageChunk(_)));
        assert!(matches!(events[2], AgentEvent::MessageChunk(_)));
        assert_eq!(events[3], AgentEvent::Completed);
    }

    #[tokio::test]
    async fn test_agent_execute_unavailable() {
        let agent = TestAgent { available: false };
        let context = ExecutionContext::new("test instruction".to_string());

        let result = agent.execute(&context).await;
        assert!(result.is_err());
        if let Err(e) = result {
            assert!(matches!(e, AgentError::NotAvailable(_)));
        }
    }

    #[test]
    fn test_execution_context_creation() {
        let context = ExecutionContext::new("Test instruction".to_string());
        assert_eq!(context.instruction, "Test instruction");
        assert!(!context.is_initial_prompt);
        assert!(context.attachments.is_empty());
    }

    #[test]
    fn test_execution_context_builder() {
        let context = ExecutionContext::new("Test".to_string())
            .with_project_path("/tmp/test".to_string())
            .with_initial_prompt(true)
            .with_attachment(Attachment::Image {
                path: "/tmp/image.png".to_string(),
                mime_type: "image/png".to_string(),
            });

        assert_eq!(context.instruction, "Test");
        assert_eq!(context.project_path, "/tmp/test");
        assert!(context.is_initial_prompt);
        assert_eq!(context.attachments.len(), 1);
    }

    #[test]
    fn test_agent_event_equality() {
        let event1 = AgentEvent::Completed;
        let event2 = AgentEvent::Completed;
        assert_eq!(event1, event2);

        let chunk1 = AgentEvent::MessageChunk("test".to_string());
        let chunk2 = AgentEvent::MessageChunk("test".to_string());
        assert_eq!(chunk1, chunk2);
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/config/error.rs">
//! Error types for configuration loading.
//!
//! This module defines all errors that can occur during configuration file
//! parsing and loading operations.

use std::path::PathBuf;
use thiserror::Error;

/// Errors that can occur during configuration loading.
#[derive(Error, Debug)]
pub enum ConfigError {
    /// Failed to read a configuration file from disk.
    #[error("Failed to read config file at {path}: {source}")]
    FileRead {
        path: PathBuf,
        source: std::io::Error,
    },

    /// Failed to parse TOML configuration.
    #[error("Failed to parse TOML file at {path}: {source}")]
    TomlParse {
        path: PathBuf,
        source: toml::de::Error,
    },

    /// Failed to parse YAML configuration.
    #[error("Failed to parse YAML file at {path}: {source}")]
    YamlParse {
        path: PathBuf,
        source: serde_yaml::Error,
    },

    /// Failed to parse Markdown front matter.
    #[error("Failed to parse Markdown front matter in {path}: {reason}")]
    MarkdownParse { path: PathBuf, reason: String },

    /// Failed to walk directory structure.
    #[error("Failed to traverse directory {path}: {source}")]
    DirectoryWalk {
        path: PathBuf,
        source: walkdir::Error,
    },

    /// Invalid configuration structure or missing required fields.
    #[error("Invalid configuration in {path}: {reason}")]
    InvalidConfig { path: PathBuf, reason: String },
}

/// Type alias for Result with ConfigError.
pub type ConfigResult<T> = Result<T, ConfigError>;
</file>

<file path="pipeline-kit-rs/crates/core/src/config/loader.rs">
//! Configuration file loader for `.pipeline-kit/` directory structure.
//!
//! This module provides functionality to load and parse all configuration files
//! from the `.pipeline-kit/` directory, including:
//! - `config.toml`: Global settings
//! - `agents/*.md`: Agent definitions with YAML front matter
//! - `pipelines/*.yaml`: Pipeline definitions
//!
//! Note: The `result_large_err` lint is suppressed for this module because
//! configuration loading is a cold path that runs once at startup. Large error
//! types have negligible performance impact in this context.

#![allow(clippy::result_large_err)]

use crate::config::error::ConfigError;
use crate::config::error::ConfigResult;
use crate::config::models::AppConfig;
use gray_matter::engine::YAML;
use gray_matter::Matter;
use pk_protocol::agent_models::Agent;
use pk_protocol::config_models::GlobalConfig;
use pk_protocol::pipeline_models::Pipeline;
use std::path::Path;
use walkdir::WalkDir;

/// Loads all configuration from the `.pipeline-kit/` directory.
///
/// This function scans the `.pipeline-kit/` directory and loads:
/// - Global configuration from `config.toml`
/// - Agent definitions from `agents/*.md` files
/// - Pipeline definitions from `pipelines/*.yaml` files
///
/// # Arguments
///
/// * `root` - Root directory containing the `.pipeline-kit/` folder
///
/// # Returns
///
/// An `AppConfig` containing all loaded configuration. If directories or files
/// are missing (but the root exists), returns an empty/default configuration
/// rather than an error.
///
/// # Errors
///
/// Returns `ConfigError` if:
/// - Files exist but cannot be read
/// - Files have invalid syntax (TOML, YAML, or Markdown front matter)
/// - Required fields are missing in configuration files
///
/// # Example
///
/// ```rust,no_run
/// use pk_core::config::loader::load_config;
/// use std::path::Path;
///
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// let config = load_config(Path::new(".")).await?;
/// println!("Loaded {} agents", config.agents.len());
/// # Ok(())
/// # }
/// ```
pub async fn load_config(root: &Path) -> ConfigResult<AppConfig> {
    let pk_dir = root.join(".pipeline-kit");

    // If .pipeline-kit doesn't exist, return default config
    if !pk_dir.exists() {
        return Ok(AppConfig::default());
    }

    // Load global config
    let global = load_global_config(&pk_dir)?;

    // Load agents
    let agents = load_agents(&pk_dir)?;

    // Load pipelines
    let pipelines = load_pipelines(&pk_dir)?;

    Ok(AppConfig {
        global,
        agents,
        pipelines,
    })
}

/// Loads global configuration from `config.toml`.
fn load_global_config(pk_dir: &Path) -> ConfigResult<GlobalConfig> {
    let config_path = pk_dir.join("config.toml");

    // If config.toml doesn't exist, return default
    if !config_path.exists() {
        return Ok(GlobalConfig { git: false });
    }

    let content =
        std::fs::read_to_string(&config_path).map_err(|source| ConfigError::FileRead {
            path: config_path.clone(),
            source,
        })?;

    let config: GlobalConfig =
        toml::from_str(&content).map_err(|source| ConfigError::TomlParse {
            path: config_path,
            source,
        })?;

    Ok(config)
}

/// Loads all agent definitions from `agents/*.md`.
fn load_agents(pk_dir: &Path) -> ConfigResult<Vec<Agent>> {
    let agents_dir = pk_dir.join("agents");

    // If agents directory doesn't exist, return empty vector
    if !agents_dir.exists() {
        return Ok(Vec::new());
    }

    let mut agents = Vec::new();

    // Walk through all .md files in the agents directory
    for entry in WalkDir::new(&agents_dir)
        .min_depth(1)
        .max_depth(1)
        .into_iter()
    {
        let entry = entry.map_err(|source| ConfigError::DirectoryWalk {
            path: agents_dir.clone(),
            source,
        })?;

        let path = entry.path();

        // Only process .md files
        if path.extension().and_then(|s| s.to_str()) != Some("md") {
            continue;
        }

        let content = std::fs::read_to_string(path).map_err(|source| ConfigError::FileRead {
            path: path.to_path_buf(),
            source,
        })?;

        // Parse Markdown with YAML front matter
        let matter = Matter::<YAML>::new();
        let result = matter.parse(&content);

        let mut agent: Agent = result
            .data
            .ok_or_else(|| ConfigError::MarkdownParse {
                path: path.to_path_buf(),
                reason: "Missing YAML front matter".to_string(),
            })?
            .deserialize()
            .map_err(|e| ConfigError::MarkdownParse {
                path: path.to_path_buf(),
                reason: format!("Failed to deserialize front matter: {}", e),
            })?;

        // Set the system prompt from the markdown body
        agent.system_prompt = result.content;

        agents.push(agent);
    }

    Ok(agents)
}

/// Loads all pipeline definitions from `pipelines/*.yaml`.
fn load_pipelines(pk_dir: &Path) -> ConfigResult<Vec<Pipeline>> {
    let pipelines_dir = pk_dir.join("pipelines");

    // If pipelines directory doesn't exist, return empty vector
    if !pipelines_dir.exists() {
        return Ok(Vec::new());
    }

    let mut pipelines = Vec::new();

    // Walk through all .yaml and .yml files in the pipelines directory
    for entry in WalkDir::new(&pipelines_dir)
        .min_depth(1)
        .max_depth(1)
        .into_iter()
    {
        let entry = entry.map_err(|source| ConfigError::DirectoryWalk {
            path: pipelines_dir.clone(),
            source,
        })?;

        let path = entry.path();

        // Only process .yaml and .yml files
        let ext = path.extension().and_then(|s| s.to_str());
        if ext != Some("yaml") && ext != Some("yml") {
            continue;
        }

        let content = std::fs::read_to_string(path).map_err(|source| ConfigError::FileRead {
            path: path.to_path_buf(),
            source,
        })?;

        let pipeline: Pipeline =
            serde_yaml::from_str(&content).map_err(|source| ConfigError::YamlParse {
                path: path.to_path_buf(),
                source,
            })?;

        pipelines.push(pipeline);
    }

    Ok(pipelines)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::tempdir;

    /// RED: This test defines our acceptance criteria and will fail initially.
    ///
    /// We create a complete `.pipeline-kit/` structure with all required files
    /// and verify that `load_config` correctly parses and loads everything.
    #[tokio::test]
    async fn test_load_config_acceptance() {
        // Setup: Create temporary .pipeline-kit directory structure
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("pipelines")).expect("Failed to create pipelines dir");
        fs::create_dir_all(pk_dir.join("agents")).expect("Failed to create agents dir");

        // Write config.toml
        let config_toml = "git = true";
        fs::write(pk_dir.join("config.toml"), config_toml).expect("Failed to write config.toml");

        // Write an agent definition (Markdown with YAML front matter)
        let agent_md = r#"---
name: code-reviewer
description: Reviews code for quality
model: claude-sonnet-4
color: blue
---

You are an expert code reviewer. Analyze code for:
- Correctness
- Performance
- Security"#;
        fs::write(pk_dir.join("agents/code-reviewer.md"), agent_md)
            .expect("Failed to write agent file");

        // Write a pipeline definition
        let pipeline_yaml = r#"name: review-pipeline
required-reference-file:
  1: "docs/standards.md"
output-file:
  1: "review-report.md"
master:
  model: claude-sonnet-4
  system-prompt: "Orchestrate code review"
  process:
    - code-reviewer
    - HUMAN_REVIEW
sub-agents:
  - code-reviewer
"#;
        fs::write(pk_dir.join("pipelines/review.yaml"), pipeline_yaml)
            .expect("Failed to write pipeline file");

        // Act: Load configuration (this will fail until we implement it)
        let config = load_config(root).await.expect("Failed to load config");

        // Assert: Verify all configuration was loaded correctly

        // Global config
        assert!(config.global.git, "Global git setting should be true");

        // Agents
        assert_eq!(config.agents.len(), 1, "Should load 1 agent");
        let agent = &config.agents[0];
        assert_eq!(agent.name, "code-reviewer");
        assert_eq!(agent.description, "Reviews code for quality");
        assert_eq!(agent.model, "claude-sonnet-4");
        assert_eq!(agent.color, "blue");
        assert!(
            agent.system_prompt.contains("expert code reviewer"),
            "System prompt should be loaded from markdown body"
        );

        // Pipelines
        assert_eq!(config.pipelines.len(), 1, "Should load 1 pipeline");
        let pipeline = &config.pipelines[0];
        assert_eq!(pipeline.name, "review-pipeline");
        assert_eq!(
            pipeline.required_reference_file.get(&1),
            Some(&"docs/standards.md".to_string())
        );
        assert_eq!(
            pipeline.output_file.get(&1),
            Some(&"review-report.md".to_string())
        );
        assert_eq!(pipeline.master.model, "claude-sonnet-4");
        assert_eq!(pipeline.master.process.len(), 2);
        assert_eq!(pipeline.sub_agents.len(), 1);
        assert_eq!(pipeline.sub_agents[0], "code-reviewer");
    }

    /// RED: Test loading from an empty directory (no .pipeline-kit folder).
    ///
    /// This should return a default/empty configuration, not an error.
    #[tokio::test]
    async fn test_load_config_empty_directory() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();

        // No .pipeline-kit directory exists
        let config = load_config(root)
            .await
            .expect("Should handle missing .pipeline-kit");

        // Should return empty/default configuration
        assert!(!config.global.git, "Default git should be false");
        assert!(config.agents.is_empty(), "Should have no agents");
        assert!(config.pipelines.is_empty(), "Should have no pipelines");
    }

    /// RED: Test partial configuration (only config.toml exists).
    #[tokio::test]
    async fn test_load_config_partial() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(&pk_dir).expect("Failed to create .pipeline-kit");

        // Only write config.toml
        fs::write(pk_dir.join("config.toml"), "git = false").expect("Failed to write config.toml");

        let config = load_config(root)
            .await
            .expect("Should handle partial config");

        assert!(!config.global.git);
        assert!(config.agents.is_empty(), "Should have no agents");
        assert!(config.pipelines.is_empty(), "Should have no pipelines");
    }

    /// REFACTOR: Test invalid TOML syntax.
    #[tokio::test]
    async fn test_load_config_invalid_toml() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(&pk_dir).expect("Failed to create .pipeline-kit");

        // Write invalid TOML
        fs::write(pk_dir.join("config.toml"), "git = [invalid toml")
            .expect("Failed to write config.toml");

        let result = load_config(root).await;
        assert!(result.is_err(), "Should fail on invalid TOML");

        if let Err(ConfigError::TomlParse { path, .. }) = result {
            assert!(path.ends_with("config.toml"));
        } else {
            panic!("Expected TomlParse error");
        }
    }

    /// REFACTOR: Test invalid YAML in pipeline file.
    #[tokio::test]
    async fn test_load_config_invalid_yaml() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("pipelines")).expect("Failed to create pipelines dir");

        // Write invalid YAML
        let invalid_yaml = "name: test\n  invalid: [yaml";
        fs::write(pk_dir.join("pipelines/test.yaml"), invalid_yaml)
            .expect("Failed to write pipeline file");

        let result = load_config(root).await;
        assert!(result.is_err(), "Should fail on invalid YAML");

        if let Err(ConfigError::YamlParse { path, .. }) = result {
            assert!(path.ends_with("test.yaml"));
        } else {
            panic!("Expected YamlParse error");
        }
    }

    /// REFACTOR: Test agent markdown file without front matter.
    #[tokio::test]
    async fn test_load_config_agent_no_frontmatter() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("agents")).expect("Failed to create agents dir");

        // Write markdown without front matter
        let no_frontmatter = "Just plain markdown content";
        fs::write(pk_dir.join("agents/test.md"), no_frontmatter)
            .expect("Failed to write agent file");

        let result = load_config(root).await;
        assert!(result.is_err(), "Should fail on agent without front matter");

        if let Err(ConfigError::MarkdownParse { path, reason }) = result {
            assert!(path.ends_with("test.md"));
            assert!(reason.contains("Missing YAML front matter"));
        } else {
            panic!("Expected MarkdownParse error");
        }
    }

    /// REFACTOR: Test agent markdown file with invalid front matter.
    #[tokio::test]
    async fn test_load_config_agent_invalid_frontmatter() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("agents")).expect("Failed to create agents dir");

        // Write markdown with incomplete front matter (missing required fields)
        let invalid_frontmatter = r#"---
name: test-agent
# Missing required fields: description, model
---

Agent content"#;
        fs::write(pk_dir.join("agents/test.md"), invalid_frontmatter)
            .expect("Failed to write agent file");

        let result = load_config(root).await;
        assert!(
            result.is_err(),
            "Should fail on agent with invalid front matter"
        );

        if let Err(ConfigError::MarkdownParse { path, reason }) = result {
            assert!(path.ends_with("test.md"));
            assert!(reason.contains("Failed to deserialize"));
        } else {
            panic!("Expected MarkdownParse error");
        }
    }

    /// REFACTOR: Test loading multiple agents and pipelines.
    #[tokio::test]
    async fn test_load_config_multiple_files() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("agents")).expect("Failed to create agents dir");
        fs::create_dir_all(pk_dir.join("pipelines")).expect("Failed to create pipelines dir");

        // Write multiple agent files
        for i in 1..=3 {
            let agent_md = format!(
                r#"---
name: agent-{}
description: Test agent {}
model: test-model
color: blue
---

System prompt for agent {}"#,
                i, i, i
            );
            fs::write(pk_dir.join(format!("agents/agent-{}.md", i)), agent_md)
                .expect("Failed to write agent file");
        }

        // Write multiple pipeline files
        for i in 1..=2 {
            let pipeline_yaml = format!(
                r#"name: pipeline-{}
master:
  model: test-model
  system-prompt: "Test prompt"
  process:
    - agent-1
sub-agents:
  - agent-1
"#,
                i
            );
            fs::write(
                pk_dir.join(format!("pipelines/pipeline-{}.yaml", i)),
                pipeline_yaml,
            )
            .expect("Failed to write pipeline file");
        }

        let config = load_config(root).await.expect("Should load multiple files");

        assert_eq!(config.agents.len(), 3, "Should load 3 agents");
        assert_eq!(config.pipelines.len(), 2, "Should load 2 pipelines");
    }

    /// REFACTOR: Test that non-matching files are ignored.
    #[tokio::test]
    async fn test_load_config_ignores_non_matching_files() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("agents")).expect("Failed to create agents dir");
        fs::create_dir_all(pk_dir.join("pipelines")).expect("Failed to create pipelines dir");

        // Write files with wrong extensions
        fs::write(pk_dir.join("agents/readme.txt"), "Not a markdown file")
            .expect("Failed to write txt file");
        fs::write(pk_dir.join("pipelines/notes.txt"), "Not a yaml file")
            .expect("Failed to write txt file");

        // Write one valid file
        let agent_md = r#"---
name: valid-agent
description: Valid agent
model: test-model
---

Valid content"#;
        fs::write(pk_dir.join("agents/valid.md"), agent_md).expect("Failed to write agent file");

        let config = load_config(root)
            .await
            .expect("Should ignore non-matching files");

        assert_eq!(config.agents.len(), 1, "Should only load .md files");
        assert_eq!(config.pipelines.len(), 0, "Should only load .yaml files");
    }

    /// REFACTOR: Test loading with .yml extension (alternative to .yaml).
    #[tokio::test]
    async fn test_load_config_yml_extension() {
        let dir = tempdir().expect("Failed to create temp dir");
        let root = dir.path();
        let pk_dir = root.join(".pipeline-kit");

        fs::create_dir_all(pk_dir.join("pipelines")).expect("Failed to create pipelines dir");

        // Write pipeline with .yml extension
        let pipeline_yaml = r#"name: yml-pipeline
master:
  model: test-model
  system-prompt: "Test"
  process:
    - test-agent
sub-agents:
  - test-agent
"#;
        fs::write(pk_dir.join("pipelines/test.yml"), pipeline_yaml)
            .expect("Failed to write pipeline file");

        let config = load_config(root).await.expect("Should load .yml files");

        assert_eq!(config.pipelines.len(), 1, "Should load .yml files");
        assert_eq!(config.pipelines[0].name, "yml-pipeline");
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/common/fixtures.rs">
//! Test fixtures for creating sample configurations and test data.

use pk_protocol::agent_models::Agent;
use pk_protocol::pipeline_models::MasterAgentConfig;
use pk_protocol::pipeline_models::Pipeline;
use pk_protocol::pipeline_models::ProcessStep;
use pk_protocol::process_models::Process;
use pk_protocol::process_models::ProcessStatus;
use std::collections::HashMap;
use tempfile::TempDir;
use uuid::Uuid;

/// Create a temporary project directory with .pipeline-kit configuration.
///
/// This creates a complete test environment with:
/// - `.pipeline-kit/pipelines/` directory
/// - `.pipeline-kit/agents/` directory
/// - Sample pipeline and agent configuration files
///
/// Returns a TempDir that must be kept alive for the test duration.
#[allow(dead_code)]
pub fn create_test_project() -> std::io::Result<TempDir> {
    let temp_dir = tempfile::tempdir()?;
    let root = temp_dir.path();

    // Create directory structure
    std::fs::create_dir_all(root.join(".pipeline-kit/pipelines"))?;
    std::fs::create_dir_all(root.join(".pipeline-kit/agents"))?;

    // Create a sample pipeline YAML
    let pipeline_yaml = r#"
name: test-pipeline
required-reference-file:
  1: "docs/requirements.md"
output-file:
  1: "src/output.rs"
master:
  model: "claude-sonnet-4"
  system-prompt: "You are a test orchestrator"
  process:
    - "agent-1"
    - "HUMAN_REVIEW"
    - "agent-2"
sub-agents:
  - "agent-1"
  - "agent-2"
"#;
    std::fs::write(
        root.join(".pipeline-kit/pipelines/test.yaml"),
        pipeline_yaml,
    )?;

    // Create sample agent markdown files
    let agent1_md = r#"---
name: agent-1
description: Test agent 1
model: claude-sonnet-4
color: blue
---
You are a helpful test agent."#;

    let agent2_md = r#"---
name: agent-2
description: Test agent 2
model: claude-sonnet-4
color: green
---
You are another helpful test agent."#;

    std::fs::write(root.join(".pipeline-kit/agents/agent-1.md"), agent1_md)?;
    std::fs::write(root.join(".pipeline-kit/agents/agent-2.md"), agent2_md)?;

    // Create dummy reference files
    std::fs::create_dir_all(root.join("docs"))?;
    std::fs::write(
        root.join("docs/requirements.md"),
        "# Test Requirements\nThis is a test.",
    )?;

    Ok(temp_dir)
}

/// Create a test Agent configuration.
pub fn create_test_agent(name: &str) -> Agent {
    Agent {
        name: name.to_string(),
        description: format!("Test agent {}", name),
        model: "test-model".to_string(),
        color: "blue".to_string(),
        system_prompt: "Test prompt".to_string(),
    }
}

/// Create a test Agent configuration that will fail.
#[allow(dead_code)]
pub fn create_failure_agent(name: &str) -> Agent {
    Agent {
        name: name.to_string(),
        description: format!("Failing test agent {}", name),
        model: "test-failure-model".to_string(),
        color: "red".to_string(),
        system_prompt: "This agent will fail".to_string(),
    }
}

/// Create a test Agent configuration that is unavailable.
#[allow(dead_code)]
pub fn create_unavailable_agent(name: &str) -> Agent {
    Agent {
        name: name.to_string(),
        description: format!("Unavailable test agent {}", name),
        model: "test-unavailable-model".to_string(),
        color: "gray".to_string(),
        system_prompt: "This agent is unavailable".to_string(),
    }
}

/// Create a test Pipeline configuration.
pub fn create_test_pipeline(name: &str, steps: Vec<ProcessStep>) -> Pipeline {
    Pipeline {
        name: name.to_string(),
        required_reference_file: HashMap::new(),
        output_file: HashMap::new(),
        master: MasterAgentConfig {
            model: "test-model".to_string(),
            system_prompt: "Test orchestration".to_string(),
            process: steps.clone(),
        },
        sub_agents: steps
            .iter()
            .filter_map(|step| match step {
                ProcessStep::Agent(name) => Some(name.clone()),
                _ => None,
            })
            .collect(),
    }
}

/// Create a simple pipeline with sequential agent steps.
#[allow(dead_code)]
pub fn create_simple_pipeline(name: &str, num_agents: usize) -> Pipeline {
    let steps = (1..=num_agents)
        .map(|i| ProcessStep::Agent(format!("agent-{}", i)))
        .collect();
    create_test_pipeline(name, steps)
}

/// Create a pipeline with HUMAN_REVIEW steps.
#[allow(dead_code)]
pub fn create_pipeline_with_human_review(name: &str) -> Pipeline {
    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
        ProcessStep::Agent("agent-2".to_string()),
    ];
    create_test_pipeline(name, steps)
}

/// Create a test Process instance.
pub fn create_test_process(pipeline_name: &str, status: ProcessStatus) -> Process {
    Process {
        id: Uuid::new_v4(),
        pipeline_name: pipeline_name.to_string(),
        status,
        current_step_index: 0,
        logs: Vec::new(),
        started_at: chrono::Utc::now(),
        completed_at: None,
        resume_notifier: std::sync::Arc::new(tokio::sync::Notify::new()),
    }
}

/// Create a running process with some log entries.
#[allow(dead_code)]
pub fn create_running_process(pipeline_name: &str) -> Process {
    let mut process = create_test_process(pipeline_name, ProcessStatus::Running);
    process.logs = vec![
        "Process started".to_string(),
        "Agent 1 executing".to_string(),
    ];
    process
}

/// Create a paused process.
#[allow(dead_code)]
pub fn create_paused_process(pipeline_name: &str) -> Process {
    create_test_process(pipeline_name, ProcessStatus::Paused)
}

/// Create a completed process.
#[allow(dead_code)]
pub fn create_completed_process(pipeline_name: &str) -> Process {
    let mut process = create_test_process(pipeline_name, ProcessStatus::Completed);
    process.completed_at = Some(chrono::Utc::now());
    process
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/common/mod.rs">
//! Common test utilities and helpers for E2E tests.
//!
//! This module provides shared functionality across all E2E tests including:
//! - Test fixtures (sample configs, pipelines)
//! - Custom assertions
//! - Mock agents
//! - Helper functions

pub mod assertions;
pub mod fixtures;
pub mod mock_agents;

#[allow(unused_imports)]
pub use assertions::*;
#[allow(unused_imports)]
pub use fixtures::*;
#[allow(unused_imports)]
pub use mock_agents::*;
</file>

<file path="pipeline-kit-rs/crates/protocol/src/ipc.rs">
//! Inter-process communication protocol.
//!
//! This module defines the message types for asynchronous communication
//! between the TUI (user interface) and the Core (business logic).
//!
//! The protocol follows an Operation/Event pattern:
//! - `Op`: Commands sent from TUI to Core
//! - `Event`: Status updates sent from Core to TUI
//!
//! Communication is asynchronous and channel-based, allowing the UI to
//! remain responsive while the core processes pipeline executions.

use serde::Deserialize;
use serde::Serialize;
use std::path::PathBuf;
use ts_rs::TS;
use uuid::Uuid;

use crate::process_models::ProcessStatus;

/// Operations sent from the UI (TUI) to the Core logic.
///
/// These represent user commands and requests for information.
/// The core processes these operations and responds with Events.
///
/// Uses tagged enum serialization for TypeScript compatibility:
/// ```json
/// {
///   "type": "startPipeline",
///   "payload": {
///     "name": "my-pipeline",
///     "reference_file": "/path/to/ref.md"
///   }
/// }
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "type", content = "payload", rename_all = "camelCase")]
pub enum Op {
    /// Start executing a pipeline.
    ///
    /// Initiates a new pipeline execution with an optional reference file.
    StartPipeline {
        /// Name of the pipeline to execute.
        name: String,
        /// Optional path to a reference file for context.
        reference_file: Option<PathBuf>,
    },

    /// Pause a running process.
    ///
    /// The process will stop after completing its current step.
    PauseProcess {
        #[ts(type = "string")]
        process_id: Uuid,
    },

    /// Resume a paused process.
    ///
    /// The process will continue from where it was paused.
    ResumeProcess {
        #[ts(type = "string")]
        process_id: Uuid,
    },

    /// Terminate a process immediately.
    ///
    /// The process will be stopped and marked as failed.
    KillProcess {
        #[ts(type = "string")]
        process_id: Uuid,
    },

    /// Request the current state of all processes.
    ///
    /// Core will respond with dashboard state information.
    GetDashboardState,

    /// Request detailed information about a specific process.
    GetProcessDetail {
        #[ts(type = "string")]
        process_id: Uuid,
    },

    /// Shut down the application gracefully.
    ///
    /// All running processes will be terminated.
    Shutdown,
}

/// Events sent from the Core logic to the UI (TUI).
///
/// These represent state changes and status updates that the UI should
/// reflect to the user.
///
/// Uses tagged enum serialization for TypeScript compatibility:
/// ```json
/// {
///   "type": "processStatusUpdate",
///   "payload": {
///     "process_id": "uuid-here",
///     "status": "RUNNING",
///     "step_index": 2
///   }
/// }
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "type", content = "payload", rename_all = "camelCase")]
pub enum Event {
    /// A new process has been started.
    ProcessStarted {
        #[ts(type = "string")]
        process_id: Uuid,
        pipeline_name: String,
    },

    /// A process's status has changed.
    ProcessStatusUpdate {
        #[ts(type = "string")]
        process_id: Uuid,
        status: ProcessStatus,
        step_index: usize,
    },

    /// A process has produced new log output.
    ///
    /// The TUI should append this to the process's log display.
    ProcessLogChunk {
        #[ts(type = "string")]
        process_id: Uuid,
        content: String,
    },

    /// A process has completed successfully.
    ProcessCompleted {
        #[ts(type = "string")]
        process_id: Uuid,
    },

    /// A process has encountered an error.
    ProcessError {
        #[ts(type = "string")]
        process_id: Uuid,
        error: String,
    },

    /// A process was killed by user request.
    ProcessKilled {
        #[ts(type = "string")]
        process_id: Uuid,
    },

    /// A process was resumed from paused state.
    ProcessResumed {
        #[ts(type = "string")]
        process_id: Uuid,
    },
}
</file>

<file path="pipeline-kit-rs/crates/tui/src/widgets/command_composer.rs">
//! Command composer widget with slash command autocomplete.
//!
//! This widget provides a text input field for entering commands, with
//! autocomplete suggestions when the user types a slash command.

use crate::event::EventStatus;
use crossterm::event::{KeyCode, KeyEvent, KeyEventKind};
use pk_protocol::Op;
use ratatui::buffer::Buffer;
use ratatui::layout::Rect;
use ratatui::style::Color;
use ratatui::style::Modifier;
use ratatui::style::Style;
use ratatui::text::Line;
use ratatui::text::Span;
use ratatui::widgets::Block;
use ratatui::widgets::Borders;
use ratatui::widgets::Paragraph;
use ratatui::widgets::Widget;
use uuid::Uuid;

/// Available slash commands with their descriptions.
const COMMANDS: &[(&str, &str)] = &[
    ("/start <pipeline>", "Start a new pipeline"),
    ("/pause <process_id>", "Pause a running process"),
    ("/resume <process_id>", "Resume a paused process"),
    ("/kill <process_id>", "Kill a process"),
    ("/list", "List all processes"),
];

/// Command composer state.
#[derive(Debug, Clone)]
pub struct CommandComposer {
    /// Current input text
    input: String,
    /// Current cursor position
    cursor_pos: usize,
    /// Whether autocomplete popup should be shown
    show_popup: bool,
    /// Selected index in the autocomplete list
    selected_index: usize,
}

impl Default for CommandComposer {
    fn default() -> Self {
        Self::new()
    }
}

impl CommandComposer {
    /// Create a new command composer.
    pub fn new() -> Self {
        Self {
            input: String::new(),
            cursor_pos: 0,
            show_popup: false,
            selected_index: 0,
        }
    }

    /// Get the current input text.
    pub fn input(&self) -> &str {
        &self.input
    }

    /// Check if autocomplete popup should be shown.
    pub fn should_show_popup(&self) -> bool {
        self.show_popup
    }

    /// Get filtered command suggestions based on current input.
    pub fn suggestions(&self) -> Vec<(&'static str, &'static str)> {
        if !self.input.starts_with('/') {
            return Vec::new();
        }

        let filter = self.input.trim();
        if filter == "/" {
            // Show all commands
            return COMMANDS.to_vec();
        }

        // Simple prefix matching for now
        COMMANDS
            .iter()
            .filter(|(cmd, _)| cmd.starts_with(filter))
            .copied()
            .collect()
    }

    /// Get the currently selected suggestion.
    pub fn selected_suggestion(&self) -> Option<(&'static str, &'static str)> {
        let suggestions = self.suggestions();
        suggestions.get(self.selected_index).copied()
    }

    /// Insert a character at the cursor position.
    pub fn insert_char(&mut self, c: char) {
        self.input.insert(self.cursor_pos, c);
        self.cursor_pos += 1;
        self.update_popup_state();
    }

    /// Delete the character before the cursor (backspace).
    pub fn delete_char(&mut self) {
        if self.cursor_pos > 0 {
            self.input.remove(self.cursor_pos - 1);
            self.cursor_pos -= 1;
            self.update_popup_state();
        }
    }

    /// Clear all input.
    pub fn clear(&mut self) {
        self.input.clear();
        self.cursor_pos = 0;
        self.show_popup = false;
        self.selected_index = 0;
    }

    /// Move cursor left.
    pub fn move_cursor_left(&mut self) {
        if self.cursor_pos > 0 {
            self.cursor_pos -= 1;
        }
    }

    /// Move cursor right.
    pub fn move_cursor_right(&mut self) {
        if self.cursor_pos < self.input.len() {
            self.cursor_pos += 1;
        }
    }

    /// Move selection up in autocomplete popup.
    pub fn move_selection_up(&mut self) {
        if self.selected_index > 0 {
            self.selected_index -= 1;
        }
    }

    /// Move selection down in autocomplete popup.
    pub fn move_selection_down(&mut self) {
        let suggestions = self.suggestions();
        if self.selected_index + 1 < suggestions.len() {
            self.selected_index += 1;
        }
    }

    /// Complete with the currently selected suggestion (Tab key).
    pub fn complete_with_selection(&mut self) {
        if let Some((cmd, _)) = self.selected_suggestion() {
            // Extract just the command name (without arguments placeholder)
            let cmd_name = cmd.split_whitespace().next().unwrap_or(cmd);
            self.input = format!("{} ", cmd_name);
            self.cursor_pos = self.input.len();
            self.show_popup = false;
            self.selected_index = 0;
        }
    }

    /// Update the popup state based on current input.
    fn update_popup_state(&mut self) {
        self.show_popup = self.input.starts_with('/') && !self.input.ends_with(' ');

        // Reset selection if needed
        let suggestions = self.suggestions();
        if self.selected_index >= suggestions.len() {
            self.selected_index = suggestions.len().saturating_sub(1);
        }
    }

    /// Render the input field.
    pub fn render(&self, area: Rect, buf: &mut Buffer) {
        let block = Block::default()
            .borders(Borders::ALL)
            .title("Command (q to quit)");

        let inner = block.inner(area);
        block.render(area, buf);

        let text = format!("> {}", self.input);
        let paragraph = Paragraph::new(text).style(Style::default().fg(Color::Yellow));
        paragraph.render(inner, buf);
    }

    /// Render the autocomplete popup.
    pub fn render_popup(&self, area: Rect, buf: &mut Buffer) {
        if !self.show_popup {
            return;
        }

        let suggestions = self.suggestions();
        if suggestions.is_empty() {
            return;
        }

        let block = Block::default()
            .borders(Borders::ALL)
            .title("Suggestions")
            .style(Style::default().bg(Color::Black));

        let inner = block.inner(area);
        block.render(area, buf);

        // Render suggestions
        let mut y = inner.y;
        for (i, (cmd, desc)) in suggestions.iter().enumerate() {
            if y >= inner.y + inner.height {
                break;
            }

            let style = if i == self.selected_index {
                Style::default()
                    .fg(Color::Black)
                    .bg(Color::Yellow)
                    .add_modifier(Modifier::BOLD)
            } else {
                Style::default().fg(Color::White)
            };

            let line = Line::from(vec![
                Span::styled(format!("{:<25}", cmd), style),
                Span::styled(desc.to_string(), style.fg(Color::Gray)),
            ]);

            buf.set_line(inner.x, y, &line, inner.width);
            y += 1;
        }
    }

    /// Handle a key event and return whether it was consumed.
    ///
    /// This method implements the widget's event handling logic, allowing the
    /// CommandComposer to handle its own keyboard input. It returns `EventStatus::Consumed`
    /// if the event was handled, or `EventStatus::NotConsumed` if it should be passed
    /// to the next handler in the chain of responsibility.
    ///
    /// Events that are NOT consumed (passed to parent handler):
    /// - Enter key (command submission handled by App)
    /// - 'q' when input is empty (global quit)
    /// - Ctrl+C (global quit)
    /// - Up/Down when popup is not shown (process navigation)
    pub fn handle_key_event(&mut self, key_event: KeyEvent) -> EventStatus {
        // Ignore key release events
        if key_event.kind != KeyEventKind::Press {
            return EventStatus::Consumed;
        }

        match key_event.code {
            KeyCode::Up | KeyCode::Down => self.handle_vertical_navigation(key_event.code),
            KeyCode::Tab => self.handle_tab(),
            KeyCode::Char(c) => self.handle_char_input(c),
            KeyCode::Backspace => self.handle_backspace(),
            KeyCode::Left | KeyCode::Right => self.handle_cursor_move(key_event.code),
            KeyCode::Esc => self.handle_escape(),
            KeyCode::Enter => EventStatus::NotConsumed,
            _ => EventStatus::NotConsumed,
        }
    }

    /// Handle vertical navigation (Up/Down keys).
    ///
    /// Only consumes the event if the autocomplete popup is shown.
    /// Otherwise, returns NotConsumed to allow App to handle process navigation.
    fn handle_vertical_navigation(&mut self, code: KeyCode) -> EventStatus {
        if !self.should_show_popup() {
            // Not consumed - let App handle process navigation
            return EventStatus::NotConsumed;
        }

        match code {
            KeyCode::Up => self.move_selection_up(),
            KeyCode::Down => self.move_selection_down(),
            _ => unreachable!(),
        }

        EventStatus::Consumed
    }

    /// Handle Tab key for autocomplete.
    ///
    /// Only consumes the event if the autocomplete popup is shown.
    fn handle_tab(&mut self) -> EventStatus {
        if !self.should_show_popup() {
            return EventStatus::NotConsumed;
        }

        self.complete_with_selection();
        EventStatus::Consumed
    }

    /// Handle character input.
    ///
    /// Special case: 'q' when input is empty is not consumed (global quit).
    fn handle_char_input(&mut self, c: char) -> EventStatus {
        // Don't consume 'q' when input is empty (it's a global quit)
        if c == 'q' && self.input.is_empty() {
            return EventStatus::NotConsumed;
        }

        self.insert_char(c);
        EventStatus::Consumed
    }

    /// Handle backspace key.
    fn handle_backspace(&mut self) -> EventStatus {
        self.delete_char();
        EventStatus::Consumed
    }

    /// Handle cursor movement (Left/Right keys).
    fn handle_cursor_move(&mut self, code: KeyCode) -> EventStatus {
        match code {
            KeyCode::Left => self.move_cursor_left(),
            KeyCode::Right => self.move_cursor_right(),
            _ => unreachable!(),
        }

        EventStatus::Consumed
    }

    /// Handle Escape key.
    fn handle_escape(&mut self) -> EventStatus {
        self.clear();
        EventStatus::Consumed
    }

    /// Parse the current input and generate an Op if valid.
    ///
    /// Returns Ok(Some(Op)) if a valid command was parsed,
    /// Ok(None) if input is empty or whitespace,
    /// Err(String) if the command is invalid.
    pub fn parse_command(&self) -> Result<Option<Op>, String> {
        let input = self.input.trim();

        if input.is_empty() {
            return Ok(None);
        }

        // Parse slash commands
        if input.starts_with('/') {
            let parts: Vec<&str> = input.split_whitespace().collect();
            let cmd = parts.first().ok_or("Empty command")?;

            match *cmd {
                "/start" => {
                    let pipeline_name = parts.get(1).ok_or("Missing pipeline name")?;
                    Ok(Some(Op::StartPipeline {
                        name: pipeline_name.to_string(),
                        reference_file: None,
                    }))
                }
                "/pause" => {
                    let process_id_str = parts.get(1).ok_or("Missing process ID")?;
                    let process_id =
                        Uuid::parse_str(process_id_str).map_err(|_| "Invalid process ID format")?;
                    Ok(Some(Op::PauseProcess { process_id }))
                }
                "/resume" => {
                    let process_id_str = parts.get(1).ok_or("Missing process ID")?;
                    let process_id =
                        Uuid::parse_str(process_id_str).map_err(|_| "Invalid process ID format")?;
                    Ok(Some(Op::ResumeProcess { process_id }))
                }
                "/kill" => {
                    let process_id_str = parts.get(1).ok_or("Missing process ID")?;
                    let process_id =
                        Uuid::parse_str(process_id_str).map_err(|_| "Invalid process ID format")?;
                    Ok(Some(Op::KillProcess { process_id }))
                }
                "/list" => Ok(Some(Op::GetDashboardState)),
                _ => Err(format!("Unknown command: {}", cmd)),
            }
        } else {
            // Non-slash commands could be handled here in the future
            Err("Invalid command. Commands must start with '/'".to_string())
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::event::EventStatus;
    use crossterm::event::{KeyCode, KeyEvent};

    #[test]
    fn test_new_composer_is_empty() {
        let composer = CommandComposer::new();
        assert_eq!(composer.input(), "");
        assert!(!composer.should_show_popup());
    }

    #[test]
    fn test_typing_slash_shows_popup() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');

        assert_eq!(composer.input(), "/");
        assert!(composer.should_show_popup());

        let suggestions = composer.suggestions();
        assert_eq!(suggestions.len(), COMMANDS.len());
    }

    #[test]
    fn test_typing_start_filters_suggestions() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');
        composer.insert_char('a');
        composer.insert_char('r');
        composer.insert_char('t');

        assert_eq!(composer.input(), "/start");
        assert!(composer.should_show_popup());

        let suggestions = composer.suggestions();
        assert_eq!(suggestions.len(), 1);
        assert_eq!(suggestions[0].0, "/start <pipeline>");
    }

    #[test]
    fn test_no_suggestions_for_non_slash_input() {
        let mut composer = CommandComposer::new();
        composer.insert_char('h');
        composer.insert_char('e');
        composer.insert_char('l');
        composer.insert_char('l');
        composer.insert_char('o');

        assert_eq!(composer.input(), "hello");
        assert!(!composer.should_show_popup());
        assert!(composer.suggestions().is_empty());
    }

    #[test]
    fn test_backspace_removes_character() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');

        assert_eq!(composer.input(), "/st");

        composer.delete_char();
        assert_eq!(composer.input(), "/s");

        composer.delete_char();
        assert_eq!(composer.input(), "/");

        composer.delete_char();
        assert_eq!(composer.input(), "");
    }

    #[test]
    fn test_clear_resets_state() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');

        composer.clear();

        assert_eq!(composer.input(), "");
        assert!(!composer.should_show_popup());
        assert_eq!(composer.cursor_pos, 0);
    }

    #[test]
    fn test_selection_navigation() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');

        // Should start at index 0
        assert_eq!(composer.selected_index, 0);

        composer.move_selection_down();
        assert_eq!(composer.selected_index, 1);

        composer.move_selection_down();
        assert_eq!(composer.selected_index, 2);

        composer.move_selection_up();
        assert_eq!(composer.selected_index, 1);

        composer.move_selection_up();
        assert_eq!(composer.selected_index, 0);

        // Should not go below 0
        composer.move_selection_up();
        assert_eq!(composer.selected_index, 0);
    }

    #[test]
    fn test_tab_completion() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');

        // Should suggest /start <pipeline>
        let suggestions = composer.suggestions();
        assert_eq!(suggestions.len(), 1);
        assert_eq!(suggestions[0].0, "/start <pipeline>");

        // Tab completion should fill in the command
        composer.complete_with_selection();

        assert_eq!(composer.input(), "/start ");
        assert!(!composer.should_show_popup());
    }

    #[test]
    fn test_selected_suggestion() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');

        let selected = composer.selected_suggestion();
        assert!(selected.is_some());
        assert_eq!(selected.unwrap().0, "/start <pipeline>");

        composer.move_selection_down();
        let selected = composer.selected_suggestion();
        assert_eq!(selected.unwrap().0, "/pause <process_id>");
    }

    #[test]
    fn test_popup_hides_after_space() {
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');
        composer.insert_char('a');
        composer.insert_char('r');
        composer.insert_char('t');

        assert!(composer.should_show_popup());

        composer.insert_char(' ');

        // Popup should hide after typing space (entering arguments)
        assert!(!composer.should_show_popup());
    }

    #[test]
    fn test_parse_start_command() {
        let mut composer = CommandComposer::new();
        for c in "/start my-pipeline".chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_ok());

        let op = result.unwrap();
        assert!(op.is_some());

        match op.unwrap() {
            Op::StartPipeline {
                name,
                reference_file,
            } => {
                assert_eq!(name, "my-pipeline");
                assert!(reference_file.is_none());
            }
            _ => panic!("Expected StartPipeline op"),
        }
    }

    #[test]
    fn test_parse_list_command() {
        let mut composer = CommandComposer::new();
        for c in "/list".chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_ok());

        let op = result.unwrap();
        assert!(op.is_some());

        match op.unwrap() {
            Op::GetDashboardState => {}
            _ => panic!("Expected GetDashboardState op"),
        }
    }

    #[test]
    fn test_parse_pause_command() {
        let process_id = Uuid::new_v4();
        let mut composer = CommandComposer::new();
        for c in format!("/pause {}", process_id).chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_ok());

        let op = result.unwrap();
        assert!(op.is_some());

        match op.unwrap() {
            Op::PauseProcess { process_id: id } => {
                assert_eq!(id, process_id);
            }
            _ => panic!("Expected PauseProcess op"),
        }
    }

    #[test]
    fn test_parse_resume_command() {
        let process_id = Uuid::new_v4();
        let mut composer = CommandComposer::new();
        for c in format!("/resume {}", process_id).chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_ok());

        let op = result.unwrap();
        assert!(op.is_some());

        match op.unwrap() {
            Op::ResumeProcess { process_id: id } => {
                assert_eq!(id, process_id);
            }
            _ => panic!("Expected ResumeProcess op"),
        }
    }

    #[test]
    fn test_parse_kill_command() {
        let process_id = Uuid::new_v4();
        let mut composer = CommandComposer::new();
        for c in format!("/kill {}", process_id).chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_ok());

        let op = result.unwrap();
        assert!(op.is_some());

        match op.unwrap() {
            Op::KillProcess { process_id: id } => {
                assert_eq!(id, process_id);
            }
            _ => panic!("Expected KillProcess op"),
        }
    }

    #[test]
    fn test_parse_empty_command() {
        let composer = CommandComposer::new();

        let result = composer.parse_command();
        assert!(result.is_ok());
        assert!(result.unwrap().is_none());
    }

    #[test]
    fn test_parse_invalid_command() {
        let mut composer = CommandComposer::new();
        for c in "/invalid".chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Unknown command"));
    }

    #[test]
    fn test_parse_missing_argument() {
        let mut composer = CommandComposer::new();
        for c in "/start".chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Missing pipeline name"));
    }

    #[test]
    fn test_parse_invalid_uuid() {
        let mut composer = CommandComposer::new();
        for c in "/pause invalid-uuid".chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Invalid process ID format"));
    }

    #[test]
    fn test_parse_non_slash_command() {
        let mut composer = CommandComposer::new();
        for c in "hello world".chars() {
            composer.insert_char(c);
        }

        let result = composer.parse_command();
        assert!(result.is_err());
        assert!(result.unwrap_err().contains("Commands must start with"));
    }

    // ========================================================================
    // RED TESTS: Event Handling Delegation (Ticket 9.3)
    // ========================================================================

    #[test]
    fn test_handle_key_event_cursor_left() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');
        assert_eq!(composer.cursor_pos, 3);

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Left));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.cursor_pos, 2);
    }

    #[test]
    fn test_handle_key_event_cursor_right() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.move_cursor_left();
        assert_eq!(composer.cursor_pos, 1);

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Right));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.cursor_pos, 2);
    }

    #[test]
    fn test_handle_key_event_char_input() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Char('/')));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.input(), "/");

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Char('s')));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.input(), "/s");
    }

    #[test]
    fn test_handle_key_event_backspace() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Backspace));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.input(), "/");
    }

    #[test]
    fn test_handle_key_event_up_with_popup() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        assert!(composer.should_show_popup());
        assert_eq!(composer.selected_index, 0);

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Down));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.selected_index, 1);

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Up));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.selected_index, 0);
    }

    #[test]
    fn test_handle_key_event_down_with_popup() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        assert!(composer.should_show_popup());
        assert_eq!(composer.selected_index, 0);

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Down));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.selected_index, 1);
    }

    #[test]
    fn test_handle_key_event_tab_completes_command() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');
        assert!(composer.should_show_popup());

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Tab));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.input(), "/start ");
        assert!(!composer.should_show_popup());
    }

    #[test]
    fn test_handle_key_event_esc_clears_input() {
        // RED: This test will fail because handle_key_event doesn't exist yet
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('s');
        composer.insert_char('t');

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Esc));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.input(), "");
    }

    #[test]
    fn test_handle_key_event_enter_not_consumed() {
        // RED: Enter should NOT be consumed by CommandComposer - it's handled by App
        let mut composer = CommandComposer::new();
        composer.insert_char('/');
        composer.insert_char('l');
        composer.insert_char('i');
        composer.insert_char('s');
        composer.insert_char('t');

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Enter));
        assert_eq!(status, EventStatus::NotConsumed);
        // Input should remain unchanged
        assert_eq!(composer.input(), "/list");
    }

    #[test]
    fn test_handle_key_event_quit_keys_not_consumed() {
        // When input is empty, 'q' should NOT be consumed (it's a global quit)
        let mut composer = CommandComposer::new();

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Char('q')));
        assert_eq!(status, EventStatus::NotConsumed);
        // Input should remain empty
        assert_eq!(composer.input(), "");
    }

    #[test]
    fn test_handle_key_event_q_consumed_with_input() {
        // But if there's already input, 'q' should be treated as a regular character
        let mut composer = CommandComposer::new();
        composer.insert_char('/');

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Char('q')));
        assert_eq!(status, EventStatus::Consumed);
        assert_eq!(composer.input(), "/q");
    }

    #[test]
    fn test_handle_key_event_up_down_without_popup_not_consumed() {
        // RED: When popup is not shown, Up/Down should NOT be consumed
        // (they're for process navigation in App)
        let mut composer = CommandComposer::new();
        composer.insert_char('h');
        composer.insert_char('e');
        composer.insert_char('l');
        composer.insert_char('l');
        composer.insert_char('o');
        assert!(!composer.should_show_popup());

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Up));
        assert_eq!(status, EventStatus::NotConsumed);

        let status = composer.handle_key_event(KeyEvent::from(KeyCode::Down));
        assert_eq!(status, EventStatus::NotConsumed);
    }
}
</file>

<file path="pipeline-kit-rs/crates/tui/src/event_handler.rs">
//! Event handling utilities for the TUI.
//!
//! This module provides functions for handling different types of events:
//! - Core events (from pk-core)
//! - Keyboard events (user input)
//! - Command parsing and submission

use crossterm::event::KeyCode;
use crossterm::event::KeyEvent;
use crossterm::event::KeyEventKind;
use pk_protocol::Event;
use pk_protocol::Op;
use pk_protocol::Process;
use pk_protocol::ProcessStatus;
use std::sync::Arc;
use tokio::sync::mpsc::UnboundedSender;
use tokio::sync::Notify;

/// Handle an event received from the core.
pub fn handle_core_event(processes: &mut Vec<Process>, event: Event) {
    match event {
        Event::ProcessStarted {
            process_id,
            pipeline_name,
        } => {
            let process = Process {
                id: process_id,
                pipeline_name,
                status: ProcessStatus::Running,
                current_step_index: 0,
                logs: Vec::new(),
                started_at: chrono::Utc::now(),
                completed_at: None,
                resume_notifier: Arc::new(Notify::new()),
            };
            processes.push(process);
        }
        Event::ProcessStatusUpdate {
            process_id,
            status,
            step_index,
        } => {
            if let Some(process) = processes.iter_mut().find(|p| p.id == process_id) {
                process.status = status;
                process.current_step_index = step_index;
            }
        }
        Event::ProcessLogChunk {
            process_id,
            content,
        } => {
            if let Some(process) = processes.iter_mut().find(|p| p.id == process_id) {
                process.logs.push(content);
            }
        }
        Event::ProcessCompleted { process_id } => {
            if let Some(process) = processes.iter_mut().find(|p| p.id == process_id) {
                process.status = ProcessStatus::Completed;
                process.completed_at = Some(chrono::Utc::now());
            }
        }
        Event::ProcessError { process_id, error } => {
            if let Some(process) = processes.iter_mut().find(|p| p.id == process_id) {
                process.status = ProcessStatus::Failed;
                process.logs.push(format!("ERROR: {}", error));
                process.completed_at = Some(chrono::Utc::now());
            }
        }
        Event::ProcessKilled { process_id } => {
            if let Some(process) = processes.iter_mut().find(|p| p.id == process_id) {
                process.status = ProcessStatus::Killed;
                process.logs.push("Process killed by user".to_string());
                process.completed_at = Some(chrono::Utc::now());
            }
        }
        Event::ProcessResumed { process_id } => {
            if let Some(process) = processes.iter_mut().find(|p| p.id == process_id) {
                process.status = ProcessStatus::Running;
                process.logs.push("Process resumed".to_string());
            }
        }
    }
}

/// Handle a keyboard event from the user.
///
/// Returns `true` if the application should exit, `false` otherwise.
pub fn handle_keyboard_event(
    key_event: KeyEvent,
    command_input: &mut String,
    selected_index: &mut usize,
    processes: &[Process],
    op_tx: &UnboundedSender<Op>,
) -> bool {
    if key_event.kind != KeyEventKind::Press {
        return false;
    }

    match key_event.code {
        KeyCode::Char('q') => {
            return true;
        }
        KeyCode::Up => {
            if *selected_index > 0 {
                *selected_index -= 1;
            }
        }
        KeyCode::Down => {
            if *selected_index < processes.len().saturating_sub(1) {
                *selected_index += 1;
            }
        }
        KeyCode::Char(c) => {
            command_input.push(c);
        }
        KeyCode::Backspace => {
            command_input.pop();
        }
        KeyCode::Enter => {
            submit_command(command_input, *selected_index, processes, op_tx);
        }
        _ => {}
    }

    false
}

/// Submit the current command input.
fn submit_command(
    command_input: &mut String,
    selected_index: usize,
    processes: &[Process],
    op_tx: &UnboundedSender<Op>,
) {
    if command_input.is_empty() {
        return;
    }

    // Parse simple slash commands
    if command_input.starts_with('/') {
        let parts: Vec<&str> = command_input.split_whitespace().collect();
        match parts.first().copied() {
            Some("/start") => {
                if let Some(name) = parts.get(1) {
                    let _ = op_tx.send(Op::StartPipeline {
                        name: name.to_string(),
                        reference_file: None,
                    });
                }
            }
            Some("/pause") => {
                if let Some(process) = processes.get(selected_index) {
                    let _ = op_tx.send(Op::PauseProcess {
                        process_id: process.id,
                    });
                }
            }
            Some("/resume") => {
                if let Some(process) = processes.get(selected_index) {
                    let _ = op_tx.send(Op::ResumeProcess {
                        process_id: process.id,
                    });
                }
            }
            Some("/kill") => {
                if let Some(process) = processes.get(selected_index) {
                    let _ = op_tx.send(Op::KillProcess {
                        process_id: process.id,
                    });
                }
            }
            _ => {}
        }
    }

    command_input.clear();
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc::unbounded_channel;
    use uuid::Uuid;

    #[test]
    fn test_handle_core_event_process_started() {
        let mut processes = Vec::new();
        let process_id = Uuid::new_v4();

        handle_core_event(
            &mut processes,
            Event::ProcessStarted {
                process_id,
                pipeline_name: "test-pipeline".to_string(),
            },
        );

        assert_eq!(processes.len(), 1);
        assert_eq!(processes[0].id, process_id);
        assert_eq!(processes[0].pipeline_name, "test-pipeline");
    }

    #[test]
    fn test_handle_keyboard_event_quit() {
        let mut command_input = String::new();
        let mut selected_index = 0;
        let processes = Vec::new();
        let (op_tx, _op_rx) = unbounded_channel();

        let should_exit = handle_keyboard_event(
            KeyEvent::from(KeyCode::Char('q')),
            &mut command_input,
            &mut selected_index,
            &processes,
            &op_tx,
        );

        assert!(should_exit);
    }

    #[test]
    fn test_handle_keyboard_event_navigation() {
        let mut command_input = String::new();
        let mut selected_index = 1;
        let (op_tx, _op_rx) = unbounded_channel();

        // Create 3 test processes
        let processes = vec![
            Process {
                id: Uuid::new_v4(),
                pipeline_name: "p1".to_string(),
                status: ProcessStatus::Running,
                current_step_index: 0,
                logs: Vec::new(),
                started_at: chrono::Utc::now(),
                completed_at: None,
                resume_notifier: Arc::new(Notify::new()),
            },
            Process {
                id: Uuid::new_v4(),
                pipeline_name: "p2".to_string(),
                status: ProcessStatus::Running,
                current_step_index: 0,
                logs: Vec::new(),
                started_at: chrono::Utc::now(),
                completed_at: None,
                resume_notifier: Arc::new(Notify::new()),
            },
            Process {
                id: Uuid::new_v4(),
                pipeline_name: "p3".to_string(),
                status: ProcessStatus::Running,
                current_step_index: 0,
                logs: Vec::new(),
                started_at: chrono::Utc::now(),
                completed_at: None,
                resume_notifier: Arc::new(Notify::new()),
            },
        ];

        // Test Down
        handle_keyboard_event(
            KeyEvent::from(KeyCode::Down),
            &mut command_input,
            &mut selected_index,
            &processes,
            &op_tx,
        );
        assert_eq!(selected_index, 2);

        // Test Up
        handle_keyboard_event(
            KeyEvent::from(KeyCode::Up),
            &mut command_input,
            &mut selected_index,
            &processes,
            &op_tx,
        );
        assert_eq!(selected_index, 1);
    }

    #[test]
    fn test_command_input() {
        let mut command_input = String::new();
        let mut selected_index = 0;
        let processes = Vec::new();
        let (op_tx, _op_rx) = unbounded_channel();

        // Type some characters
        handle_keyboard_event(
            KeyEvent::from(KeyCode::Char('/')),
            &mut command_input,
            &mut selected_index,
            &processes,
            &op_tx,
        );
        handle_keyboard_event(
            KeyEvent::from(KeyCode::Char('s')),
            &mut command_input,
            &mut selected_index,
            &processes,
            &op_tx,
        );

        assert_eq!(command_input, "/s");

        // Backspace
        handle_keyboard_event(
            KeyEvent::from(KeyCode::Backspace),
            &mut command_input,
            &mut selected_index,
            &processes,
            &op_tx,
        );

        assert_eq!(command_input, "/");
    }
}
</file>

<file path="package.json">
{
  "name": "pipeline-kit-monorepo",
  "private": true,
  "scripts": {
    "check-rs": "cd pipeline-kit-rs && cargo check --workspace"
  },
  "packageManager": "pnpm@9.12.2"
}
</file>

<file path=".github/workflows/ci.yml">
name: ci

on:
  pull_request: { branches: [main] }
  push: { branches: [main] }

jobs:
  build-test:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      # Node.js/pnpm Setup
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 9.12.2
          run_install: false

      - name: Setup Node.js
        uses: actions/setup-node@v5
        with:
          node-version: 22
          cache: "pnpm"

      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      # TypeScript Tests
      - name: Run TypeScript tests
        working-directory: pipeline-kit-cli
        run: pnpm test

      # TypeScript Type Generation & Verification
      - name: Verify TypeScript types
        run: bash scripts/verify-ts-types.sh

      # Documentation & Code Quality
      - name: Check README files contain only ASCII
        run: |
          python3 scripts/asciicheck.py README.md
          if [ -f pipeline-kit-cli/README.md ]; then
            python3 scripts/asciicheck.py pipeline-kit-cli/README.md
          fi
          if [ -f pipeline-kit-rs/README.md ]; then
            python3 scripts/asciicheck.py pipeline-kit-rs/README.md
          fi
</file>

<file path=".github/workflows/rust-ci.yml">
name: rust-ci
on:
  pull_request: {}
  push:
    branches:
      - main
  workflow_dispatch:

# CI builds in debug (dev) for faster signal.

jobs:
  # --- Detect what changed (always runs) -------------------------------------
  changed:
    name: Detect changed areas
    runs-on: ubuntu-24.04
    outputs:
      rust: ${{ steps.detect.outputs.rust }}
      typescript: ${{ steps.detect.outputs.typescript }}
      workflows: ${{ steps.detect.outputs.workflows }}
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0
      - name: Detect changed paths
        id: detect
        shell: bash
        run: |
          set -euo pipefail

          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            BASE_SHA='${{ github.event.pull_request.base.sha }}'
            echo "Base SHA: $BASE_SHA"
            # List files changed between base and current HEAD (merge-base aware)
            mapfile -t files < <(git diff --name-only --no-renames "$BASE_SHA"...HEAD)
          else
            # On push / manual runs, default to running everything
            files=("pipeline-kit-rs/force" "pipeline-kit-cli/force" ".github/force")
          fi

          rust=false
          typescript=false
          workflows=false
          for f in "${files[@]}"; do
            [[ $f == pipeline-kit-rs/* ]] && rust=true
            [[ $f == pipeline-kit-cli/* ]] && typescript=true
            [[ $f == .github/* ]] && workflows=true
          done

          echo "rust=$rust" >> "$GITHUB_OUTPUT"
          echo "typescript=$typescript" >> "$GITHUB_OUTPUT"
          echo "workflows=$workflows" >> "$GITHUB_OUTPUT"

  # --- CI that doesn't need specific targets ---------------------------------
  general:
    name: Format / Docs / etc
    runs-on: ubuntu-24.04
    needs: changed
    if: ${{ needs.changed.outputs.rust == 'true' || needs.changed.outputs.workflows == 'true' || github.event_name == 'push' }}
    defaults:
      run:
        working-directory: pipeline-kit-rs
    steps:
      - uses: actions/checkout@v5
      - uses: dtolnay/rust-toolchain@1.89
        with:
          components: rustfmt
      - name: cargo fmt
        run: cargo fmt -- --config imports_granularity=Item --check
      - name: cargo doc
        run: cargo doc --no-deps --all-features

  cargo_shear:
    name: cargo shear
    runs-on: ubuntu-24.04
    needs: changed
    if: ${{ needs.changed.outputs.rust == 'true' || needs.changed.outputs.workflows == 'true' || github.event_name == 'push' }}
    defaults:
      run:
        working-directory: pipeline-kit-rs
    steps:
      - uses: actions/checkout@v5
      - uses: dtolnay/rust-toolchain@1.89
      - uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
          key: cargo-shear-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}
      - uses: taiki-e/install-action@v2
        with:
          tool: cargo-shear
      - name: cargo shear
        run: cargo shear

  # --- CI to validate on different os/targets --------------------------------
  lint_build_test:
    name: ${{ matrix.runner }} - ${{ matrix.target }}${{ matrix.profile == 'release' && ' (release)' || '' }}
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 30
    needs: changed
    # Keep job-level if to avoid spinning up runners when not needed
    if: ${{ needs.changed.outputs.rust == 'true' || needs.changed.outputs.workflows == 'true' || github.event_name == 'push' }}
    defaults:
      run:
        working-directory: pipeline-kit-rs

    strategy:
      fail-fast: false
      matrix:
        include:
          - runner: macos-14
            target: aarch64-apple-darwin
            profile: dev
          - runner: macos-14
            target: x86_64-apple-darwin
            profile: dev
          - runner: ubuntu-24.04
            target: x86_64-unknown-linux-musl
            profile: dev
          - runner: ubuntu-24.04
            target: x86_64-unknown-linux-gnu
            profile: dev
          - runner: ubuntu-24.04-arm
            target: aarch64-unknown-linux-musl
            profile: dev
          - runner: ubuntu-24.04-arm
            target: aarch64-unknown-linux-gnu
            profile: dev
          - runner: windows-latest
            target: x86_64-pc-windows-msvc
            profile: dev

          # Also run representative release builds on Mac and Linux because
          # there could be release-only build errors we want to catch.
          # Hopefully this also pre-populates the build cache to speed up
          # releases.
          - runner: macos-14
            target: aarch64-apple-darwin
            profile: release
          - runner: ubuntu-24.04
            target: x86_64-unknown-linux-musl
            profile: release
          - runner: windows-latest
            target: x86_64-pc-windows-msvc
            profile: release

    steps:
      - uses: actions/checkout@v5
      - uses: dtolnay/rust-toolchain@1.89
        with:
          targets: ${{ matrix.target }}
          components: clippy

      - uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            ${{ github.workspace }}/pipeline-kit-rs/target/
          key: cargo-${{ matrix.runner }}-${{ matrix.target }}-${{ matrix.profile }}-${{ hashFiles('**/Cargo.lock') }}

      - if: ${{ matrix.target == 'x86_64-unknown-linux-musl' || matrix.target == 'aarch64-unknown-linux-musl'}}
        name: Install musl build tools
        run: |
          sudo apt install -y musl-tools pkg-config && sudo rm -rf /var/lib/apt/lists/*

      - name: cargo clippy
        id: clippy
        run: cargo clippy --target ${{ matrix.target }} --all-features --tests --profile ${{ matrix.profile }} -- -D warnings

      # Running `cargo build` from the workspace root builds the workspace using
      # the union of all features from third-party crates. This can mask errors
      # where individual crates have underspecified features. To avoid this, we
      # run `cargo check` for each crate individually, though because this is
      # slower, we only do this for the x86_64-unknown-linux-gnu target.
      - name: cargo check individual crates
        id: cargo_check_all_crates
        if: ${{ matrix.target == 'x86_64-unknown-linux-gnu' && matrix.profile != 'release' }}
        continue-on-error: true
        run: |
          find . -name Cargo.toml -mindepth 2 -maxdepth 2 -print0 \
            | xargs -0 -n1 -I{} bash -c 'cd "$(dirname "{}")" && cargo check --profile ${{ matrix.profile }}'

      - uses: taiki-e/install-action@v2
        with:
          tool: nextest

      - name: tests
        id: test
        # Tests take too long for release builds to run them on every PR.
        if: ${{ matrix.profile != 'release' }}
        continue-on-error: true
        run: cargo nextest run --all-features --no-fail-fast --target ${{ matrix.target }}
        env:
          RUST_BACKTRACE: 1

      - name: Generate test report
        if: always() && steps.test.outcome != 'skipped' && matrix.target == 'x86_64-unknown-linux-gnu'
        continue-on-error: true
        run: |
          cargo nextest run --all-features \
            --target ${{ matrix.target }} \
            --profile ci \
            --no-fail-fast \
            || true

      - name: Upload test results
        if: always() && steps.test.outcome != 'skipped' && matrix.target == 'x86_64-unknown-linux-gnu'
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.runner }}-${{ matrix.target }}
          path: |
            pipeline-kit-rs/target/nextest/ci/junit.xml
          retention-days: 7

      # Fail the job if any of the previous steps failed.
      - name: verify all steps passed
        if: |
          steps.clippy.outcome == 'failure' ||
          steps.cargo_check_all_crates.outcome == 'failure' ||
          steps.test.outcome == 'failure'
        run: |
          echo "One or more checks failed (clippy, cargo_check_all_crates, or test). See logs for details."
          exit 1

  # --- Gatherer job that you mark as the ONLY required status -----------------
  results:
    name: CI results (required)
    needs: [changed, general, cargo_shear, lint_build_test]
    if: always()
    runs-on: ubuntu-24.04
    steps:
      - name: Summarize
        shell: bash
        run: |
          echo "general: ${{ needs.general.result }}"
          echo "shear  : ${{ needs.cargo_shear.result }}"
          echo "matrix : ${{ needs.lint_build_test.result }}"

          # If nothing relevant changed (PR touching only root README, etc.),
          # declare success regardless of other jobs.
          if [[ '${{ needs.changed.outputs.rust }}' != 'true' && '${{ needs.changed.outputs.workflows }}' != 'true' && '${{ github.event_name }}' != 'push' ]]; then
            echo 'No relevant changes -> CI not required.'
            exit 0
          fi

          # Otherwise require the jobs to have succeeded
          [[ '${{ needs.general.result }}' == 'success' ]] || { echo 'general failed'; exit 1; }
          [[ '${{ needs.cargo_shear.result }}' == 'success' ]] || { echo 'cargo_shear failed'; exit 1; }
          [[ '${{ needs.lint_build_test.result }}' == 'success' ]] || { echo 'matrix failed'; exit 1; }
</file>

<file path="pipeline-kit-rs/crates/cli/src/main.rs">
use clap::{Parser, Subcommand};
use colored::Colorize;
use pk_core::init::{generate_pipeline_kit_structure, InitOptions};
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "pipeline-kit")]
#[command(version, about = "AI agent pipeline orchestration CLI", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Option<Commands>,
}

#[derive(Subcommand)]
enum Commands {
    /// Initialize a new .pipeline-kit directory with templates
    Init {
        /// Overwrite existing .pipeline-kit directory
        #[arg(short, long)]
        force: bool,

        /// Create minimal template only (1 agent, 1 pipeline)
        #[arg(short, long)]
        minimal: bool,

        /// Target directory (default: current directory)
        #[arg(short, long)]
        path: Option<PathBuf>,
    },
}

#[tokio::main]
async fn main() -> color_eyre::Result<()> {
    let cli = Cli::parse();

    match cli.command {
        None => {
            // When `pipeline` is called without any arguments, launch the TUI
            pk_tui::run_app()
                .await
                .map_err(|e| color_eyre::eyre::eyre!(e))
        }
        Some(Commands::Init {
            force,
            minimal,
            path,
        }) => {
            // Determine target directory
            let target_dir = path
                .unwrap_or_else(|| std::env::current_dir().unwrap_or_else(|_| PathBuf::from(".")));

            // Create init options
            let options = InitOptions {
                target_dir: target_dir.clone(),
                force,
                minimal,
            };

            // Execute initialization
            match generate_pipeline_kit_structure(options).await {
                Ok(_) => {
                    println!("{}", "‚úì Created .pipeline-kit/ directory structure".green());
                    println!("{}", "‚úì Generated config.toml".green());

                    if minimal {
                        println!("{}", "‚úì Created 1 agent template (developer)".green());
                        println!("{}", "‚úì Created 1 pipeline template (simple-task)".green());
                    } else {
                        println!(
                            "{}",
                            "‚úì Created 2 agent templates (developer, reviewer)".green()
                        );
                        println!(
                            "{}",
                            "‚úì Created 2 pipeline templates (simple-task, code-review)".green()
                        );
                    }

                    println!();
                    println!(
                        "{}",
                        "üéâ Pipeline Kit initialized successfully!".bold().green()
                    );
                    println!();
                    println!("Next steps:");
                    println!("  1. Set up your API keys:");
                    println!("     {}", "export ANTHROPIC_API_KEY=your_api_key".cyan());
                    println!();
                    println!("  2. Launch the TUI:");
                    println!("     {}", "pipeline-kit".cyan());
                    println!();
                    println!("  3. Start a pipeline:");
                    println!(
                        "     {}",
                        "Type '/start simple-task' in the command input".cyan()
                    );
                    println!();
                    println!(
                        "For more information, visit: {}",
                        "https://github.com/Vooster-AI/pipeline-kit".blue()
                    );

                    Ok(())
                }
                Err(e) => {
                    eprintln!("{} {}", "Error:".red().bold(), e);
                    std::process::exit(1);
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cli_parsing_no_args() {
        // Test that no arguments defaults to TUI mode
        let cli = Cli::try_parse_from(["pipeline-kit"]).unwrap();
        assert!(cli.command.is_none());
    }

    #[test]
    fn test_cli_parsing_init() {
        let cli = Cli::try_parse_from(["pipeline-kit", "init"]).unwrap();
        assert!(matches!(cli.command, Some(Commands::Init { .. })));
    }

    #[test]
    fn test_cli_parsing_init_with_force() {
        let cli = Cli::try_parse_from(["pipeline-kit", "init", "--force"]).unwrap();
        if let Some(Commands::Init {
            force,
            minimal,
            path,
        }) = cli.command
        {
            assert!(force);
            assert!(!minimal);
            assert!(path.is_none());
        } else {
            panic!("Expected Init command");
        }
    }

    #[test]
    fn test_cli_parsing_init_with_minimal() {
        let cli = Cli::try_parse_from(["pipeline-kit", "init", "--minimal"]).unwrap();
        if let Some(Commands::Init {
            force,
            minimal,
            path,
        }) = cli.command
        {
            assert!(!force);
            assert!(minimal);
            assert!(path.is_none());
        } else {
            panic!("Expected Init command");
        }
    }

    #[test]
    fn test_cli_parsing_init_with_path() {
        let cli = Cli::try_parse_from(["pipeline-kit", "init", "--path", "/tmp/test"]).unwrap();
        if let Some(Commands::Init {
            force,
            minimal,
            path,
        }) = cli.command
        {
            assert!(!force);
            assert!(!minimal);
            assert_eq!(path, Some(PathBuf::from("/tmp/test")));
        } else {
            panic!("Expected Init command");
        }
    }

    #[test]
    fn test_cli_parsing_init_with_all_flags() {
        let cli = Cli::try_parse_from([
            "pipeline-kit",
            "init",
            "--force",
            "--minimal",
            "--path",
            "/tmp/test",
        ])
        .unwrap();
        if let Some(Commands::Init {
            force,
            minimal,
            path,
        }) = cli.command
        {
            assert!(force);
            assert!(minimal);
            assert_eq!(path, Some(PathBuf::from("/tmp/test")));
        } else {
            panic!("Expected Init command");
        }
    }
}
</file>

<file path="pipeline-kit-rs/crates/cli/Cargo.toml">
[package]
name = "pk-cli"
version = { workspace = true }
edition = { workspace = true }
authors = { workspace = true }
license = { workspace = true }
repository = { workspace = true }
description = "CLI entry point for pipeline-kit"

[[bin]]
name = "pipeline"
path = "src/main.rs"

[dependencies]
pk-tui = { path = "../tui" }
pk-core = { path = "../core" }
tokio = { version = "1", features = ["full"] }
color-eyre = "0.6"
clap = { version = "4.5", features = ["derive"] }
colored = "2.1"
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/claude_adapter.rs">
//! Claude adapter implementation using Claude CLI subprocess.

use crate::agents::base::Agent;
use crate::agents::base::AgentError;
use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use crate::agents::cli_executor::CliExecutor;
use async_trait::async_trait;
use serde::Deserialize;
use serde::Serialize;
use std::collections::HashMap;
use std::pin::Pin;
use std::process::Stdio;
use std::sync::Arc;
use std::sync::Mutex;
use tokio::process::Command;
use tokio_stream::Stream;
use tokio_stream::StreamExt;

/// Claude adapter for executing instructions using Claude CLI.
///
/// This adapter spawns the `claude` CLI as a subprocess and parses its
/// JSON Lines output to create a stream of AgentEvents.
pub struct ClaudeAdapter {
    #[allow(dead_code)]
    name: String,
    model: String,
    system_prompt: String,
    /// Session mapping: project_id -> session_id
    session_mapping: Arc<Mutex<HashMap<String, String>>>,
}

impl ClaudeAdapter {
    /// Create a new Claude adapter.
    ///
    /// # Arguments
    ///
    /// * `name` - The agent name from configuration
    /// * `model` - The Claude model to use (e.g., "claude-sonnet-4.5")
    /// * `system_prompt` - The system prompt for the agent
    pub fn new(name: String, model: String, system_prompt: String) -> Result<Self, AgentError> {
        Ok(Self {
            name,
            model,
            system_prompt,
            session_mapping: Arc::new(Mutex::new(HashMap::new())),
        })
    }

    /// Extract project ID from project path.
    ///
    /// Uses the last component of the path as the project ID.
    fn extract_project_id(project_path: &str) -> String {
        std::path::Path::new(project_path)
            .file_name()
            .and_then(|s| s.to_str())
            .unwrap_or(project_path)
            .to_string()
    }

    /// Create a temporary settings file for the Claude CLI.
    ///
    /// The settings file contains the system prompt.
    fn create_settings_file(&self) -> Result<tempfile::NamedTempFile, AgentError> {
        use std::io::Write;

        let settings = serde_json::json!({
            "customSystemPrompt": self.system_prompt
        });

        let mut temp_file = tempfile::NamedTempFile::new().map_err(|e| {
            AgentError::ExecutionError(format!("Failed to create temp file: {}", e))
        })?;

        serde_json::to_writer(&mut temp_file, &settings)
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write settings: {}", e)))?;

        // Flush to ensure data is written
        temp_file
            .flush()
            .map_err(|e| AgentError::ExecutionError(format!("Failed to flush settings: {}", e)))?;

        Ok(temp_file)
    }
}

#[async_trait]
impl Agent for ClaudeAdapter {
    async fn check_availability(&self) -> bool {
        // Check if claude CLI is installed by running "claude -h"
        match Command::new("claude")
            .arg("-h")
            .stdout(Stdio::null())
            .stderr(Stdio::null())
            .status()
            .await
        {
            Ok(status) => status.success(),
            Err(_) => false,
        }
    }

    async fn execute(
        &self,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        // 1. Create settings file
        let settings_file = self.create_settings_file()?;
        let settings_path = settings_file
            .path()
            .to_str()
            .ok_or_else(|| AgentError::ExecutionError("Invalid settings path".to_string()))?
            .to_string();

        // 2. Get session ID for this project
        let project_id = Self::extract_project_id(&context.project_path);
        let session_id = {
            let mapping = self.session_mapping.lock().unwrap();
            mapping.get(&project_id).cloned()
        };

        // 3. Build command arguments
        let mut args = vec![
            "--settings".to_string(),
            settings_path,
            "--model".to_string(),
            self.model.clone(),
            "--permission-mode".to_string(),
            "bypassPermissions".to_string(),
            "--continue-conversation".to_string(),
        ];

        // Tool filtering based on is_initial_prompt
        if context.is_initial_prompt {
            // Initial prompt: exclude TodoWrite
            args.push("--allowed-tools".to_string());
            args.push("Read,Write,Edit,MultiEdit,Bash,Glob,Grep,LS,WebFetch,WebSearch".to_string());
            args.push("--disallowed-tools".to_string());
            args.push("TodoWrite".to_string());
        } else {
            // Subsequent prompts: include TodoWrite
            args.push("--allowed-tools".to_string());
            args.push(
                "Read,Write,Edit,MultiEdit,Bash,Glob,Grep,LS,WebFetch,WebSearch,TodoWrite"
                    .to_string(),
            );
        }

        // Session resumption
        if let Some(sid) = &session_id {
            args.push("--resume-session-id".to_string());
            args.push(sid.clone());
        }

        // Prompt
        args.push("--prompt".to_string());
        args.push(context.instruction.clone());

        // 4. Execute CLI using CliExecutor
        let json_stream =
            CliExecutor::execute("claude".to_string(), args, context.project_path.clone());

        // 5. Convert JSON stream to AgentEvents
        let session_mapping = self.session_mapping.clone();
        let project_id_clone = project_id.clone();

        let events_stream = json_stream
            .then(move |json_result| {
                let session_mapping = session_mapping.clone();
                let project_id = project_id_clone.clone();

                async move {
                    match json_result {
                        Ok(json_value) => {
                            // Parse as ClaudeMessage
                            match serde_json::from_value::<ClaudeMessage>(json_value.clone()) {
                                Ok(msg) => {
                                    convert_claude_message(msg, session_mapping, project_id).await
                                }
                                Err(e) => Some(Err(AgentError::StreamParseError(format!(
                                    "Failed to parse ClaudeMessage: {} (json: {})",
                                    e, json_value
                                )))),
                            }
                        }
                        Err(e) => Some(Err(e)),
                    }
                }
            })
            .filter_map(|opt| opt);

        Ok(Box::pin(events_stream))
    }
}

/// Claude CLI message types (JSON Lines output).
#[derive(Debug, Deserialize)]
#[serde(tag = "type")]
enum ClaudeMessage {
    #[serde(rename = "system")]
    System {
        session_id: Option<String>,
        #[allow(dead_code)]
        model: Option<String>,
    },
    #[serde(rename = "assistant")]
    Assistant { content: Vec<ContentBlock> },
    #[serde(rename = "user")]
    User {
        #[allow(dead_code)]
        content: String,
    },
    #[serde(rename = "result")]
    Result {
        session_id: Option<String>,
        #[allow(dead_code)]
        duration_ms: Option<u64>,
        #[allow(dead_code)]
        total_cost_usd: Option<f64>,
        #[allow(dead_code)]
        num_turns: Option<u32>,
        #[allow(dead_code)]
        is_error: Option<bool>,
    },
}

/// Content blocks within an assistant message.
#[derive(Debug, Deserialize, Serialize)]
#[serde(tag = "type")]
enum ContentBlock {
    #[serde(rename = "text")]
    Text { text: String },
    #[serde(rename = "tool_use")]
    ToolUse {
        id: String,
        name: String,
        input: serde_json::Value,
    },
    #[serde(rename = "tool_result")]
    ToolResult {
        tool_use_id: String,
        content: String,
    },
}

/// Convert Claude CLI message to AgentEvent.
async fn convert_claude_message(
    msg: ClaudeMessage,
    session_mapping: Arc<Mutex<HashMap<String, String>>>,
    project_id: String,
) -> Option<Result<AgentEvent, AgentError>> {
    match msg {
        ClaudeMessage::System { session_id, .. } => {
            // Save session ID
            if let Some(sid) = session_id {
                let mut mapping = session_mapping.lock().unwrap();
                mapping.insert(project_id, sid);
            }
            // System messages are not shown to UI
            None
        }
        ClaudeMessage::Assistant { content } => {
            // Process content blocks
            for block in content {
                match block {
                    ContentBlock::Text { text } => {
                        if !text.trim().is_empty() {
                            return Some(Ok(AgentEvent::MessageChunk(text)));
                        }
                    }
                    ContentBlock::ToolUse { name, input, .. } => {
                        let tool_json = serde_json::to_string(&serde_json::json!({
                            "name": name,
                            "input": input
                        }))
                        .unwrap_or_else(|_| format!("{{\"name\":\"{}\"}}", name));
                        return Some(Ok(AgentEvent::ToolCall(tool_json)));
                    }
                    ContentBlock::ToolResult { .. } => {
                        // Tool results are not shown to UI
                    }
                }
            }
            None
        }
        ClaudeMessage::User { .. } => {
            // User messages (echoes) are not shown
            None
        }
        ClaudeMessage::Result { session_id, .. } => {
            // Save session ID if present
            if let Some(sid) = session_id {
                let mut mapping = session_mapping.lock().unwrap();
                mapping.insert(project_id, sid);
            }
            // Signal completion
            Some(Ok(AgentEvent::Completed))
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_claude_adapter_new() {
        let adapter = ClaudeAdapter::new(
            "test".to_string(),
            "claude-sonnet-4.5".to_string(),
            "test prompt".to_string(),
        );
        assert!(adapter.is_ok());
    }

    #[test]
    fn test_extract_project_id() {
        assert_eq!(
            ClaudeAdapter::extract_project_id("/path/to/project"),
            "project"
        );
        assert_eq!(ClaudeAdapter::extract_project_id("/tmp/test"), "test");
        assert_eq!(ClaudeAdapter::extract_project_id("relative/path"), "path");
    }

    #[test]
    fn test_create_settings_file() {
        let adapter = ClaudeAdapter::new(
            "test".to_string(),
            "claude-sonnet-4.5".to_string(),
            "test prompt".to_string(),
        )
        .unwrap();

        let settings_file = adapter.create_settings_file();
        assert!(settings_file.is_ok());

        let file = settings_file.unwrap();
        let path = file.path();
        assert!(path.exists());

        // Read and verify content
        let content = std::fs::read_to_string(path).unwrap();
        assert!(content.contains("customSystemPrompt"));
        assert!(content.contains("test prompt"));
    }

    #[tokio::test]
    async fn test_check_availability() {
        let adapter = ClaudeAdapter::new(
            "test".to_string(),
            "claude-sonnet-4.5".to_string(),
            "test prompt".to_string(),
        )
        .unwrap();

        // This will return false unless claude CLI is actually installed
        let available = adapter.check_availability().await;
        // We can't assert true/false as it depends on the environment
        // Just verify it doesn't panic
        let _ = available;
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/cursor_adapter.rs">
//! Cursor adapter implementation using Cursor Agent CLI subprocess.

use crate::agents::base::Agent;
use crate::agents::base::AgentError;
use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use crate::agents::cli_executor::CliExecutor;
use async_trait::async_trait;
use serde::Deserialize;
use std::collections::HashMap;
use std::pin::Pin;
use std::process::Stdio;
use std::sync::Arc;
use std::sync::Mutex;
use tokio::fs;
use tokio::process::Command;
use tokio_stream::Stream;
use tokio_stream::StreamExt;

/// Cursor adapter for executing instructions using Cursor Agent CLI.
///
/// This adapter spawns the `cursor-agent` CLI as a subprocess and parses its
/// NDJSON (Newline-Delimited JSON) output to create a stream of AgentEvents.
pub struct CursorAdapter {
    #[allow(dead_code)]
    name: String,
    model: String,
    system_prompt: String,
    /// Session mapping: project_id -> session_id
    session_mapping: Arc<Mutex<HashMap<String, String>>>,
}

impl CursorAdapter {
    /// Create a new Cursor adapter.
    pub fn new(name: String, model: String, system_prompt: String) -> Result<Self, AgentError> {
        Ok(Self {
            name,
            model,
            system_prompt,
            session_mapping: Arc::new(Mutex::new(HashMap::new())),
        })
    }

    /// Extract project ID from project path.
    fn extract_project_id(project_path: &str) -> String {
        std::path::Path::new(project_path)
            .file_name()
            .and_then(|s| s.to_str())
            .unwrap_or(project_path)
            .to_string()
    }

    /// Ensure AGENTS.md file exists in the project root.
    ///
    /// Cursor uses AGENTS.md for system prompts.
    async fn ensure_agent_md(&self, project_path: &str) -> Result<(), AgentError> {
        let agent_md_path = std::path::Path::new(project_path).join("AGENTS.md");

        // Skip if already exists
        if agent_md_path.exists() {
            return Ok(());
        }

        // Write system prompt to AGENTS.md
        fs::write(&agent_md_path, &self.system_prompt)
            .await
            .map_err(|e| AgentError::ExecutionError(format!("Failed to write AGENTS.md: {}", e)))?;

        Ok(())
    }
}

#[async_trait]
impl Agent for CursorAdapter {
    async fn check_availability(&self) -> bool {
        // Check if cursor-agent CLI is installed
        match Command::new("cursor-agent")
            .arg("-h")
            .stdout(Stdio::null())
            .stderr(Stdio::null())
            .status()
            .await
        {
            Ok(status) => status.success(),
            Err(_) => false,
        }
    }

    async fn execute(
        &self,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        // 1. Ensure AGENTS.md exists
        self.ensure_agent_md(&context.project_path).await?;

        // 2. Get session ID
        let project_id = Self::extract_project_id(&context.project_path);
        let session_id = {
            let mapping = self.session_mapping.lock().unwrap();
            mapping.get(&project_id).cloned()
        };

        // 3. Build command arguments
        let mut args = vec![
            "--force".to_string(),
            "-p".to_string(),
            context.instruction.clone(),
            "--output-format".to_string(),
            "stream-json".to_string(),
            "-m".to_string(),
            self.model.clone(),
        ];

        // Session resumption
        if let Some(sid) = &session_id {
            args.push("--resume".to_string());
            args.push(sid.clone());
        }

        // API key from environment
        if let Ok(api_key) = std::env::var("CURSOR_API_KEY") {
            args.push("--api-key".to_string());
            args.push(api_key);
        }

        // 4. Execute CLI using CliExecutor
        let json_stream = CliExecutor::execute(
            "cursor-agent".to_string(),
            args,
            context.project_path.clone(),
        );

        // 5. Convert JSON stream to AgentEvents
        let session_mapping = self.session_mapping.clone();
        let project_id_clone = project_id.clone();

        let events_stream = json_stream
            .then(move |json_result| {
                let session_mapping = session_mapping.clone();
                let project_id = project_id_clone.clone();

                async move {
                    match json_result {
                        Ok(json_value) => {
                            // Parse as CursorEvent
                            match serde_json::from_value::<CursorEvent>(json_value.clone()) {
                                Ok(event) => {
                                    convert_cursor_event(event, session_mapping, project_id).await
                                }
                                Err(e) => Some(Err(AgentError::StreamParseError(format!(
                                    "Failed to parse CursorEvent: {} (json: {})",
                                    e, json_value
                                )))),
                            }
                        }
                        Err(e) => Some(Err(e)),
                    }
                }
            })
            .filter_map(|opt| opt);

        Ok(Box::pin(events_stream))
    }
}

/// Cursor NDJSON event structure.
#[derive(Debug, Deserialize)]
struct CursorEvent {
    #[serde(rename = "type")]
    event_type: String,
    subtype: Option<String>,
    message: Option<serde_json::Value>,
    tool_call: Option<serde_json::Value>,
    session_id: Option<String>,
    #[allow(dead_code)]
    duration_ms: Option<u64>,
}

/// Convert Cursor event to AgentEvent.
async fn convert_cursor_event(
    event: CursorEvent,
    session_mapping: Arc<Mutex<HashMap<String, String>>>,
    project_id: String,
) -> Option<Result<AgentEvent, AgentError>> {
    match event.event_type.as_str() {
        "system" => {
            // System initialization (hidden from UI)
            None
        }
        "user" => {
            // Echo back (suppress)
            None
        }
        "assistant" => {
            // Text delta
            if let Some(msg) = event.message {
                if let Some(content_array) = msg.get("content").and_then(|c| c.as_array()) {
                    for item in content_array {
                        if let Some(text) = item.get("text").and_then(|t| t.as_str()) {
                            if !text.trim().is_empty() {
                                return Some(Ok(AgentEvent::MessageChunk(text.to_string())));
                            }
                        }
                    }
                }
            }
            None
        }
        "tool_call" => {
            if let Some(subtype) = &event.subtype {
                if subtype == "started" {
                    // Tool call started
                    if let Some(tool_call) = event.tool_call {
                        return Some(Ok(AgentEvent::ToolCall(tool_call.to_string())));
                    }
                }
                // "completed" subtype is not shown to UI
            }
            None
        }
        "result" => {
            // Save session ID
            if let Some(sid) = event.session_id {
                let mut mapping = session_mapping.lock().unwrap();
                mapping.insert(project_id, sid);
            }
            // Signal completion
            Some(Ok(AgentEvent::Completed))
        }
        _ => None,
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cursor_adapter_new() {
        let adapter = CursorAdapter::new(
            "test".to_string(),
            "gpt-5".to_string(),
            "test prompt".to_string(),
        );
        assert!(adapter.is_ok());
    }

    #[test]
    fn test_extract_project_id() {
        assert_eq!(
            CursorAdapter::extract_project_id("/path/to/project"),
            "project"
        );
        assert_eq!(CursorAdapter::extract_project_id("/tmp/test"), "test");
    }

    #[tokio::test]
    async fn test_check_availability() {
        let adapter = CursorAdapter::new(
            "test".to_string(),
            "gpt-5".to_string(),
            "test prompt".to_string(),
        )
        .unwrap();

        // This will return false unless cursor-agent CLI is actually installed
        let available = adapter.check_availability().await;
        let _ = available;
    }

    #[tokio::test]
    async fn test_ensure_agent_md() {
        let adapter = CursorAdapter::new(
            "test".to_string(),
            "gpt-5".to_string(),
            "test system prompt".to_string(),
        )
        .unwrap();

        let temp_dir = tempfile::tempdir().unwrap();
        let project_path = temp_dir.path().to_str().unwrap();

        let result = adapter.ensure_agent_md(project_path).await;
        assert!(result.is_ok());

        // Verify file was created
        let agent_md_path = temp_dir.path().join("AGENTS.md");
        assert!(agent_md_path.exists());

        let content = fs::read_to_string(&agent_md_path).await.unwrap();
        assert_eq!(content, "test system prompt");

        // Calling again should not error (idempotent)
        let result2 = adapter.ensure_agent_md(project_path).await;
        assert!(result2.is_ok());
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/adapters/mod.rs">
//! Agent adapter implementations.

mod claude_adapter;
mod codex_adapter;
mod cursor_adapter;
mod gemini_adapter;
pub mod mock_agent;
mod qwen_adapter;

pub use claude_adapter::ClaudeAdapter;
pub use codex_adapter::CodexAdapter;
pub use cursor_adapter::CursorAdapter;
pub use gemini_adapter::GeminiAdapter;
pub use mock_agent::MockAgent;
pub use qwen_adapter::QwenAdapter;
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/manager.rs">
//! Agent manager for orchestrating multiple agents.
//!
//! The `AgentManager` is responsible for:
//! - Registering agent configurations
//! - Looking up agents by name
//! - Providing fallback logic when agents are unavailable
//! - Managing the lifecycle of agent instances

use crate::agents::base::Agent;
use crate::agents::base::AgentError;
use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use crate::agents::factory::AgentFactory;
use pk_protocol::agent_models;
use std::collections::HashMap;
use std::pin::Pin;
use std::sync::Arc;
use tokio_stream::Stream;

/// Manages all registered agents and provides orchestration logic.
///
/// The manager maintains a registry of agent adapters and provides
/// methods to look up agents by name and execute instructions with
/// automatic fallback support.
pub struct AgentManager {
    agents: HashMap<String, Arc<dyn Agent>>,
    fallback_agent_name: Option<String>,
}

impl AgentManager {
    /// Create a new AgentManager with the given agent configurations.
    ///
    /// # Arguments
    ///
    /// * `configs` - Agent configurations from `.pipeline-kit/agents/*.md`
    ///
    /// # Returns
    ///
    /// A new `AgentManager` with MockAgent instances for testing.
    ///
    /// # Note
    ///
    /// Uses `AgentFactory` to create appropriate agent adapters based on
    /// the model name in the configuration.
    pub fn new(configs: Vec<agent_models::Agent>) -> Self {
        let mut agents: HashMap<String, Arc<dyn Agent>> = HashMap::new();

        // Use AgentFactory to create appropriate adapters
        for config in configs {
            match AgentFactory::create(&config) {
                Ok(agent) => {
                    agents.insert(config.name.clone(), agent);
                }
                Err(e) => {
                    eprintln!(
                        "Warning: Failed to create agent '{}': {}. Skipping.",
                        config.name, e
                    );
                }
            }
        }

        Self {
            agents,
            fallback_agent_name: None,
        }
    }

    /// Set the fallback agent to use when the requested agent is unavailable.
    ///
    /// # Arguments
    ///
    /// * `agent_name` - The name of the agent to use as fallback
    pub fn with_fallback(mut self, agent_name: String) -> Self {
        self.fallback_agent_name = Some(agent_name);
        self
    }

    /// Get an agent by name.
    ///
    /// # Arguments
    ///
    /// * `name` - The agent name to look up
    ///
    /// # Returns
    ///
    /// `Some(Arc<dyn Agent>)` if found, `None` otherwise.
    pub fn get_agent(&self, name: &str) -> Option<Arc<dyn Agent>> {
        self.agents.get(name).cloned()
    }

    /// Execute an instruction with the specified agent.
    ///
    /// This method handles agent lookup and automatic fallback if the
    /// requested agent is unavailable.
    ///
    /// # Arguments
    ///
    /// * `agent_name` - The name of the agent to use
    /// * `context` - The execution context
    ///
    /// # Returns
    ///
    /// A stream of agent events, or an error if no suitable agent is found.
    ///
    /// # Behavior
    ///
    /// 1. Look up the requested agent
    /// 2. Check if it's available
    /// 3. If unavailable and fallback is configured, try fallback agent
    /// 4. Execute with the selected agent
    pub async fn execute(
        &self,
        agent_name: &str,
        context: &ExecutionContext,
    ) -> Result<Pin<Box<dyn Stream<Item = Result<AgentEvent, AgentError>> + Send>>, AgentError>
    {
        // Try to get the requested agent
        if let Some(agent) = self.get_agent(agent_name) {
            if agent.check_availability().await {
                return agent.execute(context).await;
            }

            // Agent exists but is not available - try fallback
            if let Some(ref fallback_name) = self.fallback_agent_name {
                if fallback_name != agent_name {
                    if let Some(fallback_agent) = self.get_agent(fallback_name) {
                        if fallback_agent.check_availability().await {
                            return fallback_agent.execute(context).await;
                        }
                    }
                }
            }

            return Err(AgentError::NotAvailable(format!(
                "Agent '{}' is not available and no fallback succeeded",
                agent_name
            )));
        }

        Err(AgentError::NotAvailable(format!(
            "Agent '{}' not found in registry",
            agent_name
        )))
    }

    /// List all registered agent names.
    pub fn list_agents(&self) -> Vec<String> {
        self.agents.keys().cloned().collect()
    }

    /// Check if an agent with the given name is registered.
    pub fn has_agent(&self, name: &str) -> bool {
        self.agents.contains_key(name)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use pk_protocol::agent_models::Agent as AgentConfig;
    use tokio_stream::StreamExt;

    fn create_test_config(name: &str) -> AgentConfig {
        AgentConfig {
            name: name.to_string(),
            description: format!("Test agent {}", name),
            model: "test-model".to_string(),
            color: "blue".to_string(),
            system_prompt: "Test prompt".to_string(),
        }
    }

    #[test]
    fn test_agent_manager_new() {
        let configs = vec![create_test_config("agent1"), create_test_config("agent2")];

        let manager = AgentManager::new(configs);
        assert!(manager.has_agent("agent1"));
        assert!(manager.has_agent("agent2"));
        assert!(!manager.has_agent("agent3"));
    }

    #[test]
    fn test_agent_manager_get_agent() {
        let configs = vec![create_test_config("test-agent")];
        let manager = AgentManager::new(configs);

        let agent = manager.get_agent("test-agent");
        assert!(agent.is_some());

        let nonexistent = manager.get_agent("nonexistent");
        assert!(nonexistent.is_none());
    }

    #[test]
    fn test_agent_manager_list_agents() {
        let configs = vec![
            create_test_config("agent1"),
            create_test_config("agent2"),
            create_test_config("agent3"),
        ];

        let manager = AgentManager::new(configs);
        let agents = manager.list_agents();

        assert_eq!(agents.len(), 3);
        assert!(agents.contains(&"agent1".to_string()));
        assert!(agents.contains(&"agent2".to_string()));
        assert!(agents.contains(&"agent3".to_string()));
    }

    #[tokio::test]
    async fn test_agent_manager_execute_success() {
        let configs = vec![create_test_config("test-agent")];
        let manager = AgentManager::new(configs);

        let context = ExecutionContext::new("test instruction".to_string());

        let stream = manager.execute("test-agent", &context).await.unwrap();
        let events: Vec<_> = stream.collect().await;

        // MockAgent::success() returns 3 events
        assert_eq!(events.len(), 3);
        assert!(matches!(events[0], Ok(AgentEvent::Thought(_))));
        assert_eq!(events[2], Ok(AgentEvent::Completed));
    }

    #[tokio::test]
    async fn test_agent_manager_execute_not_found() {
        let configs = vec![create_test_config("test-agent")];
        let manager = AgentManager::new(configs);

        let context = ExecutionContext::new("test instruction".to_string());

        let result = manager.execute("nonexistent", &context).await;
        assert!(result.is_err());
        if let Err(e) = result {
            assert!(matches!(e, AgentError::NotAvailable(_)));
        }
    }

    #[tokio::test]
    async fn test_agent_manager_with_fallback() {
        let configs = vec![
            create_test_config("primary"),
            create_test_config("fallback"),
        ];

        let manager = AgentManager::new(configs).with_fallback("fallback".to_string());

        let context = ExecutionContext::new("test instruction".to_string());

        // Primary agent should work
        let stream = manager.execute("primary", &context).await.unwrap();
        let events: Vec<_> = stream.collect().await;
        assert_eq!(events.len(), 3);
    }

    #[test]
    fn test_agent_manager_fallback_configuration() {
        let configs = vec![create_test_config("agent1"), create_test_config("agent2")];

        let manager = AgentManager::new(configs).with_fallback("agent2".to_string());
        assert_eq!(manager.fallback_agent_name, Some("agent2".to_string()));
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/state/process.rs">
//! Process state machine implementation.
//!
//! This module provides functions for managing the lifecycle of a Process,
//! including state transitions and event emission.

use pk_protocol::ipc::Event;
use pk_protocol::process_models::Process;
use pk_protocol::process_models::ProcessStatus;
use std::sync::Arc;
use tokio::sync::mpsc::Sender;
use tokio::sync::Notify;
use uuid::Uuid;

/// Create a new Process with Pending status.
///
/// # Arguments
///
/// * `pipeline_name` - The name of the pipeline to execute
///
/// # Returns
///
/// A new Process instance with a unique ID and Pending status.
pub fn create_process(pipeline_name: String) -> Process {
    Process {
        id: Uuid::new_v4(),
        pipeline_name,
        status: ProcessStatus::Pending,
        current_step_index: 0,
        started_at: chrono::Utc::now(),
        completed_at: None,
        logs: Vec::new(),
        resume_notifier: Arc::new(Notify::new()),
    }
}

/// Transition the process to Running status and emit event.
///
/// # Arguments
///
/// * `process` - The process to start
/// * `events_tx` - Channel to send status update events
pub async fn start_process(process: &mut Process, events_tx: &Sender<Event>) {
    process.status = ProcessStatus::Running;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;
}

/// Transition to HumanReview status and emit event.
///
/// This is called when a HUMAN_REVIEW step is encountered.
///
/// # Arguments
///
/// * `process` - The process to pause
/// * `events_tx` - Channel to send status update events
pub async fn pause_for_human_review(process: &mut Process, events_tx: &Sender<Event>) {
    process.status = ProcessStatus::HumanReview;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;
}

/// Transition to Paused status and emit event.
///
/// This is called when the user manually pauses the process.
///
/// # Arguments
///
/// * `process` - The process to pause
/// * `events_tx` - Channel to send status update events
pub async fn pause_process(process: &mut Process, events_tx: &Sender<Event>) {
    process.status = ProcessStatus::Paused;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;
}

/// Resume from Paused or HumanReview status to Running.
///
/// This function transitions the process back to Running status,
/// emits the appropriate events, and signals the resume_notifier
/// to wake up the waiting PipelineEngine task.
///
/// # Arguments
///
/// * `process` - The process to resume
/// * `events_tx` - Channel to send status update events
pub async fn resume_process(process: &mut Process, events_tx: &Sender<Event>) {
    process.status = ProcessStatus::Running;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;

    // Emit ProcessResumed event
    let _ = events_tx
        .send(Event::ProcessResumed {
            process_id: process.id,
        })
        .await;

    // Signal the resume notifier to wake up the waiting PipelineEngine
    process.resume_notifier.notify_one();
}

/// Mark the process as completed and emit event.
///
/// # Arguments
///
/// * `process` - The process to complete
/// * `events_tx` - Channel to send completion event
pub async fn complete_process(process: &mut Process, events_tx: &Sender<Event>) {
    process.status = ProcessStatus::Completed;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;
    let _ = events_tx
        .send(Event::ProcessCompleted {
            process_id: process.id,
        })
        .await;
}

/// Mark the process as failed and emit error event.
///
/// # Arguments
///
/// * `process` - The process to fail
/// * `events_tx` - Channel to send error event
/// * `error` - Error message describing the failure
pub async fn fail_process(process: &mut Process, events_tx: &Sender<Event>, error: String) {
    process.status = ProcessStatus::Failed;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;
    let _ = events_tx
        .send(Event::ProcessError {
            process_id: process.id,
            error,
        })
        .await;
}

/// Append a log message to the process logs and emit event.
///
/// # Arguments
///
/// * `process` - The process to log to
/// * `events_tx` - Channel to send log chunk event
/// * `message` - Log message to append
pub async fn log_to_process(process: &mut Process, events_tx: &Sender<Event>, message: String) {
    process.logs.push(message.clone());
    let _ = events_tx
        .send(Event::ProcessLogChunk {
            process_id: process.id,
            content: message,
        })
        .await;
}

/// Move to the next step in the pipeline.
///
/// # Arguments
///
/// * `process` - The process to advance
pub fn advance_step(process: &mut Process) {
    process.current_step_index += 1;
}

/// Mark the process as killed and emit event.
///
/// # Arguments
///
/// * `process` - The process to kill
/// * `events_tx` - Channel to send killed event
pub async fn kill_process_state(process: &mut Process, events_tx: &Sender<Event>) {
    process.status = ProcessStatus::Killed;
    let _ = events_tx
        .send(Event::ProcessStatusUpdate {
            process_id: process.id,
            status: process.status,
            step_index: process.current_step_index,
        })
        .await;
    let _ = events_tx
        .send(Event::ProcessKilled {
            process_id: process.id,
        })
        .await;
}

#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc;

    #[tokio::test]
    async fn test_create_process() {
        let process = create_process("test-pipeline".to_string());
        assert_eq!(process.pipeline_name, "test-pipeline");
        assert_eq!(process.status, ProcessStatus::Pending);
        assert_eq!(process.current_step_index, 0);
        assert!(process.logs.is_empty());
    }

    #[tokio::test]
    async fn test_start_process() {
        let mut process = create_process("test-pipeline".to_string());
        let (tx, mut rx) = mpsc::channel(10);

        start_process(&mut process, &tx).await;

        assert_eq!(process.status, ProcessStatus::Running);

        let event = rx.recv().await.unwrap();
        assert!(matches!(
            event,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::Running,
                step_index: 0,
                ..
            }
        ));
    }

    #[tokio::test]
    async fn test_pause_for_human_review() {
        let mut process = create_process("test-pipeline".to_string());
        let (tx, mut rx) = mpsc::channel(10);

        pause_for_human_review(&mut process, &tx).await;

        assert_eq!(process.status, ProcessStatus::HumanReview);

        let event = rx.recv().await.unwrap();
        assert!(matches!(
            event,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::HumanReview,
                ..
            }
        ));
    }

    #[tokio::test]
    async fn test_complete_process() {
        let mut process = create_process("test-pipeline".to_string());
        let (tx, mut rx) = mpsc::channel(10);

        complete_process(&mut process, &tx).await;

        assert_eq!(process.status, ProcessStatus::Completed);

        // Should receive two events: StatusUpdate and Completed
        let event1 = rx.recv().await.unwrap();
        assert!(matches!(
            event1,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::Completed,
                ..
            }
        ));

        let event2 = rx.recv().await.unwrap();
        assert!(matches!(event2, Event::ProcessCompleted { .. }));
    }

    #[tokio::test]
    async fn test_fail_process() {
        let mut process = create_process("test-pipeline".to_string());
        let (tx, mut rx) = mpsc::channel(10);

        fail_process(&mut process, &tx, "Test error".to_string()).await;

        assert_eq!(process.status, ProcessStatus::Failed);

        // Should receive two events: StatusUpdate and Error
        let event1 = rx.recv().await.unwrap();
        assert!(matches!(
            event1,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::Failed,
                ..
            }
        ));

        let event2 = rx.recv().await.unwrap();
        assert!(matches!(event2, Event::ProcessError { error, .. } if error == "Test error"));
    }

    #[tokio::test]
    async fn test_log_to_process() {
        let mut process = create_process("test-pipeline".to_string());
        let (tx, mut rx) = mpsc::channel(10);

        log_to_process(&mut process, &tx, "Test log message".to_string()).await;

        assert_eq!(process.logs.len(), 1);
        assert_eq!(process.logs[0], "Test log message");

        let event = rx.recv().await.unwrap();
        assert!(matches!(
            event,
            Event::ProcessLogChunk { content, .. } if content == "Test log message"
        ));
    }

    #[tokio::test]
    async fn test_advance_step() {
        let mut process = create_process("test-pipeline".to_string());
        assert_eq!(process.current_step_index, 0);

        advance_step(&mut process);
        assert_eq!(process.current_step_index, 1);

        advance_step(&mut process);
        assert_eq!(process.current_step_index, 2);
    }

    #[tokio::test]
    async fn test_resume_process() {
        let mut process = create_process("test-pipeline".to_string());
        let (tx, mut rx) = mpsc::channel(10);

        // First pause
        pause_process(&mut process, &tx).await;
        assert_eq!(process.status, ProcessStatus::Paused);
        let _ = rx.recv().await;

        // Then resume
        resume_process(&mut process, &tx).await;
        assert_eq!(process.status, ProcessStatus::Running);

        let event = rx.recv().await.unwrap();
        assert!(matches!(
            event,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::Running,
                ..
            }
        ));
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/lib.rs">
//! # pk-core
//!
//! Core pipeline engine and agent management for pipeline-kit.
//!
//! This crate provides:
//! - Configuration loading from `.pipeline-kit/` directory
//! - Agent abstraction layer and adapter pattern implementation
//! - Pipeline execution engine
//! - State management for running processes
//! - Initialization utilities for creating `.pipeline-kit/` structures
//!
//! ## Modules
//!
//! - [`config`]: Configuration loading and management
//! - [`agents`]: Agent trait and adapter implementations
//! - [`engine`]: Pipeline execution engine
//! - [`state`]: Process state management
//! - [`init`]: Initialization utilities for new projects

pub mod agents;
pub mod config;
pub mod engine;
pub mod init;
pub mod state;
</file>

<file path="pipeline-kit-rs/crates/core/tests/e2e_pipeline.rs">
//! E2E tests for pipeline execution.
//!
//! These tests verify end-to-end pipeline execution flows including:
//! - Starting pipelines
//! - Sequential agent execution
//! - HUMAN_REVIEW pausing/resuming
//! - Process completion and failure
//! - Error handling and recovery
//!
//! ## Test Priority (Phase 2.1-A: Core Happy Path)
//!
//! Priority 1 (Essential):
//! - test_single_agent_complete_execution (1.1.1)
//! - test_multi_agent_sequential_execution (1.1.2)
//! - test_human_review_pause_and_resume (1.2.1 + 1.2.2)
//! - test_agent_failure_stops_pipeline (2.1.1)
//! - test_event_sequence_validation (1.3.1)

mod common;

use common::assertions::*;
use common::fixtures::*;
use pk_core::agents::manager::AgentManager;
use pk_core::engine::PipelineEngine;
use pk_core::state::manager::StateManager;
use pk_protocol::ipc::Event;
use pk_protocol::pipeline_models::ProcessStep;
use pk_protocol::process_models::ProcessStatus;
use std::time::Duration;
use tokio::sync::mpsc;

/// Helper function to collect events from a channel until timeout or completion.
async fn collect_events_until_timeout(
    rx: &mut mpsc::Receiver<Event>,
    timeout: Duration,
) -> Vec<Event> {
    let mut events = Vec::new();
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        match tokio::time::timeout(Duration::from_millis(100), rx.recv()).await {
            Ok(Some(event)) => {
                let is_terminal = matches!(
                    &event,
                    Event::ProcessCompleted { .. }
                        | Event::ProcessKilled { .. }
                        | Event::ProcessError { .. }
                );
                events.push(event);
                if is_terminal {
                    break;
                }
            }
            Ok(None) => break,  // Channel closed
            Err(_) => continue, // Timeout, keep waiting
        }
    }

    events
}

/// Priority 1.1.1: Single agent complete execution
///
/// Acceptance criteria:
/// 1. Process lifecycle: Pending ‚Üí Running ‚Üí Completed
/// 2. Events emitted: ProcessStarted ‚Üí StatusUpdate(Running) ‚Üí LogChunk ‚Üí ProcessCompleted
/// 3. Logs are captured in process state
/// 4. Timestamps are set correctly
#[tokio::test]
async fn test_single_agent_complete_execution() {
    // Given: A pipeline with a single mock success agent
    let agents = vec![create_test_agent("agent-1")];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![ProcessStep::Agent("agent-1".to_string())];
    let pipeline = create_test_pipeline("single-agent-pipeline", steps);

    // When: Execute the pipeline via PipelineEngine
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let engine = PipelineEngine::new(agent_manager);

    let process = create_test_process("single-agent-pipeline", ProcessStatus::Pending);
    let process_id = process.id;

    tokio::spawn(async move {
        let _ = engine.run(&pipeline, process, events_tx).await;
    });

    // Then: Collect and verify events
    let events = collect_events_until_timeout(&mut events_rx, Duration::from_secs(5)).await;

    println!(
        "üìä Received {} events for single agent execution",
        events.len()
    );
    for (i, event) in events.iter().enumerate() {
        println!("  Event {}: {:?}", i + 1, event);
    }

    // Assertions
    assert!(
        !events.is_empty(),
        "Should receive events from pipeline execution"
    );

    assert!(
        assert_has_process_started(&events),
        "Should have ProcessStarted event"
    );

    assert!(
        assert_has_status_update(&events, ProcessStatus::Running),
        "Should transition to Running status"
    );

    assert!(
        assert_has_process_completed(&events),
        "Should complete successfully"
    );

    assert!(
        assert_has_log_chunks(&events),
        "Should have log chunks from agent execution"
    );

    // Verify event sequence
    assert_event_sequence(&events);

    // Verify process ID consistency
    let extracted_id = extract_process_id(&events).expect("Should have process ID");
    assert_eq!(extracted_id, process_id, "Process ID should be consistent");
}

/// Priority 1.1.2: Multi-agent sequential execution
///
/// Acceptance criteria:
/// 1. Agents execute in the correct order
/// 2. Each agent produces logs in sequence
/// 3. current_step_index advances correctly
/// 4. All agents complete before final completion
#[tokio::test]
async fn test_multi_agent_sequential_execution() {
    // Given: A pipeline with 3 agents
    let agents = vec![
        create_test_agent("agent-1"),
        create_test_agent("agent-2"),
        create_test_agent("agent-3"),
    ];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::Agent("agent-2".to_string()),
        ProcessStep::Agent("agent-3".to_string()),
    ];
    let pipeline = create_test_pipeline("multi-agent-pipeline", steps);

    // When: Execute the pipeline
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let engine = PipelineEngine::new(agent_manager);

    let process = create_test_process("multi-agent-pipeline", ProcessStatus::Pending);

    tokio::spawn(async move {
        let _ = engine.run(&pipeline, process, events_tx).await;
    });

    // Then: Collect and verify events
    let events = collect_events_until_timeout(&mut events_rx, Duration::from_secs(5)).await;

    println!(
        "üìä Received {} events for multi-agent execution",
        events.len()
    );

    // Should have multiple log chunks (one per agent minimum)
    let log_count = count_log_chunks(&events);
    assert!(
        log_count >= 3,
        "Should have at least 3 log chunks (one per agent), got {}",
        log_count
    );

    // Verify completion
    assert!(
        assert_has_process_completed(&events),
        "Should complete after all agents finish"
    );

    // Verify event sequence
    assert_event_sequence(&events);
}

/// Priority 1.2.1 + 1.2.2: HUMAN_REVIEW pause and resume
///
/// Acceptance criteria:
/// 1. Pipeline pauses at HUMAN_REVIEW with StatusUpdate(HumanReview)
/// 2. Execution blocks waiting for resume signal
/// 3. StateManager.resume_process_by_id() triggers continuation
/// 4. "Resumed from human review" log appears
/// 5. Pipeline completes after resume
#[tokio::test]
async fn test_human_review_pause_and_resume() {
    // Given: A pipeline with HUMAN_REVIEW between agents
    let agents = vec![create_test_agent("agent-1"), create_test_agent("agent-2")];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
        ProcessStep::Agent("agent-2".to_string()),
    ];
    let pipeline = create_test_pipeline("review-pipeline", steps);

    // When: Start pipeline via StateManager
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let state_manager = StateManager::new(agent_manager, events_tx);

    let process_id = state_manager.start_pipeline(pipeline).await;

    // Wait for HUMAN_REVIEW status
    let mut human_review_reached = false;
    let timeout = Duration::from_secs(3);

    let mut collected_events = Vec::new();
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            let is_human_review = matches!(
                &event,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::HumanReview,
                    ..
                }
            );
            collected_events.push(event);

            if is_human_review {
                human_review_reached = true;
                break;
            }
        }
    }

    println!("üìä Events before resume: {} events", collected_events.len());
    for (i, event) in collected_events.iter().enumerate() {
        println!("  Event {}: {:?}", i + 1, event);
    }

    assert!(
        human_review_reached,
        "Pipeline should pause at HUMAN_REVIEW"
    );

    // Then: Resume the process
    let resume_result = state_manager.resume_process_by_id(process_id).await;
    assert!(resume_result.is_ok(), "Resume should succeed");

    // Wait for completion
    let mut completed = false;
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            let is_completed = matches!(&event, Event::ProcessCompleted { .. });
            collected_events.push(event);

            if is_completed {
                completed = true;
                break;
            }
        }
    }

    println!(
        "üìä Total events after resume: {} events",
        collected_events.len()
    );

    assert!(completed, "Pipeline should complete after resume");

    // Verify we got ProcessResumed event
    let has_resumed = collected_events
        .iter()
        .any(|e| matches!(e, Event::ProcessResumed { .. }));
    assert!(has_resumed, "Should emit ProcessResumed event");

    // Verify final process state
    let final_process = state_manager.get_process(process_id).await;
    assert!(final_process.is_some(), "Process should exist");
    assert_eq!(
        final_process.unwrap().status,
        ProcessStatus::Completed,
        "Process should be completed"
    );
}

/// Priority 2.1.1: Agent failure stops pipeline
///
/// Acceptance criteria:
/// 1. When an agent fails, StatusUpdate(Failed) is emitted
/// 2. Error message is logged in ProcessError event
/// 3. Pipeline execution stops immediately
/// 4. Subsequent agents are NOT executed
#[tokio::test]
async fn test_agent_failure_stops_pipeline() {
    // Given: A pipeline where the second agent fails
    let agents = vec![
        create_test_agent("agent-1"),
        create_failure_agent("failing-agent"),
        create_test_agent("agent-3"),
    ];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::Agent("failing-agent".to_string()),
        ProcessStep::Agent("agent-3".to_string()),
    ];
    let pipeline = create_test_pipeline("failing-pipeline", steps);

    // When: Execute the pipeline
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let engine = PipelineEngine::new(agent_manager);

    let process = create_test_process("failing-pipeline", ProcessStatus::Pending);

    tokio::spawn(async move {
        let _ = engine.run(&pipeline, process, events_tx).await;
    });

    // Then: Collect and verify events
    let events = collect_events_until_timeout(&mut events_rx, Duration::from_secs(5)).await;

    println!("üìä Received {} events for failing pipeline", events.len());
    for (i, event) in events.iter().enumerate() {
        println!("  Event {}: {:?}", i + 1, event);
    }

    // Should transition to Failed status
    assert!(
        assert_has_status_update(&events, ProcessStatus::Failed),
        "Should transition to Failed status"
    );

    // Should have ProcessError event
    let has_error = events
        .iter()
        .any(|e| matches!(e, Event::ProcessError { .. }));
    assert!(has_error, "Should emit ProcessError event");

    // Should NOT have ProcessCompleted
    assert!(
        !assert_has_process_completed(&events),
        "Should NOT complete successfully"
    );

    // Verify that agent-3 was NOT executed by checking logs
    let logs: Vec<String> = events
        .iter()
        .filter_map(|e| match e {
            Event::ProcessLogChunk { content, .. } => Some(content.clone()),
            _ => None,
        })
        .collect();

    let has_agent3_log = logs.iter().any(|log| log.contains("agent-3"));
    assert!(
        !has_agent3_log,
        "Agent-3 should NOT be executed after failure"
    );
}

/// Priority 1.3.1: Event sequence validation
///
/// Acceptance criteria:
/// 1. ProcessStarted is ALWAYS the first event
/// 2. StatusUpdate(Running) comes before log chunks
/// 3. ProcessCompleted is the last event (for successful execution)
/// 4. No duplicate events
/// 5. Process state fields are correctly populated
#[tokio::test]
async fn test_event_sequence_validation() {
    // Given: A simple pipeline
    let agents = vec![create_test_agent("agent-1")];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![ProcessStep::Agent("agent-1".to_string())];
    let pipeline = create_test_pipeline("sequence-test", steps);

    // When: Execute the pipeline
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let engine = PipelineEngine::new(agent_manager);

    let process = create_test_process("sequence-test", ProcessStatus::Pending);
    let process_id = process.id;

    tokio::spawn(async move {
        let _ = engine.run(&pipeline, process, events_tx).await;
    });

    // Then: Collect and verify events
    let events = collect_events_until_timeout(&mut events_rx, Duration::from_secs(5)).await;

    println!("üìä Event sequence validation: {} events", events.len());
    for (i, event) in events.iter().enumerate() {
        println!("  [{:02}] {:?}", i + 1, event);
    }

    // Rule 1: ProcessStarted is first
    assert!(
        matches!(events.first(), Some(Event::ProcessStarted { .. })),
        "First event must be ProcessStarted"
    );

    // Rule 2: Find indices of key events
    let started_idx = events
        .iter()
        .position(|e| matches!(e, Event::ProcessStarted { .. }))
        .expect("Should have ProcessStarted");

    let running_idx = events
        .iter()
        .position(|e| {
            matches!(
                e,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::Running,
                    ..
                }
            )
        })
        .expect("Should have Running status");

    let first_log_idx = events
        .iter()
        .position(|e| matches!(e, Event::ProcessLogChunk { .. }));

    let completed_idx = events
        .iter()
        .position(|e| matches!(e, Event::ProcessCompleted { .. }))
        .expect("Should have ProcessCompleted");

    // Rule 3: Verify ordering
    assert!(
        started_idx < running_idx,
        "ProcessStarted should come before Running status"
    );

    if let Some(log_idx) = first_log_idx {
        assert!(
            running_idx < log_idx,
            "Running status should come before log chunks"
        );
        assert!(
            log_idx < completed_idx,
            "Log chunks should come before ProcessCompleted"
        );
    }

    // Rule 4: Last event is ProcessCompleted
    assert!(
        matches!(events.last(), Some(Event::ProcessCompleted { .. })),
        "Last event must be ProcessCompleted"
    );

    // Rule 5: Process ID consistency
    for event in &events {
        match event {
            Event::ProcessStarted { process_id: id, .. }
            | Event::ProcessStatusUpdate { process_id: id, .. }
            | Event::ProcessLogChunk { process_id: id, .. }
            | Event::ProcessCompleted { process_id: id } => {
                assert_eq!(
                    *id, process_id,
                    "All events should have the same process_id"
                );
            }
            _ => {}
        }
    }

    println!("‚úÖ Event sequence validation passed");
}

/// Additional test: Empty pipeline (edge case)
///
/// Tests that a pipeline with no steps completes immediately.
#[tokio::test]
async fn test_empty_pipeline_completes_immediately() {
    // Given: An empty pipeline
    let agents = vec![];
    let agent_manager = AgentManager::new(agents);

    let steps: Vec<ProcessStep> = vec![];
    let pipeline = create_test_pipeline("empty-pipeline", steps);

    // When: Execute the pipeline
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let engine = PipelineEngine::new(agent_manager);

    let process = create_test_process("empty-pipeline", ProcessStatus::Pending);

    tokio::spawn(async move {
        let _ = engine.run(&pipeline, process, events_tx).await;
    });

    // Then: Should complete immediately
    let events = collect_events_until_timeout(&mut events_rx, Duration::from_secs(2)).await;

    println!("üìä Empty pipeline events: {} events", events.len());

    assert!(
        assert_has_process_started(&events),
        "Should start even with no steps"
    );

    assert!(
        assert_has_process_completed(&events),
        "Should complete immediately"
    );

    // Should have minimal events: Started, StatusUpdate(Running), StatusUpdate(Completed), Completed
    assert!(
        events.len() >= 3,
        "Should have at least 3 events (Started, Running, Completed)"
    );
}

/// Additional test: Multiple HUMAN_REVIEW steps
///
/// Tests sequential pause-resume cycles.
#[tokio::test]
async fn test_multiple_human_review_steps() {
    // Given: A pipeline with multiple HUMAN_REVIEW steps
    let agents = vec![
        create_test_agent("agent-1"),
        create_test_agent("agent-2"),
        create_test_agent("agent-3"),
    ];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
        ProcessStep::Agent("agent-2".to_string()),
        ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
        ProcessStep::Agent("agent-3".to_string()),
    ];
    let pipeline = create_test_pipeline("multi-review-pipeline", steps);

    // When: Start pipeline
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let state_manager = StateManager::new(agent_manager, events_tx);

    let process_id = state_manager.start_pipeline(pipeline).await;

    // Wait for first HUMAN_REVIEW
    let mut first_review_reached = false;
    let timeout = Duration::from_secs(2);
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            if matches!(
                &event,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::HumanReview,
                    ..
                }
            ) {
                first_review_reached = true;
                break;
            }
        }
    }

    assert!(first_review_reached, "Should reach first HUMAN_REVIEW");

    // Resume first time
    state_manager
        .resume_process_by_id(process_id)
        .await
        .unwrap();

    // Wait for second HUMAN_REVIEW
    let mut second_review_reached = false;
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            if matches!(
                &event,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::HumanReview,
                    ..
                }
            ) {
                second_review_reached = true;
                break;
            }
        }
    }

    assert!(second_review_reached, "Should reach second HUMAN_REVIEW");

    // Resume second time
    state_manager
        .resume_process_by_id(process_id)
        .await
        .unwrap();

    // Wait for completion
    let mut completed = false;
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            if matches!(&event, Event::ProcessCompleted { .. }) {
                completed = true;
                break;
            }
        }
    }

    assert!(completed, "Should complete after second resume");

    println!("‚úÖ Multiple HUMAN_REVIEW test passed");
}

/// Priority 2.2.1: Nonexistent agent fails pipeline
///
/// Acceptance criteria:
/// 1. Pipeline with undefined agent name fails
/// 2. AgentManager returns appropriate error
/// 3. ProcessError event is emitted
/// 4. Process status transitions to Failed
/// 5. Error message indicates agent not found
#[tokio::test]
async fn test_nonexistent_agent_fails_pipeline() {
    // Given: A pipeline referencing a non-existent agent
    // Only register agent-1, but pipeline requests agent-2
    let agents = vec![create_test_agent("agent-1")];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::Agent("nonexistent-agent".to_string()), // This doesn't exist
        ProcessStep::Agent("agent-3".to_string()),
    ];
    let pipeline = create_test_pipeline("missing-agent-pipeline", steps);

    // When: Execute the pipeline
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let engine = PipelineEngine::new(agent_manager);

    let process = create_test_process("missing-agent-pipeline", ProcessStatus::Pending);

    tokio::spawn(async move {
        let _ = engine.run(&pipeline, process, events_tx).await;
    });

    // Then: Collect and verify events
    let events = collect_events_until_timeout(&mut events_rx, Duration::from_secs(5)).await;

    println!(
        "üìä Received {} events for missing agent pipeline",
        events.len()
    );
    for (i, event) in events.iter().enumerate() {
        println!("  Event {}: {:?}", i + 1, event);
    }

    // Should have ProcessStarted
    assert!(
        assert_has_process_started(&events),
        "Should start the pipeline"
    );

    // Should transition to Failed status
    assert!(
        assert_has_status_update(&events, ProcessStatus::Failed),
        "Should transition to Failed status when agent not found"
    );

    // Should have ProcessError event with agent not found message
    let error_events: Vec<String> = events
        .iter()
        .filter_map(|e| match e {
            Event::ProcessError { error, .. } => Some(error.clone()),
            _ => None,
        })
        .collect();

    assert!(!error_events.is_empty(), "Should emit ProcessError event");

    let error_message = &error_events[0];
    assert!(
        error_message.contains("nonexistent-agent")
            || error_message.contains("not found")
            || error_message.contains("not available"),
        "Error message should indicate agent not found: {}",
        error_message
    );

    // Should NOT complete successfully
    assert!(
        !assert_has_process_completed(&events),
        "Should NOT complete when agent is missing"
    );

    // agent-1 should have executed (before the error)
    let logs: Vec<String> = events
        .iter()
        .filter_map(|e| match e {
            Event::ProcessLogChunk { content, .. } => Some(content.clone()),
            _ => None,
        })
        .collect();

    let has_agent1_log = logs.iter().any(|log| log.contains("agent-1"));
    assert!(has_agent1_log, "Agent-1 should have executed successfully");

    // agent-3 should NOT have been reached
    let has_agent3_log = logs.iter().any(|log| log.contains("agent-3"));
    assert!(
        !has_agent3_log,
        "Agent-3 should NOT execute after missing agent error"
    );

    println!("‚úÖ Nonexistent agent failure test passed");
}

/// Priority 2.3.2: Agent failure after HUMAN_REVIEW
///
/// Acceptance criteria:
/// 1. Pipeline executes successfully until HUMAN_REVIEW
/// 2. After resume, agent failure is handled correctly
/// 3. Logs from pre-review steps are preserved
/// 4. Pipeline stops at failure point
/// 5. Process ends in Failed status
#[tokio::test]
async fn test_agent_failure_after_human_review() {
    // Given: A pipeline with HUMAN_REVIEW followed by a failing agent
    let agents = vec![
        create_test_agent("agent-1"),
        create_test_agent("agent-2"),
        create_failure_agent("failing-agent"),
    ];
    let agent_manager = AgentManager::new(agents);

    let steps = vec![
        ProcessStep::Agent("agent-1".to_string()),
        ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
        ProcessStep::Agent("agent-2".to_string()),
        ProcessStep::Agent("failing-agent".to_string()),
    ];
    let pipeline = create_test_pipeline("review-then-fail-pipeline", steps);

    // When: Start pipeline via StateManager
    let (events_tx, mut events_rx) = mpsc::channel(100);
    let state_manager = StateManager::new(agent_manager, events_tx);

    let process_id = state_manager.start_pipeline(pipeline).await;

    // Wait for HUMAN_REVIEW status
    let mut human_review_reached = false;
    let timeout = Duration::from_secs(3);

    let mut collected_events = Vec::new();
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            let is_human_review = matches!(
                &event,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::HumanReview,
                    ..
                }
            );
            collected_events.push(event);

            if is_human_review {
                human_review_reached = true;
                break;
            }
        }
    }

    assert!(
        human_review_reached,
        "Should reach HUMAN_REVIEW before failure"
    );

    println!("üìä Events before resume: {} events", collected_events.len());

    // Resume and wait for failure
    state_manager
        .resume_process_by_id(process_id)
        .await
        .unwrap();

    // Wait for failure
    let mut failed = false;
    let start = tokio::time::Instant::now();

    while start.elapsed() < timeout {
        if let Ok(Some(event)) =
            tokio::time::timeout(Duration::from_millis(100), events_rx.recv()).await
        {
            let is_failed = matches!(
                &event,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::Failed,
                    ..
                }
            ) || matches!(&event, Event::ProcessError { .. });

            collected_events.push(event);

            if is_failed {
                failed = true;
                // Continue collecting a bit more to get full error events
                tokio::time::sleep(Duration::from_millis(50)).await;
                while let Ok(event) = events_rx.try_recv() {
                    collected_events.push(event);
                }
                break;
            }
        }
    }

    println!(
        "üìä Total events after failure: {} events",
        collected_events.len()
    );
    for (i, event) in collected_events.iter().enumerate() {
        println!("  Event {}: {:?}", i + 1, event);
    }

    assert!(failed, "Pipeline should fail after resume");

    // Verify ProcessResumed event was emitted
    let has_resumed = collected_events
        .iter()
        .any(|e| matches!(e, Event::ProcessResumed { .. }));
    assert!(has_resumed, "Should emit ProcessResumed event");

    // Verify Failed status
    assert!(
        assert_has_status_update(&collected_events, ProcessStatus::Failed),
        "Should transition to Failed status"
    );

    // Verify ProcessError event
    let has_error = collected_events
        .iter()
        .any(|e| matches!(e, Event::ProcessError { .. }));
    assert!(has_error, "Should emit ProcessError event");

    // Should NOT complete successfully
    assert!(
        !assert_has_process_completed(&collected_events),
        "Should NOT complete after failure"
    );

    // Verify logs from before and after HUMAN_REVIEW
    let logs: Vec<String> = collected_events
        .iter()
        .filter_map(|e| match e {
            Event::ProcessLogChunk { content, .. } => Some(content.clone()),
            _ => None,
        })
        .collect();

    println!("üìù Collected logs:");
    for (i, log) in logs.iter().enumerate() {
        println!("  Log {}: {}", i + 1, log);
    }

    // agent-1 logs should be preserved (before HUMAN_REVIEW)
    let has_agent1_log = logs.iter().any(|log| log.contains("agent-1"));
    assert!(
        has_agent1_log,
        "Logs from agent-1 (before HUMAN_REVIEW) should be preserved"
    );

    // agent-2 logs should exist (after resume, before failure)
    let has_agent2_log = logs.iter().any(|log| log.contains("agent-2"));
    assert!(
        has_agent2_log,
        "Agent-2 should execute successfully after resume"
    );

    // failing-agent should have started
    let has_failing_agent_log = logs.iter().any(|log| log.contains("failing-agent"));
    assert!(
        has_failing_agent_log,
        "Failing agent should have been attempted"
    );

    // Verify final process state
    let final_process = state_manager.get_process(process_id).await;
    assert!(final_process.is_some(), "Process should exist");
    assert_eq!(
        final_process.unwrap().status,
        ProcessStatus::Failed,
        "Process should be in Failed state"
    );

    println!("‚úÖ Agent failure after HUMAN_REVIEW test passed");
}
</file>

<file path="pipeline-kit-rs/crates/core/tests/pipeline_engine.rs">
//! Integration tests for PipelineEngine.
//!
//! These tests verify that the PipelineEngine correctly:
//! - Executes pipeline steps sequentially
//! - Emits appropriate events through the channel
//! - Handles HUMAN_REVIEW steps by pausing execution
//! - Manages process state transitions

use pk_core::agents::manager::AgentManager;
use pk_core::engine::PipelineEngine;
use pk_protocol::agent_models::Agent as AgentConfig;
use pk_protocol::ipc::Event;
use pk_protocol::pipeline_models::MasterAgentConfig;
use pk_protocol::pipeline_models::Pipeline;
use pk_protocol::pipeline_models::ProcessStep;
use pk_protocol::process_models::ProcessStatus;
use std::collections::HashMap;
use tokio::sync::mpsc;

fn create_test_agent_config(name: &str) -> AgentConfig {
    AgentConfig {
        name: name.to_string(),
        description: format!("Test agent {}", name),
        model: "test-model".to_string(),
        color: "blue".to_string(),
        system_prompt: "Test prompt".to_string(),
    }
}

fn create_test_pipeline(name: &str, steps: Vec<ProcessStep>) -> Pipeline {
    Pipeline {
        name: name.to_string(),
        required_reference_file: HashMap::new(),
        output_file: HashMap::new(),
        master: MasterAgentConfig {
            model: "test-model".to_string(),
            system_prompt: "Test orchestration".to_string(),
            process: steps,
        },
        sub_agents: vec!["agent1".to_string(), "agent2".to_string()],
    }
}

/// RED: This test should fail because PipelineEngine is not implemented yet.
///
/// Acceptance criteria:
/// 1. ProcessStarted event is emitted when pipeline begins
/// 2. ProcessStatusUpdate(Running) is emitted when execution starts
/// 3. Each agent step executes sequentially
/// 4. ProcessStatusUpdate(HumanReview) is emitted when HUMAN_REVIEW is reached
/// 5. Execution pauses at HUMAN_REVIEW step
#[tokio::test]
async fn test_pipeline_engine_sequential_execution_with_human_review() {
    // Setup: Create a 2-agent pipeline with HUMAN_REVIEW in between
    let agent_configs = vec![
        create_test_agent_config("agent1"),
        create_test_agent_config("agent2"),
    ];

    let agent_manager = AgentManager::new(agent_configs);

    let steps = vec![
        ProcessStep::Agent("agent1".to_string()),
        ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
        ProcessStep::Agent("agent2".to_string()),
    ];

    let pipeline = create_test_pipeline("test-pipeline", steps);

    // Create event channel
    let (events_tx, mut events_rx) = mpsc::channel(100);

    // Execute pipeline
    let engine = PipelineEngine::new(agent_manager);

    // Create initial process
    let process = pk_protocol::Process {
        id: uuid::Uuid::new_v4(),
        pipeline_name: pipeline.name.clone(),
        status: ProcessStatus::Pending,
        current_step_index: 0,
        logs: Vec::new(),
        started_at: chrono::Utc::now(),
        completed_at: None,
        resume_notifier: std::sync::Arc::new(tokio::sync::Notify::new()),
    };

    // Clone resume_notifier for manual resume in test
    let resume_notifier = process.resume_notifier.clone();

    let handle = tokio::spawn(async move { engine.run(&pipeline, process, events_tx).await });

    // Collect events
    let mut received_events = Vec::new();

    // Timeout for receiving events (in case the implementation blocks)
    let timeout_duration = std::time::Duration::from_secs(2);

    while let Ok(Some(event)) = tokio::time::timeout(timeout_duration, events_rx.recv()).await {
        let should_resume = matches!(
            &event,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::HumanReview,
                ..
            }
        );
        received_events.push(event);

        // Resume process when HumanReview is reached (simulate user resume action)
        if should_resume {
            resume_notifier.notify_one();
            break;
        }
    }

    // Verify events
    assert!(
        !received_events.is_empty(),
        "Should have received at least some events"
    );

    // 1. First event should be ProcessStarted
    assert!(
        matches!(
            &received_events[0],
            Event::ProcessStarted { pipeline_name, .. } if pipeline_name == "test-pipeline"
        ),
        "First event should be ProcessStarted"
    );

    // 2. Should have ProcessStatusUpdate(Running) at some point
    let has_running_status = received_events.iter().any(|e| {
        matches!(
            e,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::Running,
                ..
            }
        )
    });
    assert!(has_running_status, "Should have Running status update");

    // 3. Should eventually reach HumanReview status
    let has_human_review = received_events.iter().any(|e| {
        matches!(
            e,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::HumanReview,
                ..
            }
        )
    });
    assert!(has_human_review, "Should reach HumanReview status");

    // 4. Should have log chunks from agent1 execution
    let has_log_chunks = received_events
        .iter()
        .any(|e| matches!(e, Event::ProcessLogChunk { .. }));
    assert!(
        has_log_chunks,
        "Should have log chunks from agent execution"
    );

    // Clean up with timeout safety - wait for remaining events after resume
    let completion_timeout = std::time::Duration::from_secs(5);
    while let Ok(Some(event)) = tokio::time::timeout(completion_timeout, events_rx.recv()).await {
        let is_completed = matches!(&event, Event::ProcessCompleted { .. });
        received_events.push(event);
        if is_completed {
            break;
        }
    }

    // Ensure pipeline completed successfully with timeout safety
    drop(events_rx);
    tokio::time::timeout(std::time::Duration::from_secs(5), handle)
        .await
        .expect("Pipeline should complete within timeout")
        .expect("Pipeline task should not panic")
        .expect("Pipeline execution should succeed");
}

/// RED: Test that pipeline completes successfully without HUMAN_REVIEW
#[tokio::test]
async fn test_pipeline_engine_completes_without_human_review() {
    let agent_configs = vec![
        create_test_agent_config("agent1"),
        create_test_agent_config("agent2"),
    ];

    let agent_manager = AgentManager::new(agent_configs);

    let steps = vec![
        ProcessStep::Agent("agent1".to_string()),
        ProcessStep::Agent("agent2".to_string()),
    ];

    let pipeline = create_test_pipeline("simple-pipeline", steps);

    let (events_tx, mut events_rx) = mpsc::channel(100);

    let engine = PipelineEngine::new(agent_manager);

    // Create initial process
    let process = pk_protocol::Process {
        id: uuid::Uuid::new_v4(),
        pipeline_name: pipeline.name.clone(),
        status: ProcessStatus::Pending,
        current_step_index: 0,
        logs: Vec::new(),
        started_at: chrono::Utc::now(),
        completed_at: None,
        resume_notifier: std::sync::Arc::new(tokio::sync::Notify::new()),
    };

    let handle = tokio::spawn(async move { engine.run(&pipeline, process, events_tx).await });

    let mut received_events = Vec::new();
    let timeout_duration = std::time::Duration::from_secs(2);

    while let Ok(Some(event)) = tokio::time::timeout(timeout_duration, events_rx.recv()).await {
        let is_completed = matches!(&event, Event::ProcessCompleted { .. });
        received_events.push(event);

        // Stop after completion
        if is_completed {
            break;
        }
    }

    // Should have ProcessCompleted event
    let has_completed = received_events
        .iter()
        .any(|e| matches!(e, Event::ProcessCompleted { .. }));
    assert!(has_completed, "Pipeline should complete successfully");

    // Should NOT have HumanReview status
    let has_human_review = received_events.iter().any(|e| {
        matches!(
            e,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::HumanReview,
                ..
            }
        )
    });
    assert!(!has_human_review, "Should not have HumanReview status");

    // Graceful cleanup with timeout safety
    drop(events_rx);
    tokio::time::timeout(std::time::Duration::from_secs(5), handle)
        .await
        .expect("Pipeline should complete within timeout")
        .expect("Pipeline task should not panic")
        .expect("Pipeline execution should succeed");
}
</file>

<file path="pipeline-kit-rs/crates/protocol/src/process_models.rs">
//! Runtime process state models.
//!
//! This module defines the structures for tracking the state of running
//! pipeline executions.

use chrono::DateTime;
use chrono::Utc;
use serde::Deserialize;
use serde::Serialize;
use std::sync::Arc;
use tokio::sync::Notify;
use ts_rs::TS;
use uuid::Uuid;

/// Represents the current lifecycle status of a running pipeline process.
///
/// The status progresses through these states during normal execution:
/// Pending -> Running -> Completed
///
/// Special states:
/// - HumanReview: Paused waiting for manual review
/// - Paused: Manually paused by user
/// - Failed: Execution encountered an error
#[derive(Serialize, Deserialize, Debug, Clone, Copy, PartialEq, Eq, TS)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
pub enum ProcessStatus {
    /// Process has been created but not started yet.
    Pending,

    /// Process is actively executing.
    Running,

    /// Process has been manually paused by the user.
    Paused,

    /// Process is waiting for human review before continuing.
    ///
    /// This happens when a HUMAN_REVIEW step is encountered.
    HumanReview,

    /// Process has completed successfully.
    Completed,

    /// Process has failed due to an error.
    Failed,

    /// Process was killed by user request.
    ///
    /// This happens when the user explicitly terminates a process.
    Killed,
}

/// Represents the runtime state of a single pipeline execution.
///
/// Each time a pipeline is started, a new Process instance is created
/// with a unique ID to track its execution state.
#[derive(Serialize, Deserialize, Debug, Clone, TS)]
pub struct Process {
    /// Unique identifier for this process execution.
    ///
    /// Generated when the process is created and used to track
    /// the process throughout its lifecycle.
    #[ts(type = "string")]
    pub id: Uuid,

    /// Name of the pipeline being executed.
    ///
    /// References a pipeline defined in `.pipeline-kit/pipelines/*.yaml`.
    pub pipeline_name: String,

    /// Current execution status.
    pub status: ProcessStatus,

    /// Zero-based index of the current step in the pipeline.
    ///
    /// Points to the step currently being executed or the next step
    /// to be executed if paused.
    pub current_step_index: usize,

    /// Accumulated log messages from this process execution.
    ///
    /// Contains output from agents, status updates, and error messages.
    pub logs: Vec<String>,

    /// Timestamp when the process was started.
    #[ts(type = "string")]
    pub started_at: DateTime<Utc>,

    /// Timestamp when the process was completed (if finished).
    #[ts(optional, type = "string")]
    pub completed_at: Option<DateTime<Utc>>,

    /// Notifier used to signal resume from paused or human review states.
    ///
    /// This field is not serialized and is used internally for async task coordination.
    /// When a process is paused, the PipelineEngine waits on this notifier.
    /// When resume is called, the StateManager signals this notifier to continue execution.
    #[serde(skip)]
    #[ts(skip)]
    pub resume_notifier: Arc<Notify>,
}
</file>

<file path="pipeline-kit-rs/crates/protocol/tests/serialization.rs">
use pk_protocol::*;

#[test]
fn test_pipeline_deserialization_from_yaml() {
    // Sample YAML structure based on the spec
    let yaml_str = r#"
name: test-pipeline
required-reference-file:
  1: "docs/requirements.md"
  2: "docs/design.md"
output-file:
  1: "src/output.rs"
  2: "tests/output_test.rs"
master:
  model: "claude-sonnet-4"
  system-prompt: "You are a helpful pipeline orchestrator"
  process:
    - "agent-1"
    - "agent-2"
    - "HUMAN_REVIEW"
    - "agent-3"
sub-agents:
  - "agent-1"
  - "agent-2"
  - "agent-3"
"#;

    let pipeline: Pipeline =
        serde_yaml::from_str(yaml_str).expect("Failed to deserialize Pipeline");

    assert_eq!(pipeline.name, "test-pipeline");
    assert_eq!(pipeline.required_reference_file.len(), 2);
    assert_eq!(pipeline.output_file.len(), 2);
    assert_eq!(pipeline.master.model, "claude-sonnet-4");
    assert_eq!(pipeline.master.process.len(), 4);
    assert_eq!(pipeline.sub_agents.len(), 3);

    // Verify ProcessStep variants
    assert_eq!(
        pipeline.master.process[0],
        ProcessStep::Agent("agent-1".to_string())
    );
    assert!(matches!(
        pipeline.master.process[2],
        ProcessStep::HumanReview(_)
    ));
}

#[test]
fn test_agent_serialization() {
    let agent = Agent {
        name: "test-agent".to_string(),
        description: "A test agent".to_string(),
        model: "claude-sonnet-4".to_string(),
        color: "blue".to_string(),
        system_prompt: "Be helpful".to_string(),
    };

    let json = serde_json::to_string(&agent).expect("Failed to serialize Agent");
    let deserialized: Agent = serde_json::from_str(&json).expect("Failed to deserialize Agent");

    assert_eq!(deserialized.name, agent.name);
    assert_eq!(deserialized.description, agent.description);
    assert_eq!(deserialized.model, agent.model);
    assert_eq!(deserialized.color, agent.color);
    // system_prompt is skipped in serialization
    assert_eq!(deserialized.system_prompt, "");
}

#[test]
fn test_process_status_serialization() {
    let status = ProcessStatus::Running;
    let json = serde_json::to_value(status).expect("Failed to serialize ProcessStatus");

    assert_eq!(json, "RUNNING");

    let deserialized: ProcessStatus =
        serde_json::from_value(json).expect("Failed to deserialize ProcessStatus");
    assert_eq!(deserialized, ProcessStatus::Running);
}

#[test]
fn test_process_serialization() {
    use uuid::Uuid;

    let process_id = Uuid::new_v4();
    let process = Process {
        id: process_id,
        pipeline_name: "test-pipeline".to_string(),
        status: ProcessStatus::Pending,
        current_step_index: 0,
        started_at: chrono::Utc::now(),
        completed_at: None,
        logs: vec!["Log entry 1".to_string(), "Log entry 2".to_string()],
        resume_notifier: std::sync::Arc::new(tokio::sync::Notify::new()),
    };

    let json = serde_json::to_string(&process).expect("Failed to serialize Process");
    let deserialized: Process = serde_json::from_str(&json).expect("Failed to deserialize Process");

    assert_eq!(deserialized.id, process.id);
    assert_eq!(deserialized.pipeline_name, process.pipeline_name);
    assert_eq!(deserialized.status, process.status);
    assert_eq!(deserialized.current_step_index, process.current_step_index);
    assert_eq!(deserialized.logs.len(), 2);
}

#[test]
fn test_global_config_serialization() {
    let config = GlobalConfig { git: true };

    let json = serde_json::to_string(&config).expect("Failed to serialize GlobalConfig");
    let deserialized: GlobalConfig =
        serde_json::from_str(&json).expect("Failed to deserialize GlobalConfig");

    assert_eq!(deserialized.git, config.git);
}

#[test]
fn test_op_enum_serialization() {
    use std::path::PathBuf;
    use uuid::Uuid;

    let op = Op::StartPipeline {
        name: "test-pipeline".to_string(),
        reference_file: Some(PathBuf::from("test.md")),
    };

    let json = serde_json::to_value(&op).expect("Failed to serialize Op");
    assert_eq!(json["type"], "startPipeline");
    assert!(json["payload"].is_object());

    let deserialized: Op = serde_json::from_value(json).expect("Failed to deserialize Op");
    match deserialized {
        Op::StartPipeline {
            name,
            reference_file,
        } => {
            assert_eq!(name, "test-pipeline");
            assert!(reference_file.is_some());
        }
        _ => panic!("Wrong variant"),
    }

    let pause_op = Op::PauseProcess {
        process_id: Uuid::new_v4(),
    };
    let json = serde_json::to_value(&pause_op).expect("Failed to serialize Op::PauseProcess");
    assert_eq!(json["type"], "pauseProcess");
}

#[test]
fn test_event_enum_serialization() {
    use uuid::Uuid;

    let event = Event::ProcessStarted {
        process_id: Uuid::new_v4(),
        pipeline_name: "test-pipeline".to_string(),
    };

    let json = serde_json::to_value(&event).expect("Failed to serialize Event");
    assert_eq!(json["type"], "processStarted");
    assert!(json["payload"].is_object());

    let status_update = Event::ProcessStatusUpdate {
        process_id: Uuid::new_v4(),
        status: ProcessStatus::Running,
        step_index: 1,
    };
    let json = serde_json::to_value(&status_update).expect("Failed to serialize Event");
    assert_eq!(json["type"], "processStatusUpdate");
}

#[test]
fn test_process_step_untagged_serialization() {
    use pk_protocol::HumanReviewMarker;

    // Test Agent variant
    let agent_step = ProcessStep::Agent("my-agent".to_string());
    let json = serde_json::to_value(&agent_step).expect("Failed to serialize ProcessStep::Agent");
    assert_eq!(json, "my-agent");

    // Test HumanReview variant
    let human_review_step = ProcessStep::HumanReview(HumanReviewMarker);
    let json = serde_json::to_value(&human_review_step)
        .expect("Failed to serialize ProcessStep::HumanReview");
    assert_eq!(json, "HUMAN_REVIEW");

    // Test deserialization of HUMAN_REVIEW
    let deserialized: ProcessStep =
        serde_json::from_str("\"HUMAN_REVIEW\"").expect("Failed to deserialize HUMAN_REVIEW");
    assert!(matches!(deserialized, ProcessStep::HumanReview(_)));
}
</file>

<file path="pipeline-kit-rs/crates/tui/src/widgets/dashboard.rs">
//! Dashboard widget for displaying process list in a table.
//!
//! This module provides a table-based view of all running processes,
//! showing their ID, name, status, and current step.

use pk_protocol::Process;
use ratatui::layout::Rect;
use ratatui::style::Color;
use ratatui::style::Modifier;
use ratatui::style::Style;
use ratatui::widgets::Block;
use ratatui::widgets::Borders;
use ratatui::widgets::Cell;
use ratatui::widgets::Row;
use ratatui::widgets::Table;
use ratatui::widgets::TableState;
use ratatui::Frame;

/// Renders the dashboard as a table showing all processes.
///
/// # Arguments
/// * `frame` - The frame to render into
/// * `area` - The area to render the table in
/// * `processes` - List of all processes to display
/// * `selected` - Index of the currently selected process
pub fn render_dashboard(frame: &mut Frame, area: Rect, processes: &[Process], selected: usize) {
    // Create table rows from processes with color-coded status
    let rows: Vec<Row> = processes
        .iter()
        .map(|p| {
            let status_style = match p.status {
                pk_protocol::ProcessStatus::Running => Style::default().fg(Color::Green),
                pk_protocol::ProcessStatus::Completed => Style::default().fg(Color::Cyan),
                pk_protocol::ProcessStatus::Failed => Style::default().fg(Color::Red),
                pk_protocol::ProcessStatus::Pending => Style::default().fg(Color::Yellow),
                pk_protocol::ProcessStatus::Paused => Style::default().fg(Color::Magenta),
                pk_protocol::ProcessStatus::HumanReview => Style::default().fg(Color::LightYellow),
                pk_protocol::ProcessStatus::Killed => Style::default().fg(Color::DarkGray),
            };

            Row::new(vec![
                Cell::from(format_uuid(&p.id)),
                Cell::from(p.pipeline_name.clone()),
                Cell::from(format!("{:?}", p.status)).style(status_style),
                Cell::from(format!("{}", p.current_step_index)),
            ])
        })
        .collect();

    // Create table header with styling
    let header = Row::new(vec![
        Cell::from("ID"),
        Cell::from("Pipeline"),
        Cell::from("Status"),
        Cell::from("Step"),
    ])
    .style(
        Style::default()
            .add_modifier(Modifier::BOLD)
            .fg(Color::Cyan),
    );

    // Create the table with proper widths
    let widths = [
        ratatui::layout::Constraint::Length(8), // Shortened UUID (first 8 chars)
        ratatui::layout::Constraint::Percentage(50),
        ratatui::layout::Constraint::Length(15),
        ratatui::layout::Constraint::Length(6),
    ];

    let table = Table::new(rows, widths)
        .header(header)
        .block(
            Block::default()
                .borders(Borders::ALL)
                .title("Dashboard - Processes")
                .style(Style::default().fg(Color::White)),
        )
        .row_highlight_style(
            Style::default()
                .bg(Color::Blue)
                .fg(Color::White)
                .add_modifier(Modifier::BOLD),
        )
        .highlight_symbol(">> ");

    // Create table state with selected index
    let mut table_state = TableState::default();
    if !processes.is_empty() {
        table_state.select(Some(selected));
    }

    frame.render_stateful_widget(table, area, &mut table_state);
}

/// Format UUID to show only the first 8 characters for better readability.
fn format_uuid(uuid: &uuid::Uuid) -> String {
    let uuid_str = uuid.to_string();
    uuid_str.chars().take(8).collect()
}

#[cfg(test)]
mod tests {
    use super::*;
    use chrono::Utc;
    use pk_protocol::Process;
    use pk_protocol::ProcessStatus;
    use ratatui::backend::TestBackend;
    use ratatui::Terminal;
    use std::sync::Arc;
    use tokio::sync::Notify;
    use uuid::Uuid;

    #[test]
    fn test_render_dashboard_empty() {
        // RED: This test should fail because we haven't implemented the table yet
        let backend = TestBackend::new(80, 10);
        let mut terminal = Terminal::new(backend).unwrap();

        let processes: Vec<Process> = vec![];

        terminal
            .draw(|frame| {
                let area = frame.area();
                render_dashboard(frame, area, &processes, 0);
            })
            .unwrap();

        let buffer = terminal.backend().buffer();
        let content = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();

        // Should contain table headers
        assert!(content.contains("ID"));
        assert!(content.contains("Pipeline"));
        assert!(content.contains("Status"));
        assert!(content.contains("Step"));
    }

    #[test]
    fn test_render_dashboard_with_processes() {
        // GREEN: This test should pass with the Table implementation
        let backend = TestBackend::new(120, 20);
        let mut terminal = Terminal::new(backend).unwrap();

        let process1 = Process {
            id: Uuid::new_v4(),
            pipeline_name: "test-pipeline".to_string(),
            status: ProcessStatus::Running,
            current_step_index: 0,
            logs: vec![],
            started_at: Utc::now(),
            completed_at: None,
            resume_notifier: Arc::new(Notify::new()),
        };

        let process2 = Process {
            id: Uuid::new_v4(),
            pipeline_name: "another-pipeline".to_string(),
            status: ProcessStatus::Completed,
            current_step_index: 2,
            logs: vec![],
            started_at: Utc::now(),
            completed_at: Some(Utc::now()),
            resume_notifier: Arc::new(Notify::new()),
        };

        let processes = vec![process1.clone(), process2.clone()];

        terminal
            .draw(|frame| {
                let area = frame.area();
                render_dashboard(frame, area, &processes, 0);
            })
            .unwrap();

        let buffer = terminal.backend().buffer();
        let content = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();

        // Should contain table headers
        assert!(content.contains("ID"));
        assert!(content.contains("Pipeline"));
        assert!(content.contains("Status"));
        assert!(content.contains("Step"));

        // Should contain process data
        assert!(content.contains("test-pipeline"));
        // Note: "another-pipeline" might be truncated due to column width constraints
        // But we should at least see "RUNNING" and step numbers
        assert!(content.contains("RUNNING") || content.contains("Running"));
    }

    #[test]
    fn test_render_dashboard_highlights_selected() {
        // GREEN: This test should pass with highlighting implementation
        let backend = TestBackend::new(120, 20);
        let mut terminal = Terminal::new(backend).unwrap();

        let process1 = Process {
            id: Uuid::new_v4(),
            pipeline_name: "first-pipeline".to_string(),
            status: ProcessStatus::Running,
            current_step_index: 0,
            logs: vec![],
            started_at: Utc::now(),
            completed_at: None,
            resume_notifier: Arc::new(Notify::new()),
        };

        let process2 = Process {
            id: Uuid::new_v4(),
            pipeline_name: "second-pipeline".to_string(),
            status: ProcessStatus::Pending,
            current_step_index: 0,
            logs: vec![],
            started_at: Utc::now(),
            completed_at: None,
            resume_notifier: Arc::new(Notify::new()),
        };

        let processes = vec![process1, process2];

        terminal
            .draw(|frame| {
                let area = frame.area();
                render_dashboard(frame, area, &processes, 1);
            })
            .unwrap();

        let buffer = terminal.backend().buffer();

        // Check if highlighting is applied by looking for blue background color
        // The Table widget with row_highlight_style should apply this to the selected row
        let mut found_blue_bg = false;
        for y in 0..buffer.area().height {
            for x in 0..buffer.area().width {
                let cell = &buffer[(x, y)];
                if cell.bg == Color::Blue {
                    found_blue_bg = true;
                    break;
                }
            }
            if found_blue_bg {
                break;
            }
        }

        assert!(
            found_blue_bg,
            "Selected process row should be highlighted with blue background"
        );
    }
}
</file>

<file path=".gitignore">
# Rust
**/target/
**/*.rs.bk
Cargo.lock

# Node
node_modules/
pnpm-lock.yaml
dist/
*.tsbuildinfo

# Coverage
lcov.info
*.profraw
*.profdata
coverage/
target/coverage/

# START Ruler Generated Files
/.codex/config.toml
/.codex/config.toml.bak
/.cursor/rules/ruler_cursor_instructions.mdc
/.cursor/rules/ruler_cursor_instructions.mdc.bak
/AGENTS.md
/AGENTS.md.bak
/CLAUDE.md
/CLAUDE.md.bak
# END Ruler Generated Files
</file>

<file path="pipeline-kit-cli/bin/pipeline-kit.js">
#!/usr/bin/env node
// Unified entry point for the Pipeline Kit CLI.

import { spawn } from "node:child_process";
import { existsSync } from "fs";
import path from "path";
import { fileURLToPath } from "url";
import { getPlatformName, getBinaryName, getVendorBinaryPath, getDevBinaryPath } from "../lib/platform.js";

// __dirname equivalent in ESM
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const { platform, arch } = process;

// Get platform name using the platform library
const platformName = getPlatformName(platform, arch);

if (!platformName) {
  // Generate list of supported platforms for error message
  const { platformMap } = await import("../lib/platform.js");
  const supportedPlatforms = Object.keys(platformMap)
    .filter(key => !key.startsWith('android')) // Don't list android explicitly
    .map(key => `  - ${key}`)
    .join('\n');

  throw new Error(
    `Unsupported platform: ${platform} (${arch})\n\n` +
    `Supported platforms:\n${supportedPlatforms}`
  );
}

// Construct paths using the platform library
const vendorRoot = path.join(__dirname, "..", "vendor");
const binaryName = getBinaryName(platform);
const binaryPath = getVendorBinaryPath(vendorRoot, platformName, binaryName);

// Development mode fallback: if vendor binary doesn't exist, try local build
let finalBinaryPath = binaryPath;
if (!existsSync(binaryPath)) {
  const cliDir = path.join(__dirname, "..");
  const devBinaryPath = getDevBinaryPath(cliDir, binaryName);

  if (existsSync(devBinaryPath)) {
    finalBinaryPath = devBinaryPath;
  } else {
    console.error(
      `Error: Pipeline Kit binary not found.\n` +
      `  Expected at: ${binaryPath}\n` +
      `  Or (dev mode): ${devBinaryPath}\n\n` +
      `Please ensure the binary is installed correctly.\n` +
      `For development, build the Rust binary first:\n` +
      `  cd pipeline-kit-rs && cargo build --release\n`
    );
    process.exit(1);
  }
}

// Use an asynchronous spawn instead of spawnSync so that Node is able to
// respond to signals (e.g. Ctrl-C / SIGINT) while the native binary is
// executing. This allows us to forward those signals to the child process
// and guarantees that when either the child terminates or the parent
// receives a fatal signal, both processes exit in a predictable manner.

const child = spawn(finalBinaryPath, process.argv.slice(2), {
  stdio: "inherit",
  env: { ...process.env, PIPELINE_KIT_MANAGED_BY_NPM: "1" },
});

child.on("error", (err) => {
  // Typically triggered when the binary is missing or not executable.
  // Re-throwing here will terminate the parent with a non-zero exit code
  // while still printing a helpful stack trace.
  console.error(err);
  process.exit(1);
});

// Forward common termination signals to the child so that it shuts down
// gracefully. In the handler we temporarily disable the default behavior of
// exiting immediately; once the child has been signaled we simply wait for
// its exit event which will in turn terminate the parent (see below).
const forwardSignal = (signal) => {
  if (child.killed) {
    return;
  }
  try {
    child.kill(signal);
  } catch {
    /* ignore */
  }
};

["SIGINT", "SIGTERM", "SIGHUP"].forEach((sig) => {
  process.on(sig, () => forwardSignal(sig));
});

// When the child exits, mirror its termination reason in the parent so that
// shell scripts and other tooling observe the correct exit status.
// Wrap the lifetime of the child process in a Promise so that we can await
// its termination in a structured way. The Promise resolves with an object
// describing how the child exited: either via exit code or due to a signal.
const childResult = await new Promise((resolve) => {
  child.on("exit", (code, signal) => {
    if (signal) {
      resolve({ type: "signal", signal });
    } else {
      resolve({ type: "code", exitCode: code ?? 1 });
    }
  });
});

if (childResult.type === "signal") {
  // Re-emit the same signal so that the parent terminates with the expected
  // semantics (this also sets the correct exit code of 128 + n).
  process.kill(process.pid, childResult.signal);
} else {
  process.exit(childResult.exitCode);
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/factory.rs">
//! Agent factory for creating agent instances from configurations.

use crate::agents::adapters::ClaudeAdapter;
use crate::agents::adapters::CodexAdapter;
use crate::agents::adapters::CursorAdapter;
use crate::agents::adapters::GeminiAdapter;
use crate::agents::adapters::MockAgent;
use crate::agents::adapters::QwenAdapter;
use crate::agents::agent_type::AgentType;
use crate::agents::base::Agent;
use anyhow::Result;
use pk_protocol::agent_models;
use std::sync::Arc;

/// Factory for creating agent instances based on configuration.
///
/// The factory determines which adapter to use based on the model name
/// and instantiates the appropriate agent type.
pub struct AgentFactory;

impl AgentFactory {
    /// Create an agent instance from a configuration.
    ///
    /// # Arguments
    ///
    /// * `config` - The agent configuration from `.pipeline-kit/agents/*.md`
    ///
    /// # Returns
    ///
    /// An `Arc<dyn Agent>` wrapping the appropriate adapter, or an error if creation fails.
    ///
    /// # Behavior
    ///
    /// The factory uses `AgentType::from_model_name()` to determine which adapter to create:
    /// - Claude models ‚Üí `ClaudeAdapter`
    /// - Cursor/GPT models ‚Üí `CursorAdapter`
    /// - Gemini models ‚Üí `GeminiAdapter`
    /// - Codex models ‚Üí `CodexAdapter` (Phase 2)
    /// - Qwen models ‚Üí `QwenAdapter` (Phase 2)
    /// - Unknown models ‚Üí `MockAgent`
    ///
    /// # Examples
    ///
    /// ```
    /// use pk_core::agents::AgentFactory;
    /// use pk_protocol::agent_models::Agent as AgentConfig;
    ///
    /// let config = AgentConfig {
    ///     name: "developer".to_string(),
    ///     model: "claude-sonnet-4.5".to_string(),
    ///     description: "Developer agent".to_string(),
    ///     color: "blue".to_string(),
    ///     system_prompt: "You are a helpful developer.".to_string(),
    /// };
    ///
    /// let agent = AgentFactory::create(&config).unwrap();
    /// ```
    pub fn create(config: &agent_models::Agent) -> Result<Arc<dyn Agent>> {
        let agent_type = AgentType::from_model_name(&config.model);

        match agent_type {
            AgentType::Claude => {
                let adapter = ClaudeAdapter::new(
                    config.name.clone(),
                    config.model.clone(),
                    config.system_prompt.clone(),
                )?;
                Ok(Arc::new(adapter))
            }
            AgentType::Cursor => {
                let adapter = CursorAdapter::new(
                    config.name.clone(),
                    config.model.clone(),
                    config.system_prompt.clone(),
                )?;
                Ok(Arc::new(adapter))
            }
            AgentType::Gemini => {
                let adapter = GeminiAdapter::new(
                    config.name.clone(),
                    config.model.clone(),
                    config.system_prompt.clone(),
                )?;
                Ok(Arc::new(adapter))
            }
            AgentType::Codex => {
                let adapter = CodexAdapter::new(
                    config.name.clone(),
                    config.model.clone(),
                    config.system_prompt.clone(),
                )?;
                Ok(Arc::new(adapter))
            }
            AgentType::Qwen => {
                let adapter = QwenAdapter::new(
                    config.name.clone(),
                    config.model.clone(),
                    config.system_prompt.clone(),
                )?;
                Ok(Arc::new(adapter))
            }
            AgentType::Mock => {
                // Support different mock types for testing based on model name
                if config.model == "test-failure-model" {
                    Ok(Arc::new(MockAgent::failing()))
                } else if config.model == "test-unavailable-model" {
                    Ok(Arc::new(MockAgent::unavailable()))
                } else {
                    // Default to success for "test-model" and others
                    Ok(Arc::new(MockAgent::success()))
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn create_test_config(name: &str, model: &str) -> agent_models::Agent {
        agent_models::Agent {
            name: name.to_string(),
            model: model.to_string(),
            description: format!("Test agent {}", name),
            color: "blue".to_string(),
            system_prompt: "Test prompt".to_string(),
        }
    }

    #[tokio::test]
    async fn test_factory_create_claude() {
        let config = create_test_config("claude-agent", "claude-sonnet-4.5");
        let agent = AgentFactory::create(&config);
        assert!(agent.is_ok());

        let agent = agent.unwrap();
        // ClaudeAdapter check_availability returns false unless `claude` CLI is installed
        // This is expected behavior in test environment
        let _ = agent.check_availability().await;
    }

    #[tokio::test]
    async fn test_factory_create_cursor() {
        let config = create_test_config("cursor-agent", "gpt-5");
        let agent = AgentFactory::create(&config);
        assert!(agent.is_ok());

        let agent = agent.unwrap();
        // CursorAdapter check_availability returns false unless `cursor-agent` CLI is installed
        // This is expected behavior in test environment
        let _ = agent.check_availability().await;
    }

    #[tokio::test]
    async fn test_factory_create_gemini() {
        let config = create_test_config("gemini-agent", "gemini-2.5-pro");
        let agent = AgentFactory::create(&config);
        assert!(agent.is_ok());

        let agent = agent.unwrap();
        // GeminiAdapter check_availability returns false unless `gemini-cli` CLI and GEMINI_API_KEY are set
        // This is expected behavior in test environment
        let _ = agent.check_availability().await;
    }

    #[tokio::test]
    async fn test_factory_create_codex() {
        let config = create_test_config("codex-agent", "codex-model");
        let agent = AgentFactory::create(&config);
        assert!(agent.is_ok());

        let agent = agent.unwrap();
        // CodexAdapter check_availability returns false unless `codex` CLI and OPENAI_API_KEY are set
        // This is expected behavior in test environment
        let _ = agent.check_availability().await;
    }

    #[tokio::test]
    async fn test_factory_create_qwen() {
        let config = create_test_config("qwen-agent", "qwen-coder");
        let agent = AgentFactory::create(&config);
        assert!(agent.is_ok());

        let agent = agent.unwrap();
        // QwenAdapter check_availability returns false unless `qwen` CLI is installed
        // This is expected behavior in test environment
        let _ = agent.check_availability().await;
    }

    #[tokio::test]
    async fn test_factory_create_mock() {
        let config = create_test_config("mock-agent", "test-model");
        let agent = AgentFactory::create(&config);
        assert!(agent.is_ok());

        let agent = agent.unwrap();
        assert!(agent.check_availability().await);
    }

    #[test]
    fn test_factory_returns_arc() {
        let config = create_test_config("test", "claude-sonnet-4.5");
        let agent1 = AgentFactory::create(&config).unwrap();
        let agent2 = agent1.clone();

        // Both should point to the same agent (Arc semantics)
        assert_eq!(Arc::strong_count(&agent1), 2);
        assert_eq!(Arc::strong_count(&agent2), 2);
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/agents/mod.rs">
//! Agent abstraction and management.
//!
//! This module provides the `Agent` trait (Adapter Pattern) and
//! the `AgentManager` for orchestrating multiple agent implementations.

pub mod adapters;
pub mod agent_type;
pub mod base;
pub mod cli_executor;
pub mod factory;
pub mod manager;

pub use adapters::MockAgent;
pub use agent_type::AgentType;
pub use base::Agent;
pub use base::AgentError;
pub use base::AgentEvent;
pub use base::Attachment;
pub use base::ExecutionContext;
pub use factory::AgentFactory;
pub use manager::AgentManager;
</file>

<file path="pipeline-kit-rs/crates/core/src/engine/mod.rs">
//! Pipeline execution engine.
//!
//! The PipelineEngine is responsible for executing pipeline steps sequentially,
//! managing agent interactions, and coordinating process state transitions.

use crate::agents::base::AgentEvent;
use crate::agents::base::ExecutionContext;
use crate::agents::manager::AgentManager;
use crate::state::process::advance_step;
use crate::state::process::complete_process;
use crate::state::process::fail_process;
use crate::state::process::log_to_process;
use crate::state::process::pause_for_human_review;
use crate::state::process::start_process;
use anyhow::anyhow;
use anyhow::Result;
use pk_protocol::ipc::Event;
use pk_protocol::pipeline_models::Pipeline;
use pk_protocol::pipeline_models::ProcessStep;
use pk_protocol::process_models::Process;
use tokio::sync::mpsc::Sender;
use tokio_stream::StreamExt;

/// The main pipeline execution engine.
///
/// PipelineEngine takes a Pipeline definition and executes its steps
/// sequentially, delegating agent execution to the AgentManager.
pub struct PipelineEngine {
    agent_manager: AgentManager,
}

impl PipelineEngine {
    /// Create a new PipelineEngine with the given AgentManager.
    ///
    /// # Arguments
    ///
    /// * `agent_manager` - The manager responsible for agent lookup and execution
    pub fn new(agent_manager: AgentManager) -> Self {
        Self { agent_manager }
    }

    /// Execute a pipeline and return the final Process state.
    ///
    /// This is the main entry point for pipeline execution. It:
    /// 1. Creates a new Process
    /// 2. Emits ProcessStarted event
    /// 3. Iterates through pipeline steps sequentially
    /// 4. Executes agents or pauses for HUMAN_REVIEW
    /// 5. Emits appropriate events for each state change
    /// 6. Returns the final Process state
    ///
    /// # Arguments
    ///
    /// * `pipeline` - The pipeline definition to execute
    /// * `events_tx` - Channel for sending events to the UI
    ///
    /// # Returns
    ///
    /// The final Process state after execution completes or pauses.
    ///
    /// # Errors
    ///
    /// Returns an error if:
    /// - An agent is not found
    /// - Agent execution fails
    /// - Any other execution error occurs
    pub async fn run(
        &self,
        pipeline: &Pipeline,
        mut process: Process,
        events_tx: Sender<Event>,
    ) -> Result<Process> {
        // Process is already created and passed in

        // Emit ProcessStarted event
        let _ = events_tx
            .send(Event::ProcessStarted {
                process_id: process.id,
                pipeline_name: pipeline.name.clone(),
            })
            .await;

        // Start the process (transition to Running)
        start_process(&mut process, &events_tx).await;

        // Execute each step in the pipeline
        for (step_index, step) in pipeline.master.process.iter().enumerate() {
            // Update current step
            if step_index > 0 {
                advance_step(&mut process);
            }

            match step {
                ProcessStep::Agent(agent_name) => {
                    // Log the step
                    log_to_process(
                        &mut process,
                        &events_tx,
                        format!("Executing agent: {}", agent_name),
                    )
                    .await;

                    // Execute the agent
                    if let Err(e) = self
                        .execute_agent_step(&mut process, agent_name, &events_tx)
                        .await
                    {
                        fail_process(
                            &mut process,
                            &events_tx,
                            format!("Agent execution failed: {}", e),
                        )
                        .await;
                        return Err(e);
                    }

                    // Log completion of this step
                    log_to_process(
                        &mut process,
                        &events_tx,
                        format!("Agent {} completed", agent_name),
                    )
                    .await;
                }
                ProcessStep::HumanReview(_) => {
                    // Log the human review step
                    log_to_process(
                        &mut process,
                        &events_tx,
                        "Pausing for human review".to_string(),
                    )
                    .await;

                    // Pause for human review
                    pause_for_human_review(&mut process, &events_tx).await;

                    // Wait for resume signal via the notifier
                    // This blocks the execution until StateManager calls notify_one()
                    let notifier = process.resume_notifier.clone();
                    notifier.notified().await;

                    // Log resumption
                    log_to_process(
                        &mut process,
                        &events_tx,
                        "Resumed from human review".to_string(),
                    )
                    .await;

                    // Continue to next step after resume
                }
            }
        }

        // All steps completed successfully
        complete_process(&mut process, &events_tx).await;

        Ok(process)
    }

    /// Execute a single agent step.
    ///
    /// This method:
    /// 1. Looks up the agent by name
    /// 2. Creates an execution context
    /// 3. Executes the agent
    /// 4. Streams events and logs from the agent
    ///
    /// # Arguments
    ///
    /// * `process` - The current process state
    /// * `agent_name` - The name of the agent to execute
    /// * `events_tx` - Channel for sending events
    ///
    /// # Errors
    ///
    /// Returns an error if the agent is not found or execution fails.
    async fn execute_agent_step(
        &self,
        process: &mut Process,
        agent_name: &str,
        events_tx: &Sender<Event>,
    ) -> Result<()> {
        // Create execution context
        // For now, we use a simple instruction. Future versions may include
        // more context from the pipeline definition.
        let context = ExecutionContext::new(format!(
            "Execute step for pipeline: {}",
            process.pipeline_name
        ));

        // Execute the agent
        let mut stream = self
            .agent_manager
            .execute(agent_name, &context)
            .await
            .map_err(|e| anyhow!("Failed to execute agent {}: {}", agent_name, e))?;

        // Process the event stream
        while let Some(event_result) = stream.next().await {
            match event_result {
                Ok(AgentEvent::Thought(thought)) => {
                    log_to_process(process, events_tx, format!("[Thought] {}", thought)).await;
                }
                Ok(AgentEvent::ToolCall(tool)) => {
                    log_to_process(process, events_tx, format!("[Tool Call] {}", tool)).await;
                }
                Ok(AgentEvent::MessageChunk(chunk)) => {
                    log_to_process(process, events_tx, chunk).await;
                }
                Ok(AgentEvent::Completed) => {
                    // Agent completed successfully
                    break;
                }
                Err(e) => {
                    // Agent execution error
                    return Err(anyhow!("Agent error: {}", e));
                }
            }
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::state::process::create_process;
    use pk_protocol::agent_models::Agent as AgentConfig;
    use pk_protocol::pipeline_models::MasterAgentConfig;
    use pk_protocol::process_models::ProcessStatus;
    use std::collections::HashMap;
    use tokio::sync::mpsc;

    fn create_test_agent_config(name: &str) -> AgentConfig {
        AgentConfig {
            name: name.to_string(),
            description: format!("Test agent {}", name),
            model: "test-model".to_string(),
            color: "blue".to_string(),
            system_prompt: "Test prompt".to_string(),
        }
    }

    fn create_test_pipeline(name: &str, steps: Vec<ProcessStep>) -> Pipeline {
        Pipeline {
            name: name.to_string(),
            required_reference_file: HashMap::new(),
            output_file: HashMap::new(),
            master: MasterAgentConfig {
                model: "test-model".to_string(),
                system_prompt: "Test orchestration".to_string(),
                process: steps,
            },
            sub_agents: vec!["agent1".to_string(), "agent2".to_string()],
        }
    }

    #[tokio::test]
    async fn test_pipeline_engine_new() {
        let configs = vec![create_test_agent_config("test-agent")];
        let manager = AgentManager::new(configs);
        let engine = PipelineEngine::new(manager);

        // Engine should be created successfully
        assert_eq!(engine.agent_manager.list_agents().len(), 1);
    }

    #[tokio::test]
    async fn test_pipeline_engine_simple_execution() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let engine = PipelineEngine::new(manager);

        let steps = vec![ProcessStep::Agent("agent1".to_string())];
        let pipeline = create_test_pipeline("simple-pipeline", steps);

        let (tx, _rx) = mpsc::channel(100);
        let process = create_process("simple-pipeline".to_string());

        let result = engine.run(&pipeline, process, tx).await;
        assert!(result.is_ok());

        let final_process = result.unwrap();
        assert_eq!(final_process.status, ProcessStatus::Completed);
        assert_eq!(final_process.pipeline_name, "simple-pipeline");
    }

    #[tokio::test]
    async fn test_pipeline_engine_with_human_review() {
        let configs = vec![
            create_test_agent_config("agent1"),
            create_test_agent_config("agent2"),
        ];
        let manager = AgentManager::new(configs);
        let engine = PipelineEngine::new(manager);

        let steps = vec![
            ProcessStep::Agent("agent1".to_string()),
            ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
            ProcessStep::Agent("agent2".to_string()),
        ];
        let pipeline = create_test_pipeline("review-pipeline", steps);

        let (tx, _rx) = mpsc::channel(100);
        let process = create_process("review-pipeline".to_string());

        // Spawn the engine in a background task since it will block at HumanReview
        let handle = tokio::spawn(async move { engine.run(&pipeline, process, tx).await });

        // Give the engine time to reach HumanReview state
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

        // The task should still be running (blocked at HumanReview)
        assert!(!handle.is_finished());

        // Abort the task to clean up
        handle.abort();
    }

    #[tokio::test]
    async fn test_pipeline_engine_agent_not_found() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let engine = PipelineEngine::new(manager);

        let steps = vec![ProcessStep::Agent("nonexistent-agent".to_string())];
        let pipeline = create_test_pipeline("failing-pipeline", steps);

        let (tx, _rx) = mpsc::channel(100);
        let process = create_process("failing-pipeline".to_string());

        let result = engine.run(&pipeline, process, tx).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_pipeline_engine_event_emission() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let engine = PipelineEngine::new(manager);

        let steps = vec![ProcessStep::Agent("agent1".to_string())];
        let pipeline = create_test_pipeline("event-test", steps);

        let (tx, mut rx) = mpsc::channel(100);
        let process = create_process("event-test".to_string());

        let handle = tokio::spawn(async move { engine.run(&pipeline, process, tx).await });

        let mut events = Vec::new();
        while let Some(event) = rx.recv().await {
            let is_completed = matches!(event, Event::ProcessCompleted { .. });
            events.push(event);
            if is_completed {
                break;
            }
        }

        // Should have ProcessStarted event
        assert!(matches!(&events[0], Event::ProcessStarted { .. }));

        // Should have ProcessStatusUpdate(Running) event
        assert!(events.iter().any(|e| matches!(
            e,
            Event::ProcessStatusUpdate {
                status: ProcessStatus::Running,
                ..
            }
        )));

        // Should have log chunks
        assert!(events
            .iter()
            .any(|e| matches!(e, Event::ProcessLogChunk { .. })));

        // Should have ProcessCompleted event
        assert!(events
            .iter()
            .any(|e| matches!(e, Event::ProcessCompleted { .. })));

        let _ = handle.await;
    }
}
</file>

<file path="pipeline-kit-rs/crates/protocol/Cargo.toml">
[package]
name = "pk-protocol"
version = { workspace = true }
edition = { workspace = true }
authors = { workspace = true }
license = { workspace = true }
repository = { workspace = true }
description = "Shared protocol definitions for pipeline-kit"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
ts-rs = { version = "9.0", features = ["serde-compat", "uuid-impl", "chrono-impl"] }
uuid = { version = "1.8", features = ["serde", "v4"] }
chrono = { version = "0.4", features = ["serde"] }
tokio = { version = "1.40", features = ["sync"] }

[dev-dependencies]
serde_yaml = "0.9"
</file>

<file path="pipeline-kit-rs/crates/tui/src/app.rs">
//! TUI application state and event loop.
//!
//! This module defines the main `App` struct that manages the TUI state
//! and the event loop using `tokio::select!`.

use anyhow::Result;
use crossterm::event::KeyEvent;
use pk_protocol::Event;
use pk_protocol::Op;
use pk_protocol::Process;
use ratatui::layout::Constraint;
use ratatui::layout::Direction;
use ratatui::layout::Layout;
use ratatui::layout::Rect;
use ratatui::style::Color;
use ratatui::style::Style;
use ratatui::widgets::Block;
use ratatui::widgets::Borders;
use ratatui::widgets::Paragraph;
use ratatui::Frame;
use tokio::select;
use tokio::sync::mpsc::UnboundedReceiver;
use tokio::sync::mpsc::UnboundedSender;
use tokio_stream::StreamExt;

use crate::event::EventStatus;
use crate::event_handler;
use crate::tui::Tui;
use crate::tui::TuiEvent;
use crate::widgets::dashboard;
use crate::widgets::CommandComposer;

/// Main TUI application state.
///
/// This struct holds all the state needed to render the UI and process events.
pub struct App {
    /// List of all processes being tracked.
    pub processes: Vec<Process>,
    /// Index of the currently selected process (for detail view).
    pub selected_index: usize,
    /// Command composer widget with autocomplete.
    pub command_composer: CommandComposer,
    /// Channel to send operations to the core.
    pub op_tx: UnboundedSender<Op>,
    /// Channel to receive events from the core.
    pub event_rx: UnboundedReceiver<Event>,
    /// Flag to indicate if the application should exit.
    pub should_exit: bool,
    /// Error message to display (if any).
    pub error_message: Option<String>,
}

impl App {
    /// Create a new App with communication channels.
    pub fn new(op_tx: UnboundedSender<Op>, event_rx: UnboundedReceiver<Event>) -> Self {
        Self {
            processes: Vec::new(),
            selected_index: 0,
            command_composer: CommandComposer::new(),
            op_tx,
            event_rx,
            should_exit: false,
            error_message: None,
        }
    }

    /// Main event loop.
    ///
    /// Uses `tokio::select!` to handle keyboard input and core events concurrently.
    pub async fn run(&mut self, tui: &mut Tui) -> Result<()> {
        let mut tui_events = tui.event_stream();

        tui.frame_requester().schedule_frame();

        while !self.should_exit {
            select! {
                Some(event) = self.event_rx.recv() => {
                    self.handle_core_event(event);
                    tui.frame_requester().schedule_frame();
                }
                Some(tui_event) = tui_events.next() => {
                    self.handle_tui_event(tui, tui_event).await?;
                }
            }
        }

        Ok(())
    }

    /// Handle events from the core (pk-core).
    fn handle_core_event(&mut self, event: Event) {
        event_handler::handle_core_event(&mut self.processes, event);
    }

    /// Handle TUI events (keyboard input, resize, draw).
    async fn handle_tui_event(&mut self, tui: &mut Tui, event: TuiEvent) -> Result<()> {
        match event {
            TuiEvent::Key(key_event) => {
                self.handle_key_event(key_event);
                tui.frame_requester().schedule_frame();
            }
            TuiEvent::Paste(_) => {
                // Ignore paste events for now
            }
            TuiEvent::Draw => {
                tui.draw(|frame| {
                    self.render(frame);
                })?;
            }
        }
        Ok(())
    }

    /// Handle keyboard events.
    ///
    /// This method implements the chain of responsibility pattern:
    /// 1. First, try to delegate the event to the CommandComposer widget
    /// 2. If the widget consumed the event, we're done
    /// 3. If not, handle global events (quit, process navigation, command submission)
    fn handle_key_event(&mut self, key_event: KeyEvent) {
        use crossterm::event::KeyCode;
        use crossterm::event::KeyEventKind;
        use crossterm::event::KeyModifiers;

        // Ignore key release events
        if key_event.kind != KeyEventKind::Press {
            return;
        }

        // First, try to delegate to CommandComposer
        let status = self.command_composer.handle_key_event(key_event);

        // If the event was consumed by the widget, clear error on char input and return
        if status == EventStatus::Consumed {
            // Clear error message when user types (only for char input)
            if matches!(key_event.code, KeyCode::Char(_)) {
                self.error_message = None;
            }
            // Also clear error on Esc
            if matches!(key_event.code, KeyCode::Esc) {
                self.error_message = None;
            }
            return;
        }

        // Event was not consumed by widget, handle global events
        match key_event.code {
            // Global quit keys
            KeyCode::Char('q') if self.command_composer.input().is_empty() => {
                self.should_exit = true;
            }
            KeyCode::Char('c') if key_event.modifiers.contains(KeyModifiers::CONTROL) => {
                self.should_exit = true;
            }

            // Process navigation (only when popup is not shown)
            KeyCode::Up => {
                if self.selected_index > 0 {
                    self.selected_index -= 1;
                }
            }
            KeyCode::Down => {
                if self.selected_index + 1 < self.processes.len() {
                    self.selected_index += 1;
                }
            }

            // Command submission
            KeyCode::Enter => {
                self.handle_command_submit();
            }

            _ => {}
        }
    }

    /// Handle command submission (Enter key).
    fn handle_command_submit(&mut self) {
        match self.command_composer.parse_command() {
            Ok(Some(op)) => {
                // Send the Op to the core
                if let Err(e) = self.op_tx.send(op) {
                    self.error_message = Some(format!("Failed to send command: {}", e));
                } else {
                    // Clear the composer on success
                    self.command_composer.clear();
                    self.error_message = None;
                }
            }
            Ok(None) => {
                // Empty command, just clear
                self.command_composer.clear();
            }
            Err(err) => {
                // Show error message
                self.error_message = Some(err);
            }
        }
    }

    /// Render the TUI.
    fn render(&self, frame: &mut Frame) {
        let area = frame.area();

        // Create a 3-panel layout: dashboard (top), detail (middle), command (bottom)
        let chunks = Layout::default()
            .direction(Direction::Vertical)
            .constraints([
                Constraint::Percentage(40), // Dashboard
                Constraint::Percentage(50), // Detail
                Constraint::Length(3),      // Command input
            ])
            .split(area);

        self.render_dashboard(frame, chunks[0]);
        self.render_detail(frame, chunks[1]);
        self.render_command_input(frame, chunks[2]);
    }

    /// Render the dashboard (list of processes).
    fn render_dashboard(&self, frame: &mut Frame, area: Rect) {
        // Delegate to the dashboard widget's render_dashboard function
        dashboard::render_dashboard(frame, area, &self.processes, self.selected_index);
    }

    /// Render the detail view (selected process logs).
    fn render_detail(&self, frame: &mut Frame, area: Rect) {
        let block = Block::default()
            .borders(Borders::ALL)
            .title("Detail - Process Logs");

        let text = if let Some(process) = self.processes.get(self.selected_index) {
            if process.logs.is_empty() {
                "No logs yet.".to_string()
            } else {
                process.logs.join("\n")
            }
        } else {
            "No process selected.".to_string()
        };

        let paragraph = Paragraph::new(text).block(block);
        frame.render_widget(paragraph, area);
    }

    /// Render the command input area.
    fn render_command_input(&self, frame: &mut Frame, area: Rect) {
        // Render the composer input
        self.command_composer.render(area, frame.buffer_mut());

        // Render autocomplete popup if needed (above the command input)
        if self.command_composer.should_show_popup() {
            let popup_height = 7.min(area.height.saturating_sub(1));
            let popup_y = area.y.saturating_sub(popup_height);
            let popup_area = Rect {
                x: area.x,
                y: popup_y,
                width: area.width,
                height: popup_height,
            };
            self.command_composer
                .render_popup(popup_area, frame.buffer_mut());
        }

        // Render error message if any
        if let Some(ref error) = self.error_message {
            let error_text = format!("Error: {}", error);
            let error_paragraph = Paragraph::new(error_text).style(Style::default().fg(Color::Red));
            let error_area = Rect {
                x: area.x + 2,
                y: area.y + area.height - 1,
                width: area.width.saturating_sub(4),
                height: 1,
            };
            frame.render_widget(error_paragraph, error_area);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crossterm::event::KeyCode;
    use ratatui::backend::TestBackend;
    use ratatui::Terminal;
    use tokio::sync::mpsc::unbounded_channel;
    use uuid::Uuid;

    #[tokio::test]
    async fn test_app_renders_empty_screen() {
        let (op_tx, _op_rx) = unbounded_channel();
        let (_event_tx, event_rx) = unbounded_channel();

        let app = App::new(op_tx, event_rx);

        let backend = TestBackend::new(80, 24);
        let mut terminal = Terminal::new(backend).unwrap();

        terminal
            .draw(|frame| {
                app.render(frame);
            })
            .unwrap();

        let buffer = terminal.backend().buffer();

        // Check that the dashboard title is rendered
        let dashboard_title = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();
        assert!(dashboard_title.contains("Dashboard"));
        assert!(dashboard_title.contains("Detail"));
        assert!(dashboard_title.contains("Command"));
    }

    #[tokio::test]
    async fn test_app_quit_on_q() {
        let (op_tx, _op_rx) = unbounded_channel();
        let (_event_tx, event_rx) = unbounded_channel();

        let mut app = App::new(op_tx, event_rx);

        assert!(!app.should_exit);

        app.handle_key_event(KeyEvent::from(KeyCode::Char('q')));

        assert!(app.should_exit);
    }

    #[tokio::test]
    async fn test_app_handles_process_started_event() {
        let (op_tx, _op_rx) = unbounded_channel();
        let (_event_tx, event_rx) = unbounded_channel();

        let mut app = App::new(op_tx, event_rx);

        assert_eq!(app.processes.len(), 0);

        let process_id = Uuid::new_v4();
        app.handle_core_event(Event::ProcessStarted {
            process_id,
            pipeline_name: "test-pipeline".to_string(),
        });

        assert_eq!(app.processes.len(), 1);
        assert_eq!(app.processes[0].pipeline_name, "test-pipeline");
        assert_eq!(app.processes[0].id, process_id);
    }

    #[tokio::test]
    async fn test_app_navigation_with_arrow_keys() {
        let (op_tx, _op_rx) = unbounded_channel();
        let (_event_tx, event_rx) = unbounded_channel();

        let mut app = App::new(op_tx, event_rx);

        // Add some test processes
        for i in 0..3 {
            app.handle_core_event(Event::ProcessStarted {
                process_id: Uuid::new_v4(),
                pipeline_name: format!("pipeline-{}", i),
            });
        }

        assert_eq!(app.selected_index, 0);

        app.handle_key_event(KeyEvent::from(KeyCode::Down));
        assert_eq!(app.selected_index, 1);

        app.handle_key_event(KeyEvent::from(KeyCode::Down));
        assert_eq!(app.selected_index, 2);

        // Should not go beyond the last index
        app.handle_key_event(KeyEvent::from(KeyCode::Down));
        assert_eq!(app.selected_index, 2);

        app.handle_key_event(KeyEvent::from(KeyCode::Up));
        assert_eq!(app.selected_index, 1);

        app.handle_key_event(KeyEvent::from(KeyCode::Up));
        assert_eq!(app.selected_index, 0);

        // Should not go below 0
        app.handle_key_event(KeyEvent::from(KeyCode::Up));
        assert_eq!(app.selected_index, 0);
    }

    #[tokio::test]
    async fn test_dashboard_renders_table_not_paragraph() {
        // RED: This test should fail because we're currently using Paragraph
        // instead of the Table widget from widgets::dashboard
        let (op_tx, _op_rx) = unbounded_channel();
        let (_event_tx, event_rx) = unbounded_channel();

        let mut app = App::new(op_tx, event_rx);

        // Add some test processes
        use chrono::Utc;
        use pk_protocol::ProcessStatus;

        use std::sync::Arc;
        use tokio::sync::Notify;

        let process1 = Process {
            id: Uuid::new_v4(),
            pipeline_name: "test-pipeline-1".to_string(),
            status: ProcessStatus::Running,
            current_step_index: 0,
            logs: vec![],
            started_at: Utc::now(),
            completed_at: None,
            resume_notifier: Arc::new(Notify::new()),
        };

        let process2 = Process {
            id: Uuid::new_v4(),
            pipeline_name: "test-pipeline-2".to_string(),
            status: ProcessStatus::Completed,
            current_step_index: 5,
            logs: vec![],
            started_at: Utc::now(),
            completed_at: Some(Utc::now()),
            resume_notifier: Arc::new(Notify::new()),
        };

        app.processes.push(process1);
        app.processes.push(process2);

        let backend = TestBackend::new(120, 30);
        let mut terminal = Terminal::new(backend).unwrap();

        terminal
            .draw(|frame| {
                app.render(frame);
            })
            .unwrap();

        let buffer = terminal.backend().buffer();
        let content = buffer
            .content()
            .iter()
            .map(|cell| cell.symbol())
            .collect::<String>();

        // The Table widget should render proper headers
        assert!(content.contains("ID"), "Should have 'ID' column header");
        assert!(
            content.contains("Pipeline"),
            "Should have 'Pipeline' column header"
        );
        assert!(
            content.contains("Status"),
            "Should have 'Status' column header"
        );
        assert!(content.contains("Step"), "Should have 'Step' column header");

        // Should contain process data
        assert!(
            content.contains("test-pipeline-1"),
            "Should show first pipeline name"
        );
        assert!(
            content.contains("test-pipeline-2"),
            "Should show second pipeline name"
        );

        // Should NOT contain the Paragraph-style ">" prefix
        // (Table uses ">>" as highlight symbol instead)
        // This assertion will fail with current Paragraph implementation
        let lines: Vec<&str> = content.split('\n').collect();
        let has_paragraph_style_prefix = lines
            .iter()
            .any(|line| line.trim_start().starts_with(">") && !line.trim_start().starts_with(">>"));
        assert!(
            !has_paragraph_style_prefix,
            "Should NOT use Paragraph-style single '>' prefix"
        );
    }
}
</file>

<file path="pipeline-kit-rs/crates/tui/Cargo.toml">
[package]
name = "pk-tui"
version = { workspace = true }
edition = { workspace = true }
authors = { workspace = true }
license = { workspace = true }
repository = { workspace = true }
description = "Terminal UI for pipeline-kit"

[[bin]]
name = "pk-tui"
path = "src/main.rs"

[lib]
path = "src/lib.rs"

[dependencies]
pk-protocol = { path = "../protocol" }

# TUI
ratatui = "0.29"
crossterm = { version = "0.28", features = ["event-stream"] }

# Async runtime
tokio = { version = "1", features = ["full"] }
tokio-stream = "0.1"

# Error handling
anyhow = "1"

# Utilities
uuid = { version = "1", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
async-stream = "0.3"
</file>

<file path="pipeline-kit-rs/crates/tui/src/lib.rs">
//! # pk-tui
//!
//! Terminal User Interface for pipeline-kit.
//!
//! This crate provides the interactive TUI for monitoring and controlling
//! pipeline execution. It communicates with `pk-core` via channels using
//! the `Op` and `Event` protocol defined in `pk-protocol`.

pub mod app;
pub mod event;
pub mod event_handler;
pub mod tui;
pub mod widgets;

pub use app::App;
pub use event::EventStatus;
pub use tui::Tui;

use anyhow::Result;
use tokio::sync::mpsc;

/// Run the TUI application.
///
/// This is the main entry point for the interactive TUI. It:
/// 1. Sets up the terminal in raw mode
/// 2. Creates communication channels between the UI and core
/// 3. Runs the main event loop
/// 4. Restores the terminal on exit
///
/// # Errors
///
/// Returns an error if terminal initialization fails or if the event loop
/// encounters an unrecoverable error.
pub async fn run_app() -> Result<()> {
    // Initialize the terminal
    let mut tui = Tui::init()?;

    // Clear the terminal
    tui.clear()?;

    // Create channels for communication between UI and core
    // Note: In a full implementation, these would connect to pk-core
    let (op_tx, mut _op_rx) = mpsc::unbounded_channel();
    let (_event_tx, event_rx) = mpsc::unbounded_channel();

    // Create and run the app
    let mut app = App::new(op_tx, event_rx);
    let result = app.run(&mut tui).await;

    // Restore terminal before returning
    tui.restore()?;

    result
}
</file>

<file path="pipeline-kit-rs/Cargo.toml">
[workspace]
members = [
    "crates/cli",
    "crates/core",
    "crates/protocol",
    "crates/protocol-ts",
    "crates/tui",
]
resolver = "2"

[workspace.package]
version = "0.1.1"
edition = "2021"
authors = ["Pipeline Kit Contributors"]
license = "MIT"
repository = "https://github.com/Vooster-AI/pipeline-kit"

[workspace.lints.clippy]
# Code quality lints (deny level)
expect_used = "deny"
unwrap_used = "deny"
identity_op = "deny"
manual_clamp = "deny"
manual_filter = "deny"
manual_find = "deny"
manual_flatten = "deny"
manual_map = "deny"
manual_memcpy = "deny"
manual_ok_or = "deny"
manual_range_contains = "deny"
manual_retain = "deny"
manual_strip = "deny"
manual_unwrap_or = "deny"
needless_borrow = "deny"
needless_borrowed_reference = "deny"
needless_collect = "deny"
needless_late_init = "deny"
needless_option_as_deref = "deny"
needless_question_mark = "deny"
redundant_clone = "deny"
redundant_closure = "deny"
redundant_closure_for_method_calls = "deny"
trivially_copy_pass_by_ref = "deny"
uninlined_format_args = "deny"
unnecessary_filter_map = "deny"
unnecessary_lazy_evaluations = "deny"
unnecessary_to_owned = "deny"
</file>

<file path="pipeline-kit-cli/package.json">
{
  "name": "pipeline-kit",
  "version": "0.1.1",
  "description": "AI agent pipeline orchestration CLI",
  "license": "MIT",
  "type": "module",
  "bin": {
    "pipeline-kit": "bin/pipeline-kit.js"
  },
  "scripts": {
    "postinstall": "node scripts/install.js",
    "test": "vitest run",
    "test:watch": "vitest",
    "test:ui": "vitest --ui",
    "test:coverage": "vitest run --coverage",
    "type-check": "tsc --noEmit",
    "publish:public": "npm publish --access public"
  },
  "engines": {
    "node": ">=16"
  },
  "files": [
    "bin",
    "lib",
    "scripts",
    "vendor"
  ],
  "repository": {
    "type": "git",
    "url": "git+https://github.com/Vooster-AI/pipeline-kit.git",
    "directory": "pipeline-kit-cli"
  },
  "publishConfig": {
    "access": "public"
  },
  "keywords": [
    "ai",
    "agent",
    "pipeline",
    "orchestration",
    "cli"
  ],
  "devDependencies": {
    "@types/node": "^24.7.1",
    "@vitest/coverage-v8": "^3.2.4",
    "@vitest/ui": "^3.2.4",
    "axios": "^1.6.0",
    "mock-fs": "^5.2.0",
    "nock": "^13.5.0",
    "tar": "^7.0.0",
    "typescript": "^5.9.3",
    "vitest": "^3.2.4"
  }
}
</file>

<file path="pipeline-kit-rs/crates/core/src/state/manager.rs">
//! State manager for coordinating multiple pipeline processes.
//!
//! The StateManager is the central orchestrator for all pipeline executions.
//! It maintains a registry of active processes and provides operations for
//! starting, pausing, resuming, and killing processes.

use crate::agents::manager::AgentManager;
use crate::engine::PipelineEngine;
use crate::state::process::kill_process_state;
use crate::state::process::pause_process;
use crate::state::process::resume_process;
use anyhow::Result;
use pk_protocol::ipc::Event;
use pk_protocol::pipeline_models::Pipeline;
use pk_protocol::process_models::Process;
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::sync::Mutex;
use tokio::task::JoinHandle;
use uuid::Uuid;

/// Manages all active pipeline processes.
///
/// The StateManager provides a centralized interface for:
/// - Starting new pipeline executions
/// - Pausing and resuming processes
/// - Killing processes
/// - Querying process state
pub struct StateManager {
    /// Registry of all active processes, indexed by their UUID.
    ///
    /// Uses `Arc<Mutex<Process>>` for thread-safe access across async tasks.
    processes: Arc<Mutex<HashMap<Uuid, Arc<Mutex<Process>>>>>,

    /// Registry of task handles for background execution, indexed by process UUID.
    ///
    /// Allows cancellation of running tasks via `.abort()`.
    task_handles: Arc<Mutex<HashMap<Uuid, JoinHandle<()>>>>,

    /// The pipeline engine for executing pipelines.
    engine: Arc<PipelineEngine>,

    /// Channel for sending events to the UI.
    events_tx: mpsc::Sender<Event>,
}

impl StateManager {
    /// Create a new StateManager.
    ///
    /// # Arguments
    ///
    /// * `agent_manager` - The agent manager for executing agents
    /// * `events_tx` - Channel for sending events to the UI
    pub fn new(agent_manager: AgentManager, events_tx: mpsc::Sender<Event>) -> Self {
        let engine = Arc::new(PipelineEngine::new(agent_manager));

        Self {
            processes: Arc::new(Mutex::new(HashMap::new())),
            task_handles: Arc::new(Mutex::new(HashMap::new())),
            engine,
            events_tx,
        }
    }

    /// Start executing a pipeline in the background.
    ///
    /// This spawns a new tokio task to run the pipeline asynchronously.
    /// The process ID is returned immediately, and events are sent
    /// through the events channel as execution progresses.
    ///
    /// # Arguments
    ///
    /// * `pipeline` - The pipeline definition to execute
    ///
    /// # Returns
    ///
    /// The UUID of the newly created process.
    pub async fn start_pipeline(&self, pipeline: Pipeline) -> Uuid {
        // Create and register the process
        let process_id = self.create_and_register_process(&pipeline.name).await;

        // Spawn the pipeline execution in the background
        self.spawn_pipeline_execution(process_id, pipeline).await;

        // Return the process ID
        process_id
    }

    /// Create a new process and register it in the process registry.
    ///
    /// This is a helper function that creates a process with a unique ID,
    /// initializes it in the Pending state, and stores it in the registry.
    ///
    /// # Arguments
    ///
    /// * `pipeline_name` - The name of the pipeline
    ///
    /// # Returns
    ///
    /// The UUID of the newly created process.
    async fn create_and_register_process(&self, pipeline_name: &str) -> Uuid {
        let process_id = Uuid::new_v4();

        let mut initial_process = crate::state::process::create_process(pipeline_name.to_string());
        initial_process.id = process_id;

        let mut procs = self.processes.lock().await;
        procs.insert(process_id, Arc::new(Mutex::new(initial_process)));

        process_id
    }

    /// Spawn a background task to execute the pipeline.
    ///
    /// This function spawns a tokio task that runs the pipeline engine
    /// and updates the process state based on the execution result.
    /// The task handle is stored for later cancellation via kill_process.
    ///
    /// # Arguments
    ///
    /// * `process_id` - The ID of the process to execute
    /// * `pipeline` - The pipeline definition to execute
    async fn spawn_pipeline_execution(&self, process_id: Uuid, pipeline: Pipeline) {
        let engine = Arc::clone(&self.engine);
        let processes = Arc::clone(&self.processes);
        let task_handles = Arc::clone(&self.task_handles);
        let events_tx = self.events_tx.clone();

        // Get the process from the registry to pass to the engine
        let process = {
            let procs = processes.lock().await;
            if let Some(process_arc) = procs.get(&process_id) {
                let p = process_arc.lock().await;
                p.clone()
            } else {
                return; // Process not found, should not happen
            }
        };

        let handle = tokio::spawn(async move {
            match engine.run(&pipeline, process, events_tx.clone()).await {
                Ok(final_process) => {
                    Self::update_process_state(processes.clone(), process_id, final_process).await;
                }
                Err(e) => {
                    Self::handle_pipeline_failure(processes.clone(), process_id, e).await;
                }
            }

            // Clean up the task handle after completion
            let mut handles = task_handles.lock().await;
            handles.remove(&process_id);
        });

        // Store the task handle
        let mut handles = self.task_handles.lock().await;
        handles.insert(process_id, handle);
    }

    /// Update the stored process state after successful execution.
    ///
    /// # Arguments
    ///
    /// * `processes` - The shared process registry
    /// * `process_id` - The ID of the process to update
    /// * `final_process` - The final process state from the engine
    async fn update_process_state(
        processes: Arc<Mutex<HashMap<Uuid, Arc<Mutex<Process>>>>>,
        process_id: Uuid,
        final_process: Process,
    ) {
        let mut procs = processes.lock().await;
        if let Some(process_arc) = procs.get_mut(&process_id) {
            let mut process = process_arc.lock().await;
            *process = final_process;
        }
    }

    /// Handle pipeline execution failure by updating the process state.
    ///
    /// # Arguments
    ///
    /// * `processes` - The shared process registry
    /// * `process_id` - The ID of the failed process
    /// * `error` - The error that caused the failure
    async fn handle_pipeline_failure(
        processes: Arc<Mutex<HashMap<Uuid, Arc<Mutex<Process>>>>>,
        process_id: Uuid,
        error: anyhow::Error,
    ) {
        eprintln!("Pipeline execution failed: {}", error);

        let mut procs = processes.lock().await;
        if let Some(process_arc) = procs.get_mut(&process_id) {
            let mut process = process_arc.lock().await;
            process.status = pk_protocol::process_models::ProcessStatus::Failed;
            process.logs.push(format!("Error: {}", error));
        }
    }

    /// Pause a running process.
    ///
    /// The process will transition to the Paused state and stop execution
    /// after completing its current step.
    ///
    /// # Arguments
    ///
    /// * `process_id` - The UUID of the process to pause
    ///
    /// # Errors
    ///
    /// Returns an error if the process is not found.
    pub async fn pause_process_by_id(&self, process_id: Uuid) -> Result<()> {
        let processes = self.processes.lock().await;

        if let Some(process_arc) = processes.get(&process_id) {
            let mut process = process_arc.lock().await;
            pause_process(&mut process, &self.events_tx).await;
            Ok(())
        } else {
            Err(anyhow::anyhow!("Process {} not found", process_id))
        }
    }

    /// Resume a paused process.
    ///
    /// The process will transition from Paused or HumanReview state
    /// back to Running and continue execution.
    ///
    /// This method uses the Notify pattern to wake up the blocked
    /// PipelineEngine task. The background task is already running
    /// and waiting on `resume_notifier.notified().await`, so no
    /// re-spawning is needed.
    ///
    /// # Arguments
    ///
    /// * `process_id` - The UUID of the process to resume
    ///
    /// # Errors
    ///
    /// Returns an error if the process is not found.
    pub async fn resume_process_by_id(&self, process_id: Uuid) -> Result<()> {
        let processes = self.processes.lock().await;

        if let Some(process_arc) = processes.get(&process_id) {
            let mut process = process_arc.lock().await;
            resume_process(&mut process, &self.events_tx).await;
            Ok(())
        } else {
            Err(anyhow::anyhow!("Process {} not found", process_id))
        }
    }

    /// Kill a running process immediately.
    ///
    /// This method aborts the background tokio task executing the pipeline,
    /// marks the process as Killed, and emits a ProcessKilled event.
    ///
    /// # Arguments
    ///
    /// * `process_id` - The UUID of the process to kill
    ///
    /// # Errors
    ///
    /// Returns an error if the process is not found.
    pub async fn kill_process(&self, process_id: Uuid) -> Result<()> {
        // 1. Abort the task handle
        let mut task_handles = self.task_handles.lock().await;
        if let Some(handle) = task_handles.remove(&process_id) {
            handle.abort();
        }
        drop(task_handles);

        // 2. Update the process state to Killed
        let processes = self.processes.lock().await;
        if let Some(process_arc) = processes.get(&process_id) {
            let mut process = process_arc.lock().await;
            kill_process_state(&mut process, &self.events_tx).await;
            Ok(())
        } else {
            Err(anyhow::anyhow!("Process {} not found", process_id))
        }
    }

    /// Get the current state of a process.
    ///
    /// # Arguments
    ///
    /// * `process_id` - The UUID of the process to query
    ///
    /// # Returns
    ///
    /// A clone of the process state, or None if not found.
    pub async fn get_process(&self, process_id: Uuid) -> Option<Process> {
        let processes = self.processes.lock().await;
        if let Some(process_arc) = processes.get(&process_id) {
            let process = process_arc.lock().await;
            Some(process.clone())
        } else {
            None
        }
    }

    /// Get all active processes.
    ///
    /// # Returns
    ///
    /// A vector of all active process states.
    pub async fn get_all_processes(&self) -> Vec<Process> {
        let processes = self.processes.lock().await;
        let mut result = Vec::new();

        for process_arc in processes.values() {
            let process = process_arc.lock().await;
            result.push(process.clone());
        }

        result
    }

    /// Get the number of active processes.
    pub async fn process_count(&self) -> usize {
        let processes = self.processes.lock().await;
        processes.len()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use pk_protocol::agent_models::Agent as AgentConfig;
    use pk_protocol::pipeline_models::MasterAgentConfig;
    use pk_protocol::pipeline_models::ProcessStep;
    use pk_protocol::process_models::ProcessStatus;
    use std::collections::HashMap as StdHashMap;

    fn create_test_agent_config(name: &str) -> AgentConfig {
        AgentConfig {
            name: name.to_string(),
            description: format!("Test agent {}", name),
            model: "test-model".to_string(),
            color: "blue".to_string(),
            system_prompt: "Test prompt".to_string(),
        }
    }

    fn create_test_pipeline(name: &str, steps: Vec<ProcessStep>) -> Pipeline {
        Pipeline {
            name: name.to_string(),
            required_reference_file: StdHashMap::new(),
            output_file: StdHashMap::new(),
            master: MasterAgentConfig {
                model: "test-model".to_string(),
                system_prompt: "Test orchestration".to_string(),
                process: steps,
            },
            sub_agents: vec!["agent1".to_string()],
        }
    }

    #[tokio::test]
    async fn test_state_manager_new() {
        let configs = vec![create_test_agent_config("test-agent")];
        let manager = AgentManager::new(configs);
        let (tx, _rx) = mpsc::channel(100);

        let state_manager = StateManager::new(manager, tx);
        assert_eq!(state_manager.process_count().await, 0);
    }

    #[tokio::test]
    async fn test_state_manager_start_pipeline() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let (tx, mut rx) = mpsc::channel(100);

        let state_manager = StateManager::new(manager, tx);

        let steps = vec![ProcessStep::Agent("agent1".to_string())];
        let pipeline = create_test_pipeline("test-pipeline", steps);

        let _process_id = state_manager.start_pipeline(pipeline).await;

        // Give the task time to start
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

        // Should receive some events
        let mut events = Vec::new();
        while let Ok(event) = rx.try_recv() {
            events.push(event);
        }

        // Should have at least ProcessStarted event
        assert!(!events.is_empty());
    }

    #[tokio::test]
    async fn test_state_manager_get_process() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let (tx, _rx) = mpsc::channel(100);

        let state_manager = StateManager::new(manager, tx);

        // Process doesn't exist yet
        let nonexistent_id = Uuid::new_v4();
        let result = state_manager.get_process(nonexistent_id).await;
        assert!(result.is_none());
    }

    /// RED: Test that start_pipeline returns unique UUIDs and stores processes
    ///
    /// This test validates that:
    /// 1. Each call to start_pipeline returns a different UUID
    /// 2. The process is stored in the StateManager's internal registry
    /// 3. Multiple processes can be started and tracked independently
    #[tokio::test]
    async fn test_start_pipeline_returns_unique_uuids_and_stores_processes() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let (tx, _rx) = mpsc::channel(100);

        let state_manager = StateManager::new(manager, tx);

        // Create two test pipelines
        let steps = vec![ProcessStep::Agent("agent1".to_string())];
        let pipeline1 = create_test_pipeline("pipeline-1", steps.clone());
        let pipeline2 = create_test_pipeline("pipeline-2", steps);

        // Start first pipeline
        let process_id_1 = state_manager.start_pipeline(pipeline1).await;

        // Start second pipeline
        let process_id_2 = state_manager.start_pipeline(pipeline2).await;

        // UUIDs should be different
        assert_ne!(process_id_1, process_id_2, "Process IDs should be unique");

        // Give the engine time to store the processes
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;

        // Both processes should be in the registry
        assert_eq!(
            state_manager.process_count().await,
            2,
            "StateManager should have 2 processes stored"
        );

        // Should be able to retrieve the processes by ID
        let proc1 = state_manager.get_process(process_id_1).await;
        assert!(proc1.is_some(), "Process 1 should be retrievable");
        assert_eq!(proc1.unwrap().pipeline_name, "pipeline-1");

        let proc2 = state_manager.get_process(process_id_2).await;
        assert!(proc2.is_some(), "Process 2 should be retrievable");
        assert_eq!(proc2.unwrap().pipeline_name, "pipeline-2");
    }

    /// RED: Acceptance test for resume_process_by_id
    ///
    /// This test validates that:
    /// 1. A process in HUMAN_REVIEW state can be resumed
    /// 2. The resume signal actually triggers continuation of pipeline execution
    /// 3. The process completes after being resumed
    /// 4. ProcessResumed event is emitted
    #[tokio::test]
    async fn test_resume_process_by_id_acceptance() {
        let configs = vec![
            create_test_agent_config("agent1"),
            create_test_agent_config("agent2"),
        ];
        let manager = AgentManager::new(configs);
        let (tx, mut rx) = mpsc::channel(100);

        let state_manager = StateManager::new(manager, tx);

        // Create a pipeline with HUMAN_REVIEW in the middle
        let steps = vec![
            ProcessStep::Agent("agent1".to_string()),
            ProcessStep::HumanReview(pk_protocol::pipeline_models::HumanReviewMarker),
            ProcessStep::Agent("agent2".to_string()),
        ];
        let pipeline = create_test_pipeline("review-pipeline", steps);

        // Start the pipeline
        let process_id = state_manager.start_pipeline(pipeline).await;

        // Collect events until HumanReview state is reached
        let timeout = tokio::time::Duration::from_secs(2);
        let mut events = Vec::new();
        let mut human_review_reached = false;

        while let Ok(Some(event)) = tokio::time::timeout(timeout, rx.recv()).await {
            let is_human_review = matches!(
                &event,
                Event::ProcessStatusUpdate {
                    status: ProcessStatus::HumanReview,
                    ..
                }
            );
            events.push(event);

            if is_human_review {
                human_review_reached = true;
                break;
            }
        }

        assert!(
            human_review_reached,
            "Pipeline should reach HUMAN_REVIEW state (verified via events)"
        );

        // NOTE: We verify HumanReview state via events because StateManager's
        // process map is only updated when engine.run() completes. While paused
        // at HumanReview, engine.run() is blocked, so the process state in the
        // map hasn't been updated yet. This is expected behavior - the events
        // are the source of truth for real-time state updates.

        // Resume the process
        let resume_result = state_manager.resume_process_by_id(process_id).await;
        assert!(resume_result.is_ok(), "Resume should succeed");

        // Wait for completion event after resume
        let mut completed = false;
        while let Ok(Some(event)) = tokio::time::timeout(timeout, rx.recv()).await {
            let is_completed = matches!(&event, Event::ProcessCompleted { .. });
            events.push(event);

            if is_completed {
                completed = true;
                break;
            }
        }

        assert!(completed, "Pipeline should complete after resume");

        // Verify the process completed
        let final_process = state_manager.get_process(process_id).await.unwrap();
        assert_eq!(
            final_process.status,
            ProcessStatus::Completed,
            "Process should complete after resume"
        );

        // Verify we received the ProcessResumed event
        let has_resumed_event = events
            .iter()
            .any(|e| matches!(e, Event::ProcessResumed { process_id: pid } if *pid == process_id));

        assert!(has_resumed_event, "Should emit ProcessResumed event");
    }

    /// RED: Acceptance test for kill_process with task cancellation
    ///
    /// This test validates that:
    /// 1. kill_process aborts the background tokio task immediately
    /// 2. The process state transitions to Killed
    /// 3. ProcessKilled event is emitted
    /// 4. The task handle is removed from the registry
    /// 5. No memory leaks occur
    #[tokio::test]
    async fn test_kill_process_aborts_background_task() {
        let configs = vec![create_test_agent_config("agent1")];
        let manager = AgentManager::new(configs);
        let (tx, mut rx) = mpsc::channel(100);

        let state_manager = StateManager::new(manager, tx);

        // Create a long-running pipeline (will simulate with a simple agent)
        let steps = vec![ProcessStep::Agent("agent1".to_string())];
        let pipeline = create_test_pipeline("long-running", steps);

        // Start the pipeline
        let process_id = state_manager.start_pipeline(pipeline).await;

        // Give the task time to start
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;

        // Verify the process is running
        let process = state_manager.get_process(process_id).await;
        assert!(process.is_some(), "Process should exist");

        // Kill the process
        let kill_result = state_manager.kill_process(process_id).await;
        assert!(kill_result.is_ok(), "Kill should succeed");

        // Wait a moment for the abortion to take effect
        tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;

        // Verify the process was marked as Killed
        let killed_process = state_manager.get_process(process_id).await;
        assert!(
            killed_process.is_some(),
            "Process should still be retrievable"
        );
        assert_eq!(
            killed_process.unwrap().status,
            ProcessStatus::Killed,
            "Process should be in Killed state"
        );

        // Verify ProcessKilled event was emitted
        let mut events = Vec::new();
        while let Ok(event) = rx.try_recv() {
            events.push(event);
        }

        let has_killed_event = events
            .iter()
            .any(|e| matches!(e, Event::ProcessKilled { process_id: pid } if *pid == process_id));

        assert!(has_killed_event, "Should emit ProcessKilled event");

        // Verify the task handle was removed (implicitly tested by successful abort)
        // If the handle wasn't removed, subsequent operations would fail
    }
}
</file>

<file path="pipeline-kit-rs/crates/core/Cargo.toml">
[package]
name = "pk-core"
version = { workspace = true }
edition = { workspace = true }
authors = { workspace = true }
license = { workspace = true }
repository = { workspace = true }
description = "Core pipeline engine and agent management"

[dependencies]
pk-protocol = { path = "../protocol" }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
toml = "0.8"
gray_matter = "0.2"
thiserror = "1.0"
walkdir = "2.4"
async-trait = "0.1"
tokio = { version = "1.40", features = ["full"] }
tokio-stream = { version = "0.1", features = ["io-util"] }
anyhow = "1.0"
uuid = { version = "1.10", features = ["v4"] }
chrono = "0.4"
tempfile = "3.10"
which = "6.0"
async-stream = "0.3.6"
rust-embed = { version = "8.0", features = ["debug-embed", "interpolate-folder-path"] }

[dev-dependencies]
tempfile = "3.10"
tokio = { version = "1.40", features = ["full"] }

[features]
integration-tests = []
e2e-cli-tests = []  # Enable real CLI integration tests (requires actual CLI tools installed)
</file>

</files>
